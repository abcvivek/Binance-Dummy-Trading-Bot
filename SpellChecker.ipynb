{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpellChecker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bgA-s2CCbC8UwQzy0xoASqz7XVUr5iUn",
      "authorship_tag": "ABX9TyOn+T9WRUpzpiihzeT4uJzX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abcvivek/Binance-Dummy-Trading-Bot/blob/master/SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2wJB55uKwzB",
        "colab_type": "code",
        "outputId": "236e2eee-7092-43f2-fb2b-ff3db85be0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 1.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiHDpEyLLHpx",
        "colab_type": "code",
        "outputId": "2ad5fb59-051b-406a-d2a1-bbee064ee1bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\"\"\" Importing Libraries\n",
        " numpy are used for maths calculations\n",
        " tenserflow is used for building Neural Networks\n",
        " os will help in managing all the os related stuff \n",
        " re is used as regular expression\n",
        "train_test_split will help in dividing dataset into training set and testing set \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import time\n",
        "from math import ceil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from collections import namedtuple\n",
        "\n",
        "################ LOADING OF DATA ##################\n",
        "\n",
        "path = '/content/drive/My Drive/clean.txt'\n",
        "\n",
        "# Function to load the contents of the dataset\n",
        "     \n",
        "def load_file(path): \n",
        "  input_file = os.path.join(path) \n",
        "  with open(input_file) as file:\n",
        "    File = file.read()\n",
        "  return File\n",
        "       \n",
        "file_content = load_file(path)\n",
        "\n",
        "\n",
        "#################  Preprocessing of Data  #################\n",
        "\n",
        "# Create a dictionary to convert the vocabulary (characters) to integers\n",
        "\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "\n",
        "for character in file_content:\n",
        "  if character not in vocab_to_int:\n",
        "    vocab_to_int[character] = count\n",
        "    count += 1\n",
        "\n",
        "\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "  vocab_to_int[code] = count\n",
        "  count += 1\n",
        "\n",
        "\n",
        "# Check the size of vocabulary and all of the values\n",
        "\n",
        "print(\"The vocabulary contains {} characters.\".format(len(vocab_to_int)))\n",
        "print(sorted(vocab_to_int))\n",
        "\n",
        "\n",
        "#Create another dictionary to convert integers to their respective characters\n",
        "\n",
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character\n",
        "\n",
        "print(int_to_vocab.items())\n",
        "\n",
        "# Split the text from the file into sentences.\n",
        "\n",
        "sentences = []\n",
        "for sentence in file_content.splitlines():\n",
        "  sentences.append(sentence)\n",
        "print(\" Dataset contains {} sentences.\".format(len(sentences)))\n",
        "\n",
        "\n",
        "# Convert character sentences to integer sentence\n",
        "\n",
        "int_sentences = []\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)\n",
        "\n",
        "\n",
        "# Limit the data we will use to train our model\n",
        "\n",
        "max_length = 250\n",
        "min_length = 30\n",
        "\n",
        "\n",
        "good_sentences = []\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))\n",
        "\n",
        "\n",
        "# Split the data into training, testing and validation sentences\n",
        "\n",
        "training, testing = train_test_split(good_sentences, test_size = 0.10, random_state = 2)\n",
        "testing, validation = train_test_split(testing, test_size = 0.70, random_state = 2)\n",
        "print(\"Number of Training sentences:\", len(training))\n",
        "print(\"Number of Validiation sentences:\", len(validation))\n",
        "print(\"Number of Testing sentences:\", len(testing))\n",
        "\n",
        "\n",
        "# Sort the sentences by length to reduce padding, which will allow the model to train faster\n",
        "\n",
        "training_sorted = []\n",
        "validation_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "\n",
        "    for sentence in validation:\n",
        "        if len(sentence) == i:\n",
        "            validation_sorted.append(sentence)\n",
        "\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)\n",
        "\n",
        "\n",
        "# Generate Artificial noise into Correct sentence\n",
        "\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "\n",
        "    '''Relocate, remove, or add characters to create spelling mistakes''' \n",
        "\n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence\n",
        "\n",
        "\n",
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()\n",
        "\n",
        "\n",
        "###########################  Building the Model  ###############################\n",
        "\n",
        "# Model input pipes for data feed to the model\n",
        "\n",
        "'''\n",
        "Tensorflow placeholders acts as pipes into the model.\n",
        "name_scope makes sure so given values are from the same graph.\n",
        "'''\n",
        "\n",
        "def model_inputs():\n",
        "\n",
        "    with tf.name_scope('inputs'):\n",
        "        # ARGS: dtype, shape of the tensor to be fed, name for operation\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "\n",
        "    # ARGS: input tensor, name for operation\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length\n",
        "\n",
        "\n",
        "# Remove last word from each batch, add <GO> token to the start of each batch\n",
        "\n",
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "\n",
        "    with tf.name_scope(\"processing_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "    return dec_input\n",
        "\n",
        "\n",
        "#Encoding layer takes the input sequence and creates a encoded representation\n",
        "\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "\n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "            return enc_output, enc_state\n",
        "\n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "\n",
        "            # Concat outputs\n",
        "            enc_output = tf.concat(enc_output, 2)\n",
        "\n",
        "            # Use only forwarded state\n",
        "            return enc_output, enc_state[0]\n",
        "\n",
        "\n",
        "# Create training logits\n",
        "\n",
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length):\n",
        "\n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, sequence_length=targets_length, time_major=False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
        "        training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return training_logits\n",
        "\n",
        "# Create inference logits\n",
        "\n",
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, max_target_length, batch_size):\n",
        "     \n",
        "     print(max_target_length)\n",
        "\n",
        "     with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
        "        inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits\n",
        "\n",
        "#Create the decoding cell and attention.\n",
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "\n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "\n",
        "    output_layer = Dense(vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, inputs_length, normalize=False, name='BahdanauAttention')\n",
        "\n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
        "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
        "    initial_state = initial_state.clone(cell_state=enc_state)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], dec_cell, initial_state, output_layer, max_target_length, batch_size)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "\n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, enc_embed_input, keep_prob, direction)\n",
        "\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "\n",
        "###############   Hyper-Parameters   ##################\n",
        "\n",
        "# The default parameters\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "num_layers = 4\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.90\n",
        "keep_probability = 0.65 #0.75\n",
        "\n",
        "\n",
        "def build_accuracy(predictions, targets):\n",
        "    correct_prediction = tf.equal(tf.cast(tf.round(predictions), tf.int32), targets)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# Build graph\n",
        "\n",
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Load model inputs\n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, inputs_length, targets_length, max_target_length, len(vocab_to_int)+1, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    accuracy = build_accuracy(predictions, targets)\n",
        "\n",
        "    # Merge summaries\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    # Export the nodes\n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length', 'predictions', 'merged', 'train_op','accuracy', 'optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "\n",
        "def get_batches(sentences, batch_size, threshold):\n",
        "\n",
        "    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n",
        "       With each epoch, sentences will receive new mistakes\"\"\"\n",
        "\n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "\n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "\n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "\n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "\n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths\n",
        "\n",
        "\n",
        "################## Training the Model   ######################\n",
        "\n",
        "def MakeSentenceReadable(correct):\n",
        "  correct_sentence = \"\"\n",
        "  for i in correct:\n",
        "    if i < 28:\n",
        "      correct_sentence += int_to_vocab[i]\n",
        "  return correct_sentence.strip()\n",
        "      \n",
        "\n",
        "\n",
        "def train(model, epochs):\n",
        "\n",
        "    '''Train the RNN'''    \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 5 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
        "        per_epoch = 1 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
        "        is_correct = 0\n",
        "\n",
        "       \n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "\n",
        "            checkpoint = \"./Model-{}.ckpt\".format(epoch_i)\n",
        "\n",
        "            print()\n",
        "            print(\"Training Model: {}\".format(epoch_i))\n",
        "\n",
        "            train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(epoch_i), sess.graph)\n",
        "            test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(epoch_i))\n",
        "\n",
        "\n",
        "             # Per batch\n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(training_sorted,batch_size,threshold)):\n",
        "                start_time = time.time()\n",
        "                summary, loss, _ = sess.run([model.merged, model.cost, model.train_op],\n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                # Print info\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    # Reset\n",
        "\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "\n",
        "                #### Run Validation Testing ####\n",
        "\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(validation_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged, model.cost],\n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Record the progress of testing\n",
        "\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "\n",
        "                    # Print Result\n",
        "\n",
        "                    for i in range(100, 120):\n",
        "\n",
        "                        correct = validation_sorted[i]\n",
        "                        text = noise_maker(validation_sorted[i],threshold)\n",
        "                        answer_logits = sess.run(model.predictions, {model.inputs: [text]* batch_size,\n",
        "                                                                 model.inputs_length: [len(text)]* batch_size,\n",
        "                                                                 model.targets_length: [len(text)+1],\n",
        "                                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "                        correct_sentence = MakeSentenceReadable(correct)\n",
        "                        text_sentence = MakeSentenceReadable(text)\n",
        "                        answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "\n",
        "                       \n",
        "                        if (answer_logits_sentence == correct_sentence):\n",
        "                            is_correct += 1\n",
        "\n",
        "                        # # Remove <PAD> from output\n",
        "                        # pad = vocab_to_int[\"<PAD>\"]\n",
        "                        # eos = vocab_to_int[\"<EOS>\"]\n",
        "\n",
        "                        # answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != eos])\n",
        "                        # answer_logits.strip()\n",
        "                        # #answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "                        # correct = \"\".join([int_to_vocab[i] for i in correct if i != eos])\n",
        "                        # correct.strip()\n",
        "\n",
        "                        print('  Validation Input: {}'.format(text_sentence))\n",
        "                        print('  Validation Output: {}'.format(answer_logits_sentence))\n",
        "                        print('  Correct: {}'.format(correct_sentence))\n",
        "                        print('  Is Correct: {}'.format(answer_logits_sentence == correct_sentence))\n",
        "                        print()\n",
        "                    \n",
        "\n",
        "                    batch_time_testing = 0\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'.format(batch_loss_testing / n_batches_testing, batch_time_testing))\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "\n",
        "\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "\n",
        "                    if is_correct > 8:\n",
        "                        print('New Accuracy Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"/content/drive/My Drive/Colab Notebooks/Model-{}.ckpt\".format(is_correct)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "                    else:\n",
        "                        if batch_loss_testing <= min(testing_loss_summary):\n",
        "                            print('New Loss Record!') \n",
        "                            stop_early = 0\n",
        "                            checkpoint = \"/content/drive/My Drive/Colab Notebooks/Model-{}.ckpt\".format(epoch_i)\n",
        "                            saver = tf.train.Saver()\n",
        "                            saver.save(sess, checkpoint)\n",
        "                        else:\n",
        "                            print(\"No Improvement.\")\n",
        "                            stop_early += 1\n",
        "                            if stop_early == stop:\n",
        "                                break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "# Train the model with the desired tuning parameters\n",
        "\n",
        "for keep_probability in [0.65]:\n",
        "    for num_layers in [4]:\n",
        "        for threshold in [0.90]:\n",
        "            \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
        "            train(model, epochs)\n",
        "\n",
        "\n",
        "#################   Testing the Model   ################\n",
        "\n",
        "def test(model,testing_set):\n",
        "    # Start session\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "        print()\n",
        "        print(\"Testing LSTM Model\")\n",
        "\n",
        "        testing_check = (len(testing_set)//batch_size//1)-1\n",
        "        tested = 0\n",
        "        is_correct = 0\n",
        "        checkpoint = \"/content/drive/My Drive/Colab Notebooks/Model-11.ckpt\"\n",
        "        saver.restore(sess, checkpoint)\n",
        "\n",
        "        # Per batch\n",
        "        for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(testing_set,batch_size,threshold)):\n",
        "            if batch_i % testing_check == 0 and batch_i > 0:  \n",
        "                \n",
        "                print_tested_each = 100\n",
        "\n",
        "                for i in range(0, len(testing_set)):\n",
        "\n",
        "                    if (tested > print_tested_each):\n",
        "                        print_tested_each  += 100\n",
        "                        print(\"Tested {}% of test set\".format((ceil(i / len(testing_set) * 100) * 100) / 100.0))\n",
        "\n",
        "                    text = noise_maker(testing_set[i],threshold)\n",
        "                    correct = testing_set[i]\n",
        "                    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size,\n",
        "                                                             model.inputs_length: [len(text)]*batch_size,\n",
        "                                                             model.targets_length: [len(text)+1],\n",
        "                                                             model.keep_prob: [1.0]})[0]\n",
        "\n",
        "                    # # Remove <PAD> from output\n",
        "                    # pad = vocab_to_int[\"<PAD>\"]\n",
        "                    # eos = vocab_to_int[\"<EOS>\"]\n",
        "\n",
        "                    # answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != eos])\n",
        "                    # #answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "                    # correct = \"\".join([int_to_vocab[i] for i in correct if i != eos])\n",
        "\n",
        "                    correct_sentence = MakeSentenceReadable(correct)\n",
        "                    text_sentence = MakeSentenceReadable(text)\n",
        "                    answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "\n",
        "                    tested += 1\n",
        "                    if (answer_logits_sentence == correct_sentence):\n",
        "                        is_correct += 1\n",
        "\n",
        "                # Reset\n",
        "                print(\"Accuracy %: {}%\".format((ceil((is_correct / tested) * 100) * 100) / 100.0))\n",
        "                print(\"Exact Accuracy: {}\".format(is_correct / tested))\n",
        "                \n",
        "                return is_correct / tested\n",
        "\n",
        "for keep_probability in [0.65]:\n",
        "    for num_layers in [4]:\n",
        "        for threshold in [0.90]:\n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
        "            total = 0\n",
        "            total = test(model, testing_sorted)\n",
        "\n",
        "            print(\"Total Accuracy %: {}\".format((ceil((total / 3) * 100) * 100) / 100.0))\n",
        "            print(\"Total Accuracy Exact: {}\".format(total / 3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Fixing Custom Sentences\n",
        "\n",
        "def text_to_ints(text):\n",
        "\n",
        "    '''Prepare the text for the model'''\n",
        "\n",
        "    return [vocab_to_int[word] for word in text]\n",
        "\n",
        "\n",
        "\n",
        "# Create your own sentence or use one from the dataset\n",
        "\n",
        "text = \"spellin is difficult whch is wyh you need to study everyday\"\n",
        "\n",
        "text = text_to_ints(text)\n",
        "\n",
        "checkpoint = checkpoint = checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Load saved model\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "\n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "# Remove the padding from the generated sentence\n",
        "\n",
        "eos = vocab_to_int[\"<EOS>\"] \n",
        "print('\\nText'),\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != eos]))\n",
        "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != eos])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary contains 31 characters.\n",
            "['\\n', ' ', '<EOS>', '<GO>', '<PAD>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "dict_items([(0, 't'), (1, 'h'), (2, 'i'), (3, 's'), (4, ' '), (5, 'w'), (6, 'a'), (7, 'y'), (8, 'o'), (9, 'u'), (10, 'c'), (11, 'v'), (12, 'e'), (13, 'r'), (14, 'b'), (15, 'p'), (16, 'f'), (17, 'l'), (18, 'd'), (19, 'n'), (20, '\\n'), (21, 'j'), (22, 'm'), (23, 'k'), (24, 'g'), (25, 'x'), (26, 'z'), (27, 'q'), (28, '<PAD>'), (29, '<EOS>'), (30, '<GO>')])\n",
            " Dataset contains 1232681 sentences.\n",
            "We will use 1017165 to train and test our model.\n",
            "Number of Training sentences: 915448\n",
            "Number of Validiation sentences: 71202\n",
            "Number of Testing sentences: 30515\n",
            "[6, 4, 5, 6, 13, 19, 2, 19, 24, 4, 5, 6, 3, 4, 24, 2, 11, 12, 19, 4, 14, 7, 4, 2, 3, 6, 6, 10, 4, 22]\n",
            "[6, 4, 5, 6, 13, 2, 19, 24, 4, 5, 6, 3, 4, 24, 2, 12, 11, 19, 4, 14, 7, 4, 2, 3, 6, 6, 22, 4]\n",
            "\n",
            "[6, 4, 16, 12, 5, 4, 15, 12, 0, 13, 8, 17, 4, 14, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 1, 13, 8, 5, 19]\n",
            "[6, 4, 16, 12, 5, 4, 15, 12, 0, 13, 8, 17, 4, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 13, 1, 8, 5, 19]\n",
            "\n",
            "[0, 1, 12, 4, 13, 12, 3, 0, 4, 8, 16, 4, 9, 3, 4, 19, 12, 12, 18, 4, 12, 6, 10, 1, 4, 8, 0, 1, 12, 13]\n",
            "[0, 1, 12, 13, 4, 3, 12, 0, 4, 8, 16, 4, 9, 3, 4, 19, 12, 12, 18, 4, 6, 10, 1, 4, 8, 0, 1, 12, 13]\n",
            "\n",
            "[3, 8, 22, 12, 4, 3, 0, 9, 22, 14, 17, 12, 4, 8, 11, 12, 13, 4, 6, 4, 16, 6, 17, 3, 12, 4, 1, 8, 15, 12]\n",
            "[3, 8, 22, 13, 12, 4, 3, 0, 9, 22, 14, 17, 12, 4, 8, 11, 12, 13, 4, 6, 4, 16, 6, 17, 3, 12, 4, 1, 8, 12, 15]\n",
            "\n",
            "[3, 2, 19, 10, 12, 4, 0, 1, 12, 4, 22, 6, 13, 23, 12, 0, 4, 5, 2, 17, 17, 4, 14, 12, 4, 24, 2, 11, 12, 19]\n",
            "[3, 2, 19, 10, 12, 4, 0, 7, 1, 12, 4, 22, 6, 13, 9, 23, 0, 12, 4, 5, 2, 17, 17, 4, 14, 12, 4, 24, 1, 2, 12, 11, 19]\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-8d5abf7de948>:234: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-8d5abf7de948>:240: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Tensor(\"max_target_len:0\", shape=(), dtype=int32)\n",
            "\n",
            "Training Model: 1\n",
            "Epoch   1/100 Batch   30/14303 - Loss:  2.701, Seconds: 8.29\n",
            "Epoch   1/100 Batch   60/14303 - Loss:  1.995, Seconds: 5.33\n",
            "Epoch   1/100 Batch   90/14303 - Loss:  1.534, Seconds: 5.31\n",
            "Epoch   1/100 Batch  120/14303 - Loss:  1.070, Seconds: 5.25\n",
            "Epoch   1/100 Batch  150/14303 - Loss:  0.775, Seconds: 5.27\n",
            "Epoch   1/100 Batch  180/14303 - Loss:  0.499, Seconds: 5.47\n",
            "Epoch   1/100 Batch  210/14303 - Loss:  0.481, Seconds: 5.29\n",
            "Epoch   1/100 Batch  240/14303 - Loss:  0.432, Seconds: 5.25\n",
            "Epoch   1/100 Batch  270/14303 - Loss:  0.378, Seconds: 5.26\n",
            "Epoch   1/100 Batch  300/14303 - Loss:  0.376, Seconds: 5.27\n",
            "Epoch   1/100 Batch  330/14303 - Loss:  0.350, Seconds: 5.38\n",
            "Epoch   1/100 Batch  360/14303 - Loss:  0.347, Seconds: 5.43\n",
            "Epoch   1/100 Batch  390/14303 - Loss:  0.328, Seconds: 5.39\n",
            "Epoch   1/100 Batch  420/14303 - Loss:  0.332, Seconds: 5.42\n",
            "Epoch   1/100 Batch  450/14303 - Loss:  0.334, Seconds: 5.42\n",
            "Epoch   1/100 Batch  480/14303 - Loss:  0.326, Seconds: 5.41\n",
            "Epoch   1/100 Batch  510/14303 - Loss:  0.311, Seconds: 5.42\n",
            "Epoch   1/100 Batch  540/14303 - Loss:  0.304, Seconds: 5.42\n",
            "Epoch   1/100 Batch  570/14303 - Loss:  0.292, Seconds: 5.49\n",
            "Epoch   1/100 Batch  600/14303 - Loss:  0.296, Seconds: 5.45\n",
            "Epoch   1/100 Batch  630/14303 - Loss:  0.297, Seconds: 5.58\n",
            "Epoch   1/100 Batch  660/14303 - Loss:  0.289, Seconds: 5.53\n",
            "Epoch   1/100 Batch  690/14303 - Loss:  0.279, Seconds: 5.52\n",
            "Epoch   1/100 Batch  720/14303 - Loss:  0.287, Seconds: 5.59\n",
            "Epoch   1/100 Batch  750/14303 - Loss:  0.275, Seconds: 5.51\n",
            "Epoch   1/100 Batch  780/14303 - Loss:  0.273, Seconds: 5.58\n",
            "Epoch   1/100 Batch  810/14303 - Loss:  0.279, Seconds: 5.57\n",
            "Epoch   1/100 Batch  840/14303 - Loss:  0.270, Seconds: 5.55\n",
            "Epoch   1/100 Batch  870/14303 - Loss:  0.261, Seconds: 5.57\n",
            "Epoch   1/100 Batch  900/14303 - Loss:  0.266, Seconds: 5.51\n",
            "Epoch   1/100 Batch  930/14303 - Loss:  0.259, Seconds: 5.63\n",
            "Epoch   1/100 Batch  960/14303 - Loss:  0.258, Seconds: 5.72\n",
            "Epoch   1/100 Batch  990/14303 - Loss:  0.260, Seconds: 5.79\n",
            "Epoch   1/100 Batch 1020/14303 - Loss:  0.268, Seconds: 5.76\n",
            "Epoch   1/100 Batch 1050/14303 - Loss:  0.257, Seconds: 5.74\n",
            "Epoch   1/100 Batch 1080/14303 - Loss:  0.261, Seconds: 5.71\n",
            "Epoch   1/100 Batch 1110/14303 - Loss:  0.243, Seconds: 5.75\n",
            "Epoch   1/100 Batch 1140/14303 - Loss:  0.243, Seconds: 5.75\n",
            "Epoch   1/100 Batch 1170/14303 - Loss:  0.248, Seconds: 5.70\n",
            "Epoch   1/100 Batch 1200/14303 - Loss:  0.239, Seconds: 5.66\n",
            "Epoch   1/100 Batch 1230/14303 - Loss:  0.247, Seconds: 5.72\n",
            "Epoch   1/100 Batch 1260/14303 - Loss:  0.233, Seconds: 5.90\n",
            "Epoch   1/100 Batch 1290/14303 - Loss:  0.232, Seconds: 5.93\n",
            "Epoch   1/100 Batch 1320/14303 - Loss:  0.229, Seconds: 6.07\n",
            "Epoch   1/100 Batch 1350/14303 - Loss:  0.232, Seconds: 5.97\n",
            "Epoch   1/100 Batch 1380/14303 - Loss:  0.225, Seconds: 5.91\n",
            "Epoch   1/100 Batch 1410/14303 - Loss:  0.227, Seconds: 5.91\n",
            "Epoch   1/100 Batch 1440/14303 - Loss:  0.226, Seconds: 5.91\n",
            "Epoch   1/100 Batch 1470/14303 - Loss:  0.231, Seconds: 5.90\n",
            "Epoch   1/100 Batch 1500/14303 - Loss:  0.219, Seconds: 5.91\n",
            "Epoch   1/100 Batch 1530/14303 - Loss:  0.222, Seconds: 5.85\n",
            "Epoch   1/100 Batch 1560/14303 - Loss:  0.231, Seconds: 5.99\n",
            "Epoch   1/100 Batch 1590/14303 - Loss:  0.221, Seconds: 6.08\n",
            "Epoch   1/100 Batch 1620/14303 - Loss:  0.212, Seconds: 6.07\n",
            "Epoch   1/100 Batch 1650/14303 - Loss:  0.215, Seconds: 6.06\n",
            "Epoch   1/100 Batch 1680/14303 - Loss:  0.214, Seconds: 6.21\n",
            "Epoch   1/100 Batch 1710/14303 - Loss:  0.212, Seconds: 6.11\n",
            "Epoch   1/100 Batch 1740/14303 - Loss:  0.207, Seconds: 6.12\n",
            "Epoch   1/100 Batch 1770/14303 - Loss:  0.209, Seconds: 6.16\n",
            "Epoch   1/100 Batch 1800/14303 - Loss:  0.207, Seconds: 6.12\n",
            "Epoch   1/100 Batch 1830/14303 - Loss:  0.205, Seconds: 6.06\n",
            "Epoch   1/100 Batch 1860/14303 - Loss:  0.207, Seconds: 6.07\n",
            "Epoch   1/100 Batch 1890/14303 - Loss:  0.214, Seconds: 6.22\n",
            "Epoch   1/100 Batch 1920/14303 - Loss:  0.211, Seconds: 6.28\n",
            "Epoch   1/100 Batch 1950/14303 - Loss:  0.204, Seconds: 6.23\n",
            "Epoch   1/100 Batch 1980/14303 - Loss:  0.198, Seconds: 6.21\n",
            "Epoch   1/100 Batch 2010/14303 - Loss:  0.200, Seconds: 6.23\n",
            "Epoch   1/100 Batch 2040/14303 - Loss:  0.200, Seconds: 6.21\n",
            "Epoch   1/100 Batch 2070/14303 - Loss:  0.198, Seconds: 6.21\n",
            "Epoch   1/100 Batch 2100/14303 - Loss:  0.202, Seconds: 6.24\n",
            "Epoch   1/100 Batch 2130/14303 - Loss:  0.197, Seconds: 6.27\n",
            "Epoch   1/100 Batch 2160/14303 - Loss:  0.194, Seconds: 6.22\n",
            "Epoch   1/100 Batch 2190/14303 - Loss:  0.198, Seconds: 6.24\n",
            "Epoch   1/100 Batch 2220/14303 - Loss:  0.200, Seconds: 6.44\n",
            "Epoch   1/100 Batch 2250/14303 - Loss:  0.203, Seconds: 6.40\n",
            "Epoch   1/100 Batch 2280/14303 - Loss:  0.196, Seconds: 6.38\n",
            "Epoch   1/100 Batch 2310/14303 - Loss:  0.201, Seconds: 6.37\n",
            "Epoch   1/100 Batch 2340/14303 - Loss:  0.205, Seconds: 6.36\n",
            "Epoch   1/100 Batch 2370/14303 - Loss:  0.185, Seconds: 6.38\n",
            "Epoch   1/100 Batch 2400/14303 - Loss:  0.188, Seconds: 6.41\n",
            "Epoch   1/100 Batch 2430/14303 - Loss:  0.181, Seconds: 6.36\n",
            "Epoch   1/100 Batch 2460/14303 - Loss:  0.192, Seconds: 6.41\n",
            "Epoch   1/100 Batch 2490/14303 - Loss:  0.188, Seconds: 6.48\n",
            "Epoch   1/100 Batch 2520/14303 - Loss:  0.583, Seconds: 6.48\n",
            "Epoch   1/100 Batch 2550/14303 - Loss:  0.342, Seconds: 6.53\n",
            "Epoch   1/100 Batch 2580/14303 - Loss:  0.251, Seconds: 6.49\n",
            "Epoch   1/100 Batch 2610/14303 - Loss:  0.228, Seconds: 6.54\n",
            "Epoch   1/100 Batch 2640/14303 - Loss:  0.213, Seconds: 6.61\n",
            "Epoch   1/100 Batch 2670/14303 - Loss:  0.209, Seconds: 6.53\n",
            "Epoch   1/100 Batch 2700/14303 - Loss:  0.199, Seconds: 6.65\n",
            "Epoch   1/100 Batch 2730/14303 - Loss:  0.200, Seconds: 6.73\n",
            "Epoch   1/100 Batch 2760/14303 - Loss:  0.193, Seconds: 6.58\n",
            "Epoch   1/100 Batch 2790/14303 - Loss:  0.195, Seconds: 6.52\n",
            "Epoch   1/100 Batch 2820/14303 - Loss:  0.194, Seconds: 6.59\n",
            "Epoch   1/100 Batch 2850/14303 - Loss:  0.197, Seconds: 6.76\n",
            "Epoch   1/100 Batch 2880/14303 - Loss:  0.190, Seconds: 6.69\n",
            "Epoch   1/100 Batch 2910/14303 - Loss:  0.185, Seconds: 6.66\n",
            "Epoch   1/100 Batch 2940/14303 - Loss:  0.184, Seconds: 6.73\n",
            "Epoch   1/100 Batch 2970/14303 - Loss:  0.183, Seconds: 6.75\n",
            "Epoch   1/100 Batch 3000/14303 - Loss:  0.181, Seconds: 6.79\n",
            "Epoch   1/100 Batch 3030/14303 - Loss:  0.185, Seconds: 6.84\n",
            "Epoch   1/100 Batch 3060/14303 - Loss:  0.176, Seconds: 6.68\n",
            "Epoch   1/100 Batch 3090/14303 - Loss:  0.177, Seconds: 6.68\n",
            "Epoch   1/100 Batch 3120/14303 - Loss:  0.181, Seconds: 6.68\n",
            "Epoch   1/100 Batch 3150/14303 - Loss:  0.173, Seconds: 6.71\n",
            "Epoch   1/100 Batch 3180/14303 - Loss:  0.183, Seconds: 6.90\n",
            "Epoch   1/100 Batch 3210/14303 - Loss:  0.183, Seconds: 6.86\n",
            "Epoch   1/100 Batch 3240/14303 - Loss:  0.177, Seconds: 6.78\n",
            "Epoch   1/100 Batch 3270/14303 - Loss:  0.172, Seconds: 6.86\n",
            "Epoch   1/100 Batch 3300/14303 - Loss:  0.172, Seconds: 6.83\n",
            "Epoch   1/100 Batch 3330/14303 - Loss:  0.171, Seconds: 6.84\n",
            "Epoch   1/100 Batch 3360/14303 - Loss:  0.183, Seconds: 6.83\n",
            "Epoch   1/100 Batch 3390/14303 - Loss:  0.174, Seconds: 6.84\n",
            "Epoch   1/100 Batch 3420/14303 - Loss:  0.167, Seconds: 6.83\n",
            "Epoch   1/100 Batch 3450/14303 - Loss:  0.164, Seconds: 6.84\n",
            "Epoch   1/100 Batch 3480/14303 - Loss:  0.171, Seconds: 6.97\n",
            "Epoch   1/100 Batch 3510/14303 - Loss:  0.175, Seconds: 7.05\n",
            "Epoch   1/100 Batch 3540/14303 - Loss:  0.170, Seconds: 7.05\n",
            "Epoch   1/100 Batch 3570/14303 - Loss:  0.168, Seconds: 7.07\n",
            "Epoch   1/100 Batch 3600/14303 - Loss:  0.169, Seconds: 7.02\n",
            "Epoch   1/100 Batch 3630/14303 - Loss:  0.167, Seconds: 7.02\n",
            "Epoch   1/100 Batch 3660/14303 - Loss:  0.165, Seconds: 7.00\n",
            "Epoch   1/100 Batch 3690/14303 - Loss:  0.166, Seconds: 6.98\n",
            "Epoch   1/100 Batch 3720/14303 - Loss:  0.165, Seconds: 7.00\n",
            "Epoch   1/100 Batch 3750/14303 - Loss:  0.161, Seconds: 7.01\n",
            "Epoch   1/100 Batch 3780/14303 - Loss:  0.173, Seconds: 7.04\n",
            "Epoch   1/100 Batch 3810/14303 - Loss:  0.173, Seconds: 7.17\n",
            "Epoch   1/100 Batch 3840/14303 - Loss:  0.164, Seconds: 7.22\n",
            "Epoch   1/100 Batch 3870/14303 - Loss:  0.162, Seconds: 7.14\n",
            "Epoch   1/100 Batch 3900/14303 - Loss:  0.160, Seconds: 7.19\n",
            "Epoch   1/100 Batch 3930/14303 - Loss:  0.167, Seconds: 7.19\n",
            "Epoch   1/100 Batch 3960/14303 - Loss:  0.166, Seconds: 7.23\n",
            "Epoch   1/100 Batch 3990/14303 - Loss:  0.165, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4020/14303 - Loss:  0.165, Seconds: 7.17\n",
            "Epoch   1/100 Batch 4050/14303 - Loss:  0.160, Seconds: 7.20\n",
            "Epoch   1/100 Batch 4080/14303 - Loss:  0.163, Seconds: 7.15\n",
            "Epoch   1/100 Batch 4110/14303 - Loss:  0.162, Seconds: 7.19\n",
            "Epoch   1/100 Batch 4140/14303 - Loss:  0.171, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4170/14303 - Loss:  0.156, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4200/14303 - Loss:  0.157, Seconds: 7.38\n",
            "Epoch   1/100 Batch 4230/14303 - Loss:  0.156, Seconds: 7.39\n",
            "Epoch   1/100 Batch 4260/14303 - Loss:  0.159, Seconds: 7.39\n",
            "Epoch   1/100 Batch 4290/14303 - Loss:  0.152, Seconds: 7.36\n",
            "Epoch   1/100 Batch 4320/14303 - Loss:  0.161, Seconds: 7.36\n",
            "Epoch   1/100 Batch 4350/14303 - Loss:  0.159, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4380/14303 - Loss:  0.162, Seconds: 7.36\n",
            "Epoch   1/100 Batch 4410/14303 - Loss:  0.160, Seconds: 7.37\n",
            "Epoch   1/100 Batch 4440/14303 - Loss:  0.161, Seconds: 7.54\n",
            "Epoch   1/100 Batch 4470/14303 - Loss:  0.161, Seconds: 7.49\n",
            "Epoch   1/100 Batch 4500/14303 - Loss:  0.159, Seconds: 7.54\n",
            "Epoch   1/100 Batch 4530/14303 - Loss:  0.154, Seconds: 7.50\n",
            "Epoch   1/100 Batch 4560/14303 - Loss:  0.158, Seconds: 7.50\n",
            "Epoch   1/100 Batch 4590/14303 - Loss:  0.152, Seconds: 7.51\n",
            "Epoch   1/100 Batch 4620/14303 - Loss:  0.155, Seconds: 7.54\n",
            "Epoch   1/100 Batch 4650/14303 - Loss:  0.160, Seconds: 7.52\n",
            "Epoch   1/100 Batch 4680/14303 - Loss:  0.157, Seconds: 7.55\n",
            "Epoch   1/100 Batch 4710/14303 - Loss:  0.156, Seconds: 7.53\n",
            "Epoch   1/100 Batch 4740/14303 - Loss:  0.160, Seconds: 7.63\n",
            "Epoch   1/100 Batch 4770/14303 - Loss:  0.157, Seconds: 7.67\n",
            "Epoch   1/100 Batch 4800/14303 - Loss:  0.151, Seconds: 7.63\n",
            "Epoch   1/100 Batch 4830/14303 - Loss:  0.154, Seconds: 7.67\n",
            "Epoch   1/100 Batch 4860/14303 - Loss:  0.152, Seconds: 7.68\n",
            "Epoch   1/100 Batch 4890/14303 - Loss:  0.150, Seconds: 7.65\n",
            "Epoch   1/100 Batch 4920/14303 - Loss:  0.152, Seconds: 7.63\n",
            "Epoch   1/100 Batch 4950/14303 - Loss:  0.157, Seconds: 7.67\n",
            "Epoch   1/100 Batch 4980/14303 - Loss:  0.151, Seconds: 7.65\n",
            "Epoch   1/100 Batch 5010/14303 - Loss:  0.147, Seconds: 7.63\n",
            "Epoch   1/100 Batch 5040/14303 - Loss:  0.151, Seconds: 7.75\n",
            "Epoch   1/100 Batch 5070/14303 - Loss:  0.155, Seconds: 7.82\n",
            "Epoch   1/100 Batch 5100/14303 - Loss:  0.159, Seconds: 7.86\n",
            "Epoch   1/100 Batch 5130/14303 - Loss:  0.156, Seconds: 7.94\n",
            "Epoch   1/100 Batch 5160/14303 - Loss:  0.146, Seconds: 7.89\n",
            "Epoch   1/100 Batch 5190/14303 - Loss:  0.147, Seconds: 7.83\n",
            "Epoch   1/100 Batch 5220/14303 - Loss:  0.150, Seconds: 7.82\n",
            "Epoch   1/100 Batch 5250/14303 - Loss:  0.149, Seconds: 7.81\n",
            "Epoch   1/100 Batch 5280/14303 - Loss:  0.152, Seconds: 7.81\n",
            "Epoch   1/100 Batch 5310/14303 - Loss:  0.147, Seconds: 7.93\n",
            "Epoch   1/100 Batch 5340/14303 - Loss:  0.148, Seconds: 7.86\n",
            "Epoch   1/100 Batch 5370/14303 - Loss:  0.156, Seconds: 8.11\n",
            "Epoch   1/100 Batch 5400/14303 - Loss:  0.149, Seconds: 8.06\n",
            "Epoch   1/100 Batch 5430/14303 - Loss:  0.144, Seconds: 8.02\n",
            "Epoch   1/100 Batch 5460/14303 - Loss:  0.142, Seconds: 7.98\n",
            "Epoch   1/100 Batch 5490/14303 - Loss:  0.153, Seconds: 7.98\n",
            "Epoch   1/100 Batch 5520/14303 - Loss:  0.147, Seconds: 7.98\n",
            "Epoch   1/100 Batch 5550/14303 - Loss:  0.147, Seconds: 8.02\n",
            "Epoch   1/100 Batch 5580/14303 - Loss:  0.138, Seconds: 8.05\n",
            "Epoch   1/100 Batch 5610/14303 - Loss:  0.144, Seconds: 8.08\n",
            "Epoch   1/100 Batch 5640/14303 - Loss:  0.144, Seconds: 7.95\n",
            "Epoch   1/100 Batch 5670/14303 - Loss:  0.144, Seconds: 8.14\n",
            "Epoch   1/100 Batch 5700/14303 - Loss:  0.144, Seconds: 8.18\n",
            "Epoch   1/100 Batch 5730/14303 - Loss:  0.144, Seconds: 8.17\n",
            "Epoch   1/100 Batch 5760/14303 - Loss:  0.147, Seconds: 8.17\n",
            "Epoch   1/100 Batch 5790/14303 - Loss:  0.141, Seconds: 8.16\n",
            "Epoch   1/100 Batch 5820/14303 - Loss:  0.146, Seconds: 8.19\n",
            "Epoch   1/100 Batch 5850/14303 - Loss:  0.140, Seconds: 8.15\n",
            "Epoch   1/100 Batch 5880/14303 - Loss:  0.143, Seconds: 8.28\n",
            "Epoch   1/100 Batch 5910/14303 - Loss:  0.143, Seconds: 8.10\n",
            "Epoch   1/100 Batch 5940/14303 - Loss:  0.142, Seconds: 8.20\n",
            "Epoch   1/100 Batch 5970/14303 - Loss:  0.144, Seconds: 8.36\n",
            "Epoch   1/100 Batch 6000/14303 - Loss:  0.142, Seconds: 8.32\n",
            "Epoch   1/100 Batch 6030/14303 - Loss:  0.145, Seconds: 8.32\n",
            "Epoch   1/100 Batch 6060/14303 - Loss:  0.145, Seconds: 8.34\n",
            "Epoch   1/100 Batch 6090/14303 - Loss:  0.142, Seconds: 8.35\n",
            "Epoch   1/100 Batch 6120/14303 - Loss:  0.145, Seconds: 8.36\n",
            "Epoch   1/100 Batch 6150/14303 - Loss:  0.140, Seconds: 8.40\n",
            "Epoch   1/100 Batch 6180/14303 - Loss:  0.141, Seconds: 8.38\n",
            "Epoch   1/100 Batch 6210/14303 - Loss:  0.147, Seconds: 8.49\n",
            "Epoch   1/100 Batch 6240/14303 - Loss:  0.147, Seconds: 8.49\n",
            "Epoch   1/100 Batch 6270/14303 - Loss:  0.144, Seconds: 8.55\n",
            "Epoch   1/100 Batch 6300/14303 - Loss:  0.137, Seconds: 8.50\n",
            "Epoch   1/100 Batch 6330/14303 - Loss:  0.143, Seconds: 8.50\n",
            "Epoch   1/100 Batch 6360/14303 - Loss:  0.134, Seconds: 8.51\n",
            "Epoch   1/100 Batch 6390/14303 - Loss:  0.138, Seconds: 8.49\n",
            "Epoch   1/100 Batch 6420/14303 - Loss:  0.138, Seconds: 8.60\n",
            "Epoch   1/100 Batch 6450/14303 - Loss:  0.135, Seconds: 8.48\n",
            "Epoch   1/100 Batch 6480/14303 - Loss:  0.138, Seconds: 8.50\n",
            "Epoch   1/100 Batch 6510/14303 - Loss:  0.137, Seconds: 8.48\n",
            "Epoch   1/100 Batch 6540/14303 - Loss:  0.134, Seconds: 8.69\n",
            "Epoch   1/100 Batch 6570/14303 - Loss:  0.143, Seconds: 8.65\n",
            "Epoch   1/100 Batch 6600/14303 - Loss:  0.146, Seconds: 8.64\n",
            "Epoch   1/100 Batch 6630/14303 - Loss:  0.139, Seconds: 8.68\n",
            "Epoch   1/100 Batch 6660/14303 - Loss:  0.137, Seconds: 8.71\n",
            "Epoch   1/100 Batch 6690/14303 - Loss:  0.137, Seconds: 8.71\n",
            "Epoch   1/100 Batch 6720/14303 - Loss:  0.134, Seconds: 8.63\n",
            "Epoch   1/100 Batch 6750/14303 - Loss:  0.138, Seconds: 8.64\n",
            "Epoch   1/100 Batch 6780/14303 - Loss:  0.133, Seconds: 8.69\n",
            "Epoch   1/100 Batch 6810/14303 - Loss:  0.136, Seconds: 8.80\n",
            "Epoch   1/100 Batch 6840/14303 - Loss:  0.132, Seconds: 8.92\n",
            "Epoch   1/100 Batch 6870/14303 - Loss:  0.131, Seconds: 8.94\n",
            "Epoch   1/100 Batch 6900/14303 - Loss:  0.136, Seconds: 8.91\n",
            "Epoch   1/100 Batch 6930/14303 - Loss:  0.132, Seconds: 8.96\n",
            "Epoch   1/100 Batch 6960/14303 - Loss:  0.134, Seconds: 8.94\n",
            "Epoch   1/100 Batch 6990/14303 - Loss:  0.132, Seconds: 8.86\n",
            "Epoch   1/100 Batch 7020/14303 - Loss:  0.140, Seconds: 8.95\n",
            "Epoch   1/100 Batch 7050/14303 - Loss:  0.133, Seconds: 8.91\n",
            "Epoch   1/100 Batch 7080/14303 - Loss:  0.135, Seconds: 8.97\n",
            "Epoch   1/100 Batch 7110/14303 - Loss:  0.132, Seconds: 9.09\n",
            "Epoch   1/100 Batch 7140/14303 - Loss:  0.135, Seconds: 9.07\n",
            "Epoch   1/100 Batch 7170/14303 - Loss:  0.130, Seconds: 9.08\n",
            "Epoch   1/100 Batch 7200/14303 - Loss:  0.132, Seconds: 9.28\n",
            "Epoch   1/100 Batch 7230/14303 - Loss:  0.134, Seconds: 9.20\n",
            "Epoch   1/100 Batch 7260/14303 - Loss:  0.127, Seconds: 8.99\n",
            "Epoch   1/100 Batch 7290/14303 - Loss:  0.129, Seconds: 9.03\n",
            "Epoch   1/100 Batch 7320/14303 - Loss:  0.133, Seconds: 9.02\n",
            "Epoch   1/100 Batch 7350/14303 - Loss:  0.129, Seconds: 9.14\n",
            "Epoch   1/100 Batch 7380/14303 - Loss:  0.130, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7410/14303 - Loss:  0.133, Seconds: 9.23\n",
            "Epoch   1/100 Batch 7440/14303 - Loss:  0.135, Seconds: 9.33\n",
            "Epoch   1/100 Batch 7470/14303 - Loss:  0.130, Seconds: 9.22\n",
            "Epoch   1/100 Batch 7500/14303 - Loss:  0.127, Seconds: 9.23\n",
            "Epoch   1/100 Batch 7530/14303 - Loss:  0.123, Seconds: 9.20\n",
            "Epoch   1/100 Batch 7560/14303 - Loss:  0.132, Seconds: 9.20\n",
            "Epoch   1/100 Batch 7590/14303 - Loss:  0.130, Seconds: 9.25\n",
            "Epoch   1/100 Batch 7620/14303 - Loss:  0.134, Seconds: 9.29\n",
            "Epoch   1/100 Batch 7650/14303 - Loss:  0.134, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7680/14303 - Loss:  0.130, Seconds: 9.42\n",
            "Epoch   1/100 Batch 7710/14303 - Loss:  0.131, Seconds: 9.40\n",
            "Epoch   1/100 Batch 7740/14303 - Loss:  0.132, Seconds: 9.36\n",
            "Epoch   1/100 Batch 7770/14303 - Loss:  0.130, Seconds: 9.36\n",
            "Epoch   1/100 Batch 7800/14303 - Loss:  0.131, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7830/14303 - Loss:  0.128, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7860/14303 - Loss:  0.128, Seconds: 9.34\n",
            "Epoch   1/100 Batch 7890/14303 - Loss:  0.124, Seconds: 9.48\n",
            "Epoch   1/100 Batch 7920/14303 - Loss:  0.130, Seconds: 9.57\n",
            "Epoch   1/100 Batch 7950/14303 - Loss:  0.126, Seconds: 9.54\n",
            "Epoch   1/100 Batch 7980/14303 - Loss:  0.131, Seconds: 9.55\n",
            "Epoch   1/100 Batch 8010/14303 - Loss:  0.126, Seconds: 9.52\n",
            "Epoch   1/100 Batch 8040/14303 - Loss:  0.130, Seconds: 9.50\n",
            "Epoch   1/100 Batch 8070/14303 - Loss:  0.121, Seconds: 9.52\n",
            "Epoch   1/100 Batch 8100/14303 - Loss:  0.124, Seconds: 9.53\n",
            "Epoch   1/100 Batch 8130/14303 - Loss:  0.124, Seconds: 9.79\n",
            "Epoch   1/100 Batch 8160/14303 - Loss:  0.125, Seconds: 9.80\n",
            "Epoch   1/100 Batch 8190/14303 - Loss:  0.121, Seconds: 9.68\n",
            "Epoch   1/100 Batch 8220/14303 - Loss:  0.128, Seconds: 9.63\n",
            "Epoch   1/100 Batch 8250/14303 - Loss:  0.127, Seconds: 9.69\n",
            "Epoch   1/100 Batch 8280/14303 - Loss:  0.128, Seconds: 9.82\n",
            "Epoch   1/100 Batch 8310/14303 - Loss:  0.121, Seconds: 9.71\n",
            "Epoch   1/100 Batch 8340/14303 - Loss:  0.125, Seconds: 9.70\n",
            "Epoch   1/100 Batch 8370/14303 - Loss:  0.130, Seconds: 9.86\n",
            "Epoch   1/100 Batch 8400/14303 - Loss:  0.125, Seconds: 9.91\n",
            "Epoch   1/100 Batch 8430/14303 - Loss:  0.126, Seconds: 9.83\n",
            "Epoch   1/100 Batch 8460/14303 - Loss:  0.127, Seconds: 9.87\n",
            "Epoch   1/100 Batch 8490/14303 - Loss:  0.121, Seconds: 9.83\n",
            "Epoch   1/100 Batch 8520/14303 - Loss:  0.122, Seconds: 9.87\n",
            "Epoch   1/100 Batch 8550/14303 - Loss:  0.123, Seconds: 9.81\n",
            "Epoch   1/100 Batch 8580/14303 - Loss:  0.122, Seconds: 9.85\n",
            "Epoch   1/100 Batch 8610/14303 - Loss:  0.129, Seconds: 10.09\n",
            "Epoch   1/100 Batch 8640/14303 - Loss:  0.120, Seconds: 10.04\n",
            "Epoch   1/100 Batch 8670/14303 - Loss:  0.124, Seconds: 10.07\n",
            "Epoch   1/100 Batch 8700/14303 - Loss:  0.117, Seconds: 10.00\n",
            "Epoch   1/100 Batch 8730/14303 - Loss:  0.123, Seconds: 10.02\n",
            "Epoch   1/100 Batch 8760/14303 - Loss:  0.121, Seconds: 10.02\n",
            "Epoch   1/100 Batch 8790/14303 - Loss:  0.123, Seconds: 10.06\n",
            "Epoch   1/100 Batch 8820/14303 - Loss:  0.124, Seconds: 10.13\n",
            "Epoch   1/100 Batch 8850/14303 - Loss:  0.125, Seconds: 10.26\n",
            "Epoch   1/100 Batch 8880/14303 - Loss:  0.121, Seconds: 10.21\n",
            "Epoch   1/100 Batch 8910/14303 - Loss:  0.121, Seconds: 10.23\n",
            "Epoch   1/100 Batch 8940/14303 - Loss:  0.122, Seconds: 10.21\n",
            "Epoch   1/100 Batch 8970/14303 - Loss:  0.120, Seconds: 10.23\n",
            "Epoch   1/100 Batch 9000/14303 - Loss:  0.120, Seconds: 10.23\n",
            "Epoch   1/100 Batch 9030/14303 - Loss:  0.126, Seconds: 10.37\n",
            "Epoch   1/100 Batch 9060/14303 - Loss:  0.120, Seconds: 10.41\n",
            "Epoch   1/100 Batch 9090/14303 - Loss:  0.121, Seconds: 10.41\n",
            "Epoch   1/100 Batch 9120/14303 - Loss:  0.122, Seconds: 10.39\n",
            "Epoch   1/100 Batch 9150/14303 - Loss:  0.119, Seconds: 10.50\n",
            "Epoch   1/100 Batch 9180/14303 - Loss:  0.119, Seconds: 10.37\n",
            "Epoch   1/100 Batch 9210/14303 - Loss:  0.119, Seconds: 10.28\n",
            "Epoch   1/100 Batch 9240/14303 - Loss:  0.117, Seconds: 10.34\n",
            "Epoch   1/100 Batch 9270/14303 - Loss:  0.118, Seconds: 10.51\n",
            "Epoch   1/100 Batch 9300/14303 - Loss:  0.116, Seconds: 10.59\n",
            "Epoch   1/100 Batch 9330/14303 - Loss:  0.117, Seconds: 10.58\n",
            "Epoch   1/100 Batch 9360/14303 - Loss:  0.121, Seconds: 10.53\n",
            "Epoch   1/100 Batch 9390/14303 - Loss:  0.125, Seconds: 10.49\n",
            "Epoch   1/100 Batch 9420/14303 - Loss:  0.114, Seconds: 10.56\n",
            "Epoch   1/100 Batch 9450/14303 - Loss:  0.121, Seconds: 10.56\n",
            "Epoch   1/100 Batch 9480/14303 - Loss:  0.121, Seconds: 10.65\n",
            "Epoch   1/100 Batch 9510/14303 - Loss:  0.123, Seconds: 10.76\n",
            "Epoch   1/100 Batch 9540/14303 - Loss:  0.117, Seconds: 10.67\n",
            "Epoch   1/100 Batch 9570/14303 - Loss:  0.119, Seconds: 10.64\n",
            "Epoch   1/100 Batch 9600/14303 - Loss:  0.115, Seconds: 10.67\n",
            "Epoch   1/100 Batch 9630/14303 - Loss:  0.120, Seconds: 10.71\n",
            "Epoch   1/100 Batch 9660/14303 - Loss:  0.117, Seconds: 10.71\n",
            "Epoch   1/100 Batch 9690/14303 - Loss:  0.119, Seconds: 10.91\n",
            "Epoch   1/100 Batch 9720/14303 - Loss:  0.119, Seconds: 10.95\n",
            "Epoch   1/100 Batch 9750/14303 - Loss:  0.115, Seconds: 10.89\n",
            "Epoch   1/100 Batch 9780/14303 - Loss:  0.120, Seconds: 10.91\n",
            "Epoch   1/100 Batch 9810/14303 - Loss:  0.116, Seconds: 10.86\n",
            "Epoch   1/100 Batch 9840/14303 - Loss:  0.117, Seconds: 10.99\n",
            "Epoch   1/100 Batch 9870/14303 - Loss:  0.124, Seconds: 11.03\n",
            "Epoch   1/100 Batch 9900/14303 - Loss:  0.138, Seconds: 11.04\n",
            "Epoch   1/100 Batch 9930/14303 - Loss:  0.133, Seconds: 11.13\n",
            "Epoch   1/100 Batch 9960/14303 - Loss:  0.124, Seconds: 11.14\n",
            "Epoch   1/100 Batch 9990/14303 - Loss:  0.126, Seconds: 11.07\n",
            "Epoch   1/100 Batch 10020/14303 - Loss:  0.123, Seconds: 11.04\n",
            "Epoch   1/100 Batch 10050/14303 - Loss:  0.120, Seconds: 11.10\n",
            "Epoch   1/100 Batch 10080/14303 - Loss:  0.120, Seconds: 11.23\n",
            "Epoch   1/100 Batch 10110/14303 - Loss:  0.117, Seconds: 11.26\n",
            "Epoch   1/100 Batch 10140/14303 - Loss:  0.117, Seconds: 11.23\n",
            "Epoch   1/100 Batch 10170/14303 - Loss:  0.113, Seconds: 11.24\n",
            "Epoch   1/100 Batch 10200/14303 - Loss:  0.118, Seconds: 11.21\n",
            "Epoch   1/100 Batch 10230/14303 - Loss:  0.113, Seconds: 11.28\n",
            "Epoch   1/100 Batch 10260/14303 - Loss:  0.115, Seconds: 11.44\n",
            "Epoch   1/100 Batch 10290/14303 - Loss:  0.119, Seconds: 11.38\n",
            "Epoch   1/100 Batch 10320/14303 - Loss:  0.112, Seconds: 11.44\n",
            "Epoch   1/100 Batch 10350/14303 - Loss:  0.113, Seconds: 11.44\n",
            "Epoch   1/100 Batch 10380/14303 - Loss:  0.111, Seconds: 11.40\n",
            "Epoch   1/100 Batch 10410/14303 - Loss:  0.116, Seconds: 11.45\n",
            "Epoch   1/100 Batch 10440/14303 - Loss:  0.114, Seconds: 11.52\n",
            "Epoch   1/100 Batch 10470/14303 - Loss:  0.116, Seconds: 11.56\n",
            "Epoch   1/100 Batch 10500/14303 - Loss:  0.114, Seconds: 11.59\n",
            "Epoch   1/100 Batch 10530/14303 - Loss:  0.113, Seconds: 11.66\n",
            "Epoch   1/100 Batch 10560/14303 - Loss:  0.117, Seconds: 11.57\n",
            "Epoch   1/100 Batch 10590/14303 - Loss:  0.117, Seconds: 11.65\n",
            "Epoch   1/100 Batch 10620/14303 - Loss:  0.113, Seconds: 11.90\n",
            "Epoch   1/100 Batch 10650/14303 - Loss:  0.115, Seconds: 11.75\n",
            "Epoch   1/100 Batch 10680/14303 - Loss:  0.113, Seconds: 11.79\n",
            "Epoch   1/100 Batch 10710/14303 - Loss:  0.112, Seconds: 11.84\n",
            "Epoch   1/100 Batch 10740/14303 - Loss:  0.113, Seconds: 11.75\n",
            "Epoch   1/100 Batch 10770/14303 - Loss:  0.113, Seconds: 11.94\n",
            "Epoch   1/100 Batch 10800/14303 - Loss:  0.112, Seconds: 11.93\n",
            "Epoch   1/100 Batch 10830/14303 - Loss:  0.107, Seconds: 11.90\n",
            "Epoch   1/100 Batch 10860/14303 - Loss:  0.113, Seconds: 11.88\n",
            "Epoch   1/100 Batch 10890/14303 - Loss:  0.113, Seconds: 11.94\n",
            "Epoch   1/100 Batch 10920/14303 - Loss:  0.114, Seconds: 12.14\n",
            "Epoch   1/100 Batch 10950/14303 - Loss:  0.110, Seconds: 12.09\n",
            "Epoch   1/100 Batch 10980/14303 - Loss:  0.111, Seconds: 12.08\n",
            "Epoch   1/100 Batch 11010/14303 - Loss:  0.108, Seconds: 12.03\n",
            "Epoch   1/100 Batch 11040/14303 - Loss:  0.107, Seconds: 12.06\n",
            "Epoch   1/100 Batch 11070/14303 - Loss:  0.109, Seconds: 12.22\n",
            "Epoch   1/100 Batch 11100/14303 - Loss:  0.114, Seconds: 12.32\n",
            "Epoch   1/100 Batch 11130/14303 - Loss:  0.115, Seconds: 12.20\n",
            "Epoch   1/100 Batch 11160/14303 - Loss:  0.115, Seconds: 12.29\n",
            "Epoch   1/100 Batch 11190/14303 - Loss:  0.109, Seconds: 12.24\n",
            "Epoch   1/100 Batch 11220/14303 - Loss:  0.112, Seconds: 12.42\n",
            "Epoch   1/100 Batch 11250/14303 - Loss:  0.111, Seconds: 12.42\n",
            "Epoch   1/100 Batch 11280/14303 - Loss:  0.109, Seconds: 12.56\n",
            "Epoch   1/100 Batch 11310/14303 - Loss:  0.111, Seconds: 12.39\n",
            "Epoch   1/100 Batch 11340/14303 - Loss:  0.107, Seconds: 12.66\n",
            "Epoch   1/100 Batch 11370/14303 - Loss:  0.110, Seconds: 12.60\n",
            "Epoch   1/100 Batch 11400/14303 - Loss:  0.107, Seconds: 12.70\n",
            "Epoch   1/100 Batch 11430/14303 - Loss:  0.107, Seconds: 12.61\n",
            "Epoch   1/100 Batch 11460/14303 - Loss:  0.108, Seconds: 12.74\n",
            "Epoch   1/100 Batch 11490/14303 - Loss:  0.109, Seconds: 12.74\n",
            "Epoch   1/100 Batch 11520/14303 - Loss:  0.112, Seconds: 12.81\n",
            "Epoch   1/100 Batch 11550/14303 - Loss:  0.110, Seconds: 12.76\n",
            "Epoch   1/100 Batch 11580/14303 - Loss:  0.107, Seconds: 12.75\n",
            "Epoch   1/100 Batch 11610/14303 - Loss:  0.110, Seconds: 12.87\n",
            "Epoch   1/100 Batch 11640/14303 - Loss:  0.109, Seconds: 13.09\n",
            "Epoch   1/100 Batch 11670/14303 - Loss:  0.108, Seconds: 12.96\n",
            "Epoch   1/100 Batch 11700/14303 - Loss:  0.110, Seconds: 12.95\n",
            "Epoch   1/100 Batch 11730/14303 - Loss:  0.107, Seconds: 12.97\n",
            "Epoch   1/100 Batch 11760/14303 - Loss:  0.106, Seconds: 13.18\n",
            "Epoch   1/100 Batch 11790/14303 - Loss:  0.108, Seconds: 13.19\n",
            "Epoch   1/100 Batch 11820/14303 - Loss:  0.112, Seconds: 13.14\n",
            "Epoch   1/100 Batch 11850/14303 - Loss:  0.105, Seconds: 13.19\n",
            "Epoch   1/100 Batch 11880/14303 - Loss:  0.109, Seconds: 13.31\n",
            "Epoch   1/100 Batch 11910/14303 - Loss:  0.104, Seconds: 13.28\n",
            "Epoch   1/100 Batch 11940/14303 - Loss:  0.104, Seconds: 13.38\n",
            "Epoch   1/100 Batch 11970/14303 - Loss:  0.104, Seconds: 13.38\n",
            "Epoch   1/100 Batch 12000/14303 - Loss:  0.107, Seconds: 13.65\n",
            "Epoch   1/100 Batch 12030/14303 - Loss:  0.110, Seconds: 13.57\n",
            "Epoch   1/100 Batch 12060/14303 - Loss:  0.106, Seconds: 13.65\n",
            "Epoch   1/100 Batch 12090/14303 - Loss:  0.107, Seconds: 13.61\n",
            "Epoch   1/100 Batch 12120/14303 - Loss:  0.104, Seconds: 13.69\n",
            "Epoch   1/100 Batch 12150/14303 - Loss:  0.110, Seconds: 13.75\n",
            "Epoch   1/100 Batch 12180/14303 - Loss:  0.104, Seconds: 13.69\n",
            "Epoch   1/100 Batch 12210/14303 - Loss:  0.107, Seconds: 13.86\n",
            "Epoch   1/100 Batch 12240/14303 - Loss:  0.107, Seconds: 13.86\n",
            "Epoch   1/100 Batch 12270/14303 - Loss:  0.106, Seconds: 13.85\n",
            "Epoch   1/100 Batch 12300/14303 - Loss:  0.105, Seconds: 14.07\n",
            "Epoch   1/100 Batch 12330/14303 - Loss:  0.106, Seconds: 14.09\n",
            "Epoch   1/100 Batch 12360/14303 - Loss:  0.105, Seconds: 14.04\n",
            "Epoch   1/100 Batch 12390/14303 - Loss:  0.105, Seconds: 14.13\n",
            "Epoch   1/100 Batch 12420/14303 - Loss:  0.103, Seconds: 14.23\n",
            "Epoch   1/100 Batch 12450/14303 - Loss:  0.103, Seconds: 14.25\n",
            "Epoch   1/100 Batch 12480/14303 - Loss:  0.106, Seconds: 14.35\n",
            "Epoch   1/100 Batch 12510/14303 - Loss:  0.104, Seconds: 14.42\n",
            "Epoch   1/100 Batch 12540/14303 - Loss:  0.105, Seconds: 14.40\n",
            "Epoch   1/100 Batch 12570/14303 - Loss:  0.103, Seconds: 14.38\n",
            "Epoch   1/100 Batch 12600/14303 - Loss:  0.103, Seconds: 14.54\n",
            "Epoch   1/100 Batch 12630/14303 - Loss:  0.105, Seconds: 14.80\n",
            "Epoch   1/100 Batch 12660/14303 - Loss:  0.105, Seconds: 14.74\n",
            "Epoch   1/100 Batch 12690/14303 - Loss:  0.101, Seconds: 14.84\n",
            "Epoch   1/100 Batch 12720/14303 - Loss:  0.103, Seconds: 14.76\n",
            "Epoch   1/100 Batch 12750/14303 - Loss:  0.104, Seconds: 14.98\n",
            "Epoch   1/100 Batch 12780/14303 - Loss:  0.102, Seconds: 15.05\n",
            "Epoch   1/100 Batch 12810/14303 - Loss:  0.100, Seconds: 15.03\n",
            "Epoch   1/100 Batch 12840/14303 - Loss:  0.102, Seconds: 15.20\n",
            "Epoch   1/100 Batch 12870/14303 - Loss:  0.102, Seconds: 15.15\n",
            "Epoch   1/100 Batch 12900/14303 - Loss:  0.104, Seconds: 15.33\n",
            "Epoch   1/100 Batch 12930/14303 - Loss:  0.102, Seconds: 15.40\n",
            "Epoch   1/100 Batch 12960/14303 - Loss:  0.105, Seconds: 15.44\n",
            "Epoch   1/100 Batch 12990/14303 - Loss:  0.102, Seconds: 15.60\n",
            "Epoch   1/100 Batch 13020/14303 - Loss:  0.106, Seconds: 15.53\n",
            "Epoch   1/100 Batch 13050/14303 - Loss:  0.101, Seconds: 15.74\n",
            "Epoch   1/100 Batch 13080/14303 - Loss:  0.105, Seconds: 15.78\n",
            "Epoch   1/100 Batch 13110/14303 - Loss:  0.101, Seconds: 15.94\n",
            "Epoch   1/100 Batch 13140/14303 - Loss:  0.102, Seconds: 15.98\n",
            "Epoch   1/100 Batch 13170/14303 - Loss:  0.101, Seconds: 16.08\n",
            "Epoch   1/100 Batch 13200/14303 - Loss:  0.099, Seconds: 16.42\n",
            "Epoch   1/100 Batch 13230/14303 - Loss:  0.102, Seconds: 16.50\n",
            "Epoch   1/100 Batch 13260/14303 - Loss:  0.101, Seconds: 16.56\n",
            "Epoch   1/100 Batch 13290/14303 - Loss:  0.100, Seconds: 17.43\n",
            "Epoch   1/100 Batch 13320/14303 - Loss:  0.101, Seconds: 17.12\n",
            "Epoch   1/100 Batch 13350/14303 - Loss:  0.100, Seconds: 17.46\n",
            "Epoch   1/100 Batch 13380/14303 - Loss:  0.103, Seconds: 17.34\n",
            "Epoch   1/100 Batch 13410/14303 - Loss:  0.101, Seconds: 17.78\n",
            "Epoch   1/100 Batch 13440/14303 - Loss:  0.100, Seconds: 17.70\n",
            "Epoch   1/100 Batch 13470/14303 - Loss:  0.099, Seconds: 17.90\n",
            "Epoch   1/100 Batch 13500/14303 - Loss:  0.100, Seconds: 17.88\n",
            "Epoch   1/100 Batch 13530/14303 - Loss:  0.100, Seconds: 17.84\n",
            "Epoch   1/100 Batch 13560/14303 - Loss:  0.098, Seconds: 17.93\n",
            "Epoch   1/100 Batch 13590/14303 - Loss:  0.099, Seconds: 18.24\n",
            "Epoch   1/100 Batch 13620/14303 - Loss:  0.100, Seconds: 18.17\n",
            "Epoch   1/100 Batch 13650/14303 - Loss:  0.101, Seconds: 18.44\n",
            "Epoch   1/100 Batch 13680/14303 - Loss:  0.101, Seconds: 18.61\n",
            "Epoch   1/100 Batch 13710/14303 - Loss:  0.098, Seconds: 19.39\n",
            "Epoch   1/100 Batch 13740/14303 - Loss:  0.099, Seconds: 19.46\n",
            "Epoch   1/100 Batch 13770/14303 - Loss:  0.099, Seconds: 19.60\n",
            "Epoch   1/100 Batch 13800/14303 - Loss:  0.099, Seconds: 19.67\n",
            "Epoch   1/100 Batch 13830/14303 - Loss:  0.291, Seconds: 19.99\n",
            "Epoch   1/100 Batch 13860/14303 - Loss:  0.164, Seconds: 19.90\n",
            "Epoch   1/100 Batch 13890/14303 - Loss:  0.125, Seconds: 20.05\n",
            "Epoch   1/100 Batch 13920/14303 - Loss:  0.118, Seconds: 20.36\n",
            "Epoch   1/100 Batch 13950/14303 - Loss:  0.112, Seconds: 20.74\n",
            "Epoch   1/100 Batch 13980/14303 - Loss:  0.113, Seconds: 20.86\n",
            "Epoch   1/100 Batch 14010/14303 - Loss:  0.108, Seconds: 21.33\n",
            "Epoch   1/100 Batch 14040/14303 - Loss:  0.107, Seconds: 21.57\n",
            "Epoch   1/100 Batch 14070/14303 - Loss:  0.107, Seconds: 22.19\n",
            "Epoch   1/100 Batch 14100/14303 - Loss:  0.107, Seconds: 22.71\n",
            "Epoch   1/100 Batch 14130/14303 - Loss:  0.101, Seconds: 23.46\n",
            "Epoch   1/100 Batch 14160/14303 - Loss:  0.102, Seconds: 23.97\n",
            "Epoch   1/100 Batch 14190/14303 - Loss:  0.102, Seconds: 24.65\n",
            "Epoch   1/100 Batch 14220/14303 - Loss:  0.104, Seconds: 25.66\n",
            "Epoch   1/100 Batch 14250/14303 - Loss:  0.103, Seconds: 27.23\n",
            "Epoch   1/100 Batch 14280/14303 - Loss:  0.103, Seconds: 29.64\n",
            "  Validation Input: no tas bad as one sandalthink\n",
            "  Validation Output: not as bad as one sandalthink\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is aso cajlled granth sahib\n",
            "  Validation Output: it is also called granth sacalle\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is intended for adults onlhy\n",
            "  Validation Output: holdy be it is intended for adult\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reason things out for yourselqf\n",
            "  Validation Output: reason things out for yourself\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: it happenegdt o dwork quite well\n",
            "  Validation Output: it happened to work quite well\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: chf gand i are ding veyrc wll\n",
            "  Validation Output: wll gand i are doing very will\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: ecnter for the aerforming arts\n",
            "  Validation Output: recenter for the are forming art\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: spooninto a hot borwl tos eve\n",
            "  Validation Output: every very spoon into a hot bro\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: peoplem eet hera and they swoo\n",
            "  Validation Output: people meet her and they swore a\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my poor litlte hhaed si boilingr\n",
            "  Validation Output: my poor little had sin boiling\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this had become a commons cecne\n",
            "  Validation Output: this had become a common science\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: een wentthrough the speicals\n",
            "  Validation Output: selects been went through the\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: wprheat a steamer on hxigh heat\n",
            "  Validation Output: wheat a steamer on high heater on\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lol well somene ad to sa it\n",
            "  Validation Output: thell someone and to say it\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he was weaving mats on the glst\n",
            "  Validation Output: the was weaving mats on the glast\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: tis is wrog for many reasons\n",
            "  Validation Output: shows is wrong for many reason\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: today e headed agaito cubaoe\n",
            "  Validation Output: boarden today be headed again\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep an dmember statcus raeuqired\n",
            "  Validation Output: rep and member status required\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: tit snowed midilng hard to day\n",
            "  Validation Output: you day bettity snowed middling\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: thy obt hwear rleather aadnals\n",
            "  Validation Output: they both wear relather and also\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  0.269, Seconds: 0.00\n",
            "New Loss Record!\n",
            "\n",
            "Training Model: 2\n",
            "Epoch   2/100 Batch   30/14303 - Loss:  0.163, Seconds: 5.74\n",
            "Epoch   2/100 Batch   60/14303 - Loss:  0.126, Seconds: 5.71\n",
            "Epoch   2/100 Batch   90/14303 - Loss:  0.116, Seconds: 5.47\n",
            "Epoch   2/100 Batch  120/14303 - Loss:  0.114, Seconds: 5.45\n",
            "Epoch   2/100 Batch  150/14303 - Loss:  0.118, Seconds: 5.42\n",
            "Epoch   2/100 Batch  180/14303 - Loss:  0.111, Seconds: 5.40\n",
            "Epoch   2/100 Batch  210/14303 - Loss:  0.114, Seconds: 5.42\n",
            "Epoch   2/100 Batch  240/14303 - Loss:  0.107, Seconds: 5.37\n",
            "Epoch   2/100 Batch  270/14303 - Loss:  0.112, Seconds: 5.41\n",
            "Epoch   2/100 Batch  300/14303 - Loss:  0.145, Seconds: 5.49\n",
            "Epoch   2/100 Batch  330/14303 - Loss:  0.115, Seconds: 5.59\n",
            "Epoch   2/100 Batch  360/14303 - Loss:  0.109, Seconds: 5.60\n",
            "Epoch   2/100 Batch  390/14303 - Loss:  0.114, Seconds: 5.61\n",
            "Epoch   2/100 Batch  420/14303 - Loss:  0.112, Seconds: 5.67\n",
            "Epoch   2/100 Batch  450/14303 - Loss:  0.106, Seconds: 5.60\n",
            "Epoch   2/100 Batch  480/14303 - Loss:  0.111, Seconds: 5.65\n",
            "Epoch   2/100 Batch  510/14303 - Loss:  0.111, Seconds: 5.60\n",
            "Epoch   2/100 Batch  540/14303 - Loss:  0.108, Seconds: 5.61\n",
            "Epoch   2/100 Batch  570/14303 - Loss:  0.112, Seconds: 5.56\n",
            "Epoch   2/100 Batch  600/14303 - Loss:  0.111, Seconds: 5.67\n",
            "Epoch   2/100 Batch  630/14303 - Loss:  0.105, Seconds: 5.72\n",
            "Epoch   2/100 Batch  660/14303 - Loss:  0.104, Seconds: 5.78\n",
            "Epoch   2/100 Batch  690/14303 - Loss:  0.105, Seconds: 5.83\n",
            "Epoch   2/100 Batch  720/14303 - Loss:  0.102, Seconds: 5.83\n",
            "Epoch   2/100 Batch  750/14303 - Loss:  0.105, Seconds: 5.78\n",
            "Epoch   2/100 Batch  780/14303 - Loss:  0.108, Seconds: 5.75\n",
            "Epoch   2/100 Batch  810/14303 - Loss:  0.104, Seconds: 5.79\n",
            "Epoch   2/100 Batch  840/14303 - Loss:  0.106, Seconds: 5.81\n",
            "Epoch   2/100 Batch  870/14303 - Loss:  0.106, Seconds: 5.76\n",
            "Epoch   2/100 Batch  900/14303 - Loss:  0.099, Seconds: 5.88\n",
            "Epoch   2/100 Batch  930/14303 - Loss:  0.108, Seconds: 6.07\n",
            "Epoch   2/100 Batch  960/14303 - Loss:  0.104, Seconds: 6.02\n",
            "Epoch   2/100 Batch  990/14303 - Loss:  0.108, Seconds: 5.98\n",
            "Epoch   2/100 Batch 1020/14303 - Loss:  0.103, Seconds: 5.95\n",
            "Epoch   2/100 Batch 1050/14303 - Loss:  0.109, Seconds: 5.99\n",
            "Epoch   2/100 Batch 1080/14303 - Loss:  0.101, Seconds: 5.92\n",
            "Epoch   2/100 Batch 1110/14303 - Loss:  0.107, Seconds: 5.95\n",
            "Epoch   2/100 Batch 1140/14303 - Loss:  0.101, Seconds: 5.94\n",
            "Epoch   2/100 Batch 1170/14303 - Loss:  0.101, Seconds: 6.03\n",
            "Epoch   2/100 Batch 1200/14303 - Loss:  0.101, Seconds: 6.06\n",
            "Epoch   2/100 Batch 1230/14303 - Loss:  0.102, Seconds: 6.04\n",
            "Epoch   2/100 Batch 1260/14303 - Loss:  0.103, Seconds: 6.23\n",
            "Epoch   2/100 Batch 1290/14303 - Loss:  0.104, Seconds: 6.17\n",
            "Epoch   2/100 Batch 1320/14303 - Loss:  0.103, Seconds: 6.20\n",
            "Epoch   2/100 Batch 1350/14303 - Loss:  0.099, Seconds: 6.24\n",
            "Epoch   2/100 Batch 1380/14303 - Loss:  0.100, Seconds: 6.23\n",
            "Epoch   2/100 Batch 1410/14303 - Loss:  0.102, Seconds: 6.12\n",
            "Epoch   2/100 Batch 1440/14303 - Loss:  0.107, Seconds: 6.29\n",
            "Epoch   2/100 Batch 1470/14303 - Loss:  0.103, Seconds: 6.22\n",
            "Epoch   2/100 Batch 1500/14303 - Loss:  0.102, Seconds: 6.33\n",
            "Epoch   2/100 Batch 1530/14303 - Loss:  0.103, Seconds: 6.24\n",
            "Epoch   2/100 Batch 1560/14303 - Loss:  0.104, Seconds: 6.34\n",
            "Epoch   2/100 Batch 1590/14303 - Loss:  0.097, Seconds: 6.37\n",
            "Epoch   2/100 Batch 1620/14303 - Loss:  0.099, Seconds: 6.30\n",
            "Epoch   2/100 Batch 1650/14303 - Loss:  0.101, Seconds: 6.32\n",
            "Epoch   2/100 Batch 1680/14303 - Loss:  0.098, Seconds: 6.31\n",
            "Epoch   2/100 Batch 1710/14303 - Loss:  0.098, Seconds: 6.26\n",
            "Epoch   2/100 Batch 1740/14303 - Loss:  0.102, Seconds: 6.24\n",
            "Epoch   2/100 Batch 1770/14303 - Loss:  0.100, Seconds: 6.32\n",
            "Epoch   2/100 Batch 1800/14303 - Loss:  0.100, Seconds: 6.29\n",
            "Epoch   2/100 Batch 1830/14303 - Loss:  0.098, Seconds: 6.26\n",
            "Epoch   2/100 Batch 1860/14303 - Loss:  0.100, Seconds: 6.32\n",
            "Epoch   2/100 Batch 1890/14303 - Loss:  0.101, Seconds: 6.47\n",
            "Epoch   2/100 Batch 1920/14303 - Loss:  0.100, Seconds: 6.45\n",
            "Epoch   2/100 Batch 1950/14303 - Loss:  0.102, Seconds: 6.41\n",
            "Epoch   2/100 Batch 1980/14303 - Loss:  0.103, Seconds: 6.39\n",
            "Epoch   2/100 Batch 2010/14303 - Loss:  0.098, Seconds: 6.44\n",
            "Epoch   2/100 Batch 2040/14303 - Loss:  0.095, Seconds: 6.45\n",
            "Epoch   2/100 Batch 2070/14303 - Loss:  0.100, Seconds: 6.43\n",
            "Epoch   2/100 Batch 2100/14303 - Loss:  0.103, Seconds: 6.44\n",
            "Epoch   2/100 Batch 2130/14303 - Loss:  0.097, Seconds: 6.48\n",
            "Epoch   2/100 Batch 2160/14303 - Loss:  0.098, Seconds: 6.45\n",
            "Epoch   2/100 Batch 2190/14303 - Loss:  0.097, Seconds: 6.48\n",
            "Epoch   2/100 Batch 2220/14303 - Loss:  0.099, Seconds: 6.61\n",
            "Epoch   2/100 Batch 2250/14303 - Loss:  0.100, Seconds: 6.65\n",
            "Epoch   2/100 Batch 2280/14303 - Loss:  0.098, Seconds: 6.73\n",
            "Epoch   2/100 Batch 2310/14303 - Loss:  0.096, Seconds: 6.65\n",
            "Epoch   2/100 Batch 2340/14303 - Loss:  0.105, Seconds: 6.72\n",
            "Epoch   2/100 Batch 2370/14303 - Loss:  0.099, Seconds: 6.58\n",
            "Epoch   2/100 Batch 2400/14303 - Loss:  0.100, Seconds: 6.54\n",
            "Epoch   2/100 Batch 2430/14303 - Loss:  0.099, Seconds: 6.58\n",
            "Epoch   2/100 Batch 2460/14303 - Loss:  0.101, Seconds: 6.53\n",
            "Epoch   2/100 Batch 2490/14303 - Loss:  0.100, Seconds: 6.59\n",
            "Epoch   2/100 Batch 2520/14303 - Loss:  0.098, Seconds: 6.60\n",
            "Epoch   2/100 Batch 2550/14303 - Loss:  0.106, Seconds: 6.78\n",
            "Epoch   2/100 Batch 2580/14303 - Loss:  0.098, Seconds: 6.76\n",
            "Epoch   2/100 Batch 2610/14303 - Loss:  0.100, Seconds: 6.75\n",
            "Epoch   2/100 Batch 2640/14303 - Loss:  0.099, Seconds: 6.74\n",
            "Epoch   2/100 Batch 2670/14303 - Loss:  0.103, Seconds: 6.75\n",
            "Epoch   2/100 Batch 2700/14303 - Loss:  0.094, Seconds: 6.69\n",
            "Epoch   2/100 Batch 2730/14303 - Loss:  0.093, Seconds: 6.75\n",
            "Epoch   2/100 Batch 2760/14303 - Loss:  0.097, Seconds: 6.73\n",
            "Epoch   2/100 Batch 2790/14303 - Loss:  0.098, Seconds: 6.72\n",
            "Epoch   2/100 Batch 2820/14303 - Loss:  0.098, Seconds: 6.77\n",
            "Epoch   2/100 Batch 2850/14303 - Loss:  0.102, Seconds: 6.96\n",
            "Epoch   2/100 Batch 2880/14303 - Loss:  0.098, Seconds: 6.93\n",
            "Epoch   2/100 Batch 2910/14303 - Loss:  0.096, Seconds: 6.94\n",
            "Epoch   2/100 Batch 2940/14303 - Loss:  0.092, Seconds: 6.98\n",
            "Epoch   2/100 Batch 2970/14303 - Loss:  0.099, Seconds: 6.96\n",
            "Epoch   2/100 Batch 3000/14303 - Loss:  0.096, Seconds: 6.92\n",
            "Epoch   2/100 Batch 3030/14303 - Loss:  0.097, Seconds: 6.93\n",
            "Epoch   2/100 Batch 3060/14303 - Loss:  0.100, Seconds: 7.00\n",
            "Epoch   2/100 Batch 3090/14303 - Loss:  0.096, Seconds: 6.95\n",
            "Epoch   2/100 Batch 3120/14303 - Loss:  0.099, Seconds: 6.94\n",
            "Epoch   2/100 Batch 3150/14303 - Loss:  0.095, Seconds: 6.88\n",
            "Epoch   2/100 Batch 3180/14303 - Loss:  0.095, Seconds: 7.13\n",
            "Epoch   2/100 Batch 3210/14303 - Loss:  0.099, Seconds: 7.08\n",
            "Epoch   2/100 Batch 3240/14303 - Loss:  0.096, Seconds: 7.14\n",
            "Epoch   2/100 Batch 3270/14303 - Loss:  0.093, Seconds: 7.21\n",
            "Epoch   2/100 Batch 3300/14303 - Loss:  0.094, Seconds: 7.04\n",
            "Epoch   2/100 Batch 3330/14303 - Loss:  0.093, Seconds: 7.19\n",
            "Epoch   2/100 Batch 3360/14303 - Loss:  0.096, Seconds: 7.23\n",
            "Epoch   2/100 Batch 3390/14303 - Loss:  0.098, Seconds: 7.15\n",
            "Epoch   2/100 Batch 3420/14303 - Loss:  0.098, Seconds: 7.17\n",
            "Epoch   2/100 Batch 3450/14303 - Loss:  0.097, Seconds: 7.10\n",
            "Epoch   2/100 Batch 3480/14303 - Loss:  0.097, Seconds: 7.21\n",
            "Epoch   2/100 Batch 3510/14303 - Loss:  0.096, Seconds: 7.43\n",
            "Epoch   2/100 Batch 3540/14303 - Loss:  0.095, Seconds: 7.36\n",
            "Epoch   2/100 Batch 3570/14303 - Loss:  0.097, Seconds: 7.54\n",
            "Epoch   2/100 Batch 3600/14303 - Loss:  0.098, Seconds: 7.48\n",
            "Epoch   2/100 Batch 3630/14303 - Loss:  0.101, Seconds: 7.41\n",
            "Epoch   2/100 Batch 3660/14303 - Loss:  0.096, Seconds: 7.43\n",
            "Epoch   2/100 Batch 3690/14303 - Loss:  0.098, Seconds: 7.43\n",
            "Epoch   2/100 Batch 3720/14303 - Loss:  0.093, Seconds: 7.57\n",
            "Epoch   2/100 Batch 3750/14303 - Loss:  0.093, Seconds: 7.46\n",
            "Epoch   2/100 Batch 3780/14303 - Loss:  0.096, Seconds: 7.43\n",
            "Epoch   2/100 Batch 3810/14303 - Loss:  0.098, Seconds: 7.54\n",
            "Epoch   2/100 Batch 3840/14303 - Loss:  0.097, Seconds: 7.59\n",
            "Epoch   2/100 Batch 3870/14303 - Loss:  0.097, Seconds: 7.72\n",
            "Epoch   2/100 Batch 3900/14303 - Loss:  0.095, Seconds: 7.54\n",
            "Epoch   2/100 Batch 3930/14303 - Loss:  0.097, Seconds: 7.62\n",
            "Epoch   2/100 Batch 3960/14303 - Loss:  0.097, Seconds: 7.55\n",
            "Epoch   2/100 Batch 3990/14303 - Loss:  0.097, Seconds: 7.56\n",
            "Epoch   2/100 Batch 4020/14303 - Loss:  0.090, Seconds: 7.51\n",
            "Epoch   2/100 Batch 4050/14303 - Loss:  0.096, Seconds: 7.54\n",
            "Epoch   2/100 Batch 4080/14303 - Loss:  0.092, Seconds: 7.59\n",
            "Epoch   2/100 Batch 4110/14303 - Loss:  0.091, Seconds: 7.56\n",
            "Epoch   2/100 Batch 4140/14303 - Loss:  0.099, Seconds: 7.72\n",
            "Epoch   2/100 Batch 4170/14303 - Loss:  0.093, Seconds: 7.86\n",
            "Epoch   2/100 Batch 4200/14303 - Loss:  0.090, Seconds: 7.75\n",
            "Epoch   2/100 Batch 4230/14303 - Loss:  0.099, Seconds: 7.84\n",
            "Epoch   2/100 Batch 4260/14303 - Loss:  0.092, Seconds: 7.90\n",
            "Epoch   2/100 Batch 4290/14303 - Loss:  0.095, Seconds: 8.01\n",
            "Epoch   2/100 Batch 4320/14303 - Loss:  0.094, Seconds: 7.74\n",
            "Epoch   2/100 Batch 4350/14303 - Loss:  0.094, Seconds: 7.73\n",
            "Epoch   2/100 Batch 4380/14303 - Loss:  0.101, Seconds: 7.79\n",
            "Epoch   2/100 Batch 4410/14303 - Loss:  0.096, Seconds: 7.79\n",
            "Epoch   2/100 Batch 4440/14303 - Loss:  0.100, Seconds: 7.92\n",
            "Epoch   2/100 Batch 4470/14303 - Loss:  0.100, Seconds: 7.91\n",
            "Epoch   2/100 Batch 4500/14303 - Loss:  0.101, Seconds: 7.96\n",
            "Epoch   2/100 Batch 4530/14303 - Loss:  0.092, Seconds: 7.94\n",
            "Epoch   2/100 Batch 4560/14303 - Loss:  0.098, Seconds: 8.05\n",
            "Epoch   2/100 Batch 4590/14303 - Loss:  0.089, Seconds: 7.89\n",
            "Epoch   2/100 Batch 4620/14303 - Loss:  0.094, Seconds: 7.99\n",
            "Epoch   2/100 Batch 4650/14303 - Loss:  0.092, Seconds: 7.96\n",
            "Epoch   2/100 Batch 4680/14303 - Loss:  0.093, Seconds: 7.93\n",
            "Epoch   2/100 Batch 4710/14303 - Loss:  0.095, Seconds: 8.14\n",
            "Epoch   2/100 Batch 4740/14303 - Loss:  0.095, Seconds: 8.06\n",
            "Epoch   2/100 Batch 4770/14303 - Loss:  0.093, Seconds: 8.06\n",
            "Epoch   2/100 Batch 4800/14303 - Loss:  0.095, Seconds: 8.18\n",
            "Epoch   2/100 Batch 4830/14303 - Loss:  0.093, Seconds: 8.19\n",
            "Epoch   2/100 Batch 4860/14303 - Loss:  0.095, Seconds: 8.26\n",
            "Epoch   2/100 Batch 4890/14303 - Loss:  0.097, Seconds: 8.30\n",
            "Epoch   2/100 Batch 4920/14303 - Loss:  0.093, Seconds: 8.34\n",
            "Epoch   2/100 Batch 4950/14303 - Loss:  0.094, Seconds: 8.40\n",
            "Epoch   2/100 Batch 4980/14303 - Loss:  0.094, Seconds: 8.39\n",
            "Epoch   2/100 Batch 5010/14303 - Loss:  0.097, Seconds: 8.46\n",
            "Epoch   2/100 Batch 5040/14303 - Loss:  0.095, Seconds: 8.52\n",
            "Epoch   2/100 Batch 5070/14303 - Loss:  0.094, Seconds: 8.62\n",
            "Epoch   2/100 Batch 5100/14303 - Loss:  0.092, Seconds: 8.56\n",
            "Epoch   2/100 Batch 5130/14303 - Loss:  0.099, Seconds: 8.62\n",
            "Epoch   2/100 Batch 5160/14303 - Loss:  0.090, Seconds: 8.61\n",
            "Epoch   2/100 Batch 5190/14303 - Loss:  0.094, Seconds: 8.62\n",
            "Epoch   2/100 Batch 5220/14303 - Loss:  0.094, Seconds: 8.63\n",
            "Epoch   2/100 Batch 5250/14303 - Loss:  0.093, Seconds: 8.61\n",
            "Epoch   2/100 Batch 5280/14303 - Loss:  0.097, Seconds: 8.65\n",
            "Epoch   2/100 Batch 5310/14303 - Loss:  0.089, Seconds: 8.59\n",
            "Epoch   2/100 Batch 5340/14303 - Loss:  0.094, Seconds: 8.55\n",
            "Epoch   2/100 Batch 5370/14303 - Loss:  0.094, Seconds: 8.73\n",
            "Epoch   2/100 Batch 5400/14303 - Loss:  0.099, Seconds: 8.81\n",
            "Epoch   2/100 Batch 5430/14303 - Loss:  0.093, Seconds: 8.80\n",
            "Epoch   2/100 Batch 5460/14303 - Loss:  0.092, Seconds: 8.76\n",
            "Epoch   2/100 Batch 5490/14303 - Loss:  0.096, Seconds: 8.90\n",
            "Epoch   2/100 Batch 5520/14303 - Loss:  0.095, Seconds: 8.95\n",
            "Epoch   2/100 Batch 5550/14303 - Loss:  0.092, Seconds: 8.90\n",
            "Epoch   2/100 Batch 5580/14303 - Loss:  0.091, Seconds: 8.85\n",
            "Epoch   2/100 Batch 5610/14303 - Loss:  0.091, Seconds: 8.77\n",
            "Epoch   2/100 Batch 5640/14303 - Loss:  0.093, Seconds: 9.11\n",
            "Epoch   2/100 Batch 5670/14303 - Loss:  0.095, Seconds: 9.04\n",
            "Epoch   2/100 Batch 5700/14303 - Loss:  0.094, Seconds: 9.16\n",
            "Epoch   2/100 Batch 5730/14303 - Loss:  0.094, Seconds: 9.29\n",
            "Epoch   2/100 Batch 5760/14303 - Loss:  0.090, Seconds: 9.09\n",
            "Epoch   2/100 Batch 5790/14303 - Loss:  0.090, Seconds: 9.15\n",
            "Epoch   2/100 Batch 5820/14303 - Loss:  0.096, Seconds: 9.21\n",
            "Epoch   2/100 Batch 5850/14303 - Loss:  0.089, Seconds: 9.04\n",
            "Epoch   2/100 Batch 5880/14303 - Loss:  0.093, Seconds: 8.97\n",
            "Epoch   2/100 Batch 5910/14303 - Loss:  0.096, Seconds: 8.69\n",
            "Epoch   2/100 Batch 5940/14303 - Loss:  0.089, Seconds: 8.72\n",
            "Epoch   2/100 Batch 5970/14303 - Loss:  0.089, Seconds: 8.75\n",
            "Epoch   2/100 Batch 6000/14303 - Loss:  0.096, Seconds: 8.78\n",
            "Epoch   2/100 Batch 6030/14303 - Loss:  0.092, Seconds: 8.96\n",
            "Epoch   2/100 Batch 6060/14303 - Loss:  0.090, Seconds: 8.84\n",
            "Epoch   2/100 Batch 6090/14303 - Loss:  0.096, Seconds: 8.73\n",
            "Epoch   2/100 Batch 6120/14303 - Loss:  0.092, Seconds: 8.88\n",
            "Epoch   2/100 Batch 6150/14303 - Loss:  0.095, Seconds: 8.76\n",
            "Epoch   2/100 Batch 6180/14303 - Loss:  0.091, Seconds: 8.64\n",
            "Epoch   2/100 Batch 6210/14303 - Loss:  0.090, Seconds: 8.75\n",
            "Epoch   2/100 Batch 6240/14303 - Loss:  0.096, Seconds: 8.84\n",
            "Epoch   2/100 Batch 6270/14303 - Loss:  0.091, Seconds: 8.87\n",
            "Epoch   2/100 Batch 6300/14303 - Loss:  0.094, Seconds: 8.99\n",
            "Epoch   2/100 Batch 6330/14303 - Loss:  0.096, Seconds: 8.93\n",
            "Epoch   2/100 Batch 6360/14303 - Loss:  0.093, Seconds: 8.91\n",
            "Epoch   2/100 Batch 6390/14303 - Loss:  0.090, Seconds: 8.92\n",
            "Epoch   2/100 Batch 6420/14303 - Loss:  0.090, Seconds: 8.84\n",
            "Epoch   2/100 Batch 6450/14303 - Loss:  0.093, Seconds: 8.90\n",
            "Epoch   2/100 Batch 6480/14303 - Loss:  0.094, Seconds: 8.84\n",
            "Epoch   2/100 Batch 6510/14303 - Loss:  0.093, Seconds: 8.89\n",
            "Epoch   2/100 Batch 6540/14303 - Loss:  0.089, Seconds: 9.09\n",
            "Epoch   2/100 Batch 6570/14303 - Loss:  0.089, Seconds: 9.00\n",
            "Epoch   2/100 Batch 6600/14303 - Loss:  0.094, Seconds: 9.18\n",
            "Epoch   2/100 Batch 6630/14303 - Loss:  0.098, Seconds: 9.05\n",
            "Epoch   2/100 Batch 6660/14303 - Loss:  0.093, Seconds: 9.04\n",
            "Epoch   2/100 Batch 6690/14303 - Loss:  0.089, Seconds: 9.19\n",
            "Epoch   2/100 Batch 6720/14303 - Loss:  0.094, Seconds: 9.15\n",
            "Epoch   2/100 Batch 6750/14303 - Loss:  0.092, Seconds: 9.01\n",
            "Epoch   2/100 Batch 6780/14303 - Loss:  0.090, Seconds: 9.01\n",
            "Epoch   2/100 Batch 6810/14303 - Loss:  0.092, Seconds: 9.38\n",
            "Epoch   2/100 Batch 6840/14303 - Loss:  0.093, Seconds: 9.22\n",
            "Epoch   2/100 Batch 6870/14303 - Loss:  0.088, Seconds: 9.14\n",
            "Epoch   2/100 Batch 6900/14303 - Loss:  0.092, Seconds: 9.09\n",
            "Epoch   2/100 Batch 6930/14303 - Loss:  0.093, Seconds: 9.33\n",
            "Epoch   2/100 Batch 6960/14303 - Loss:  0.095, Seconds: 9.41\n",
            "Epoch   2/100 Batch 6990/14303 - Loss:  0.090, Seconds: 9.38\n",
            "Epoch   2/100 Batch 7020/14303 - Loss:  0.093, Seconds: 9.38\n",
            "Epoch   2/100 Batch 7050/14303 - Loss:  0.090, Seconds: 9.43\n",
            "Epoch   2/100 Batch 7080/14303 - Loss:  0.090, Seconds: 9.45\n",
            "Epoch   2/100 Batch 7110/14303 - Loss:  0.091, Seconds: 9.49\n",
            "Epoch   2/100 Batch 7140/14303 - Loss:  0.093, Seconds: 9.55\n",
            "Epoch   2/100 Batch 7170/14303 - Loss:  0.088, Seconds: 9.48\n",
            "Epoch   2/100 Batch 7200/14303 - Loss:  0.088, Seconds: 9.56\n",
            "Epoch   2/100 Batch 7230/14303 - Loss:  0.091, Seconds: 9.54\n",
            "Epoch   2/100 Batch 7260/14303 - Loss:  0.087, Seconds: 9.50\n",
            "Epoch   2/100 Batch 7290/14303 - Loss:  0.091, Seconds: 9.63\n",
            "Epoch   2/100 Batch 7320/14303 - Loss:  0.092, Seconds: 9.53\n",
            "Epoch   2/100 Batch 7350/14303 - Loss:  0.090, Seconds: 9.61\n",
            "Epoch   2/100 Batch 7380/14303 - Loss:  0.091, Seconds: 9.69\n",
            "Epoch   2/100 Batch 7410/14303 - Loss:  0.093, Seconds: 9.66\n",
            "Epoch   2/100 Batch 7440/14303 - Loss:  0.089, Seconds: 9.60\n",
            "Epoch   2/100 Batch 7470/14303 - Loss:  0.090, Seconds: 9.64\n",
            "Epoch   2/100 Batch 7500/14303 - Loss:  0.087, Seconds: 9.79\n",
            "Epoch   2/100 Batch 7530/14303 - Loss:  0.088, Seconds: 9.88\n",
            "Epoch   2/100 Batch 7560/14303 - Loss:  0.093, Seconds: 9.77\n",
            "Epoch   2/100 Batch 7590/14303 - Loss:  0.090, Seconds: 9.78\n",
            "Epoch   2/100 Batch 7620/14303 - Loss:  0.093, Seconds: 10.16\n",
            "Epoch   2/100 Batch 7650/14303 - Loss:  0.092, Seconds: 9.94\n",
            "Epoch   2/100 Batch 7680/14303 - Loss:  0.091, Seconds: 9.81\n",
            "Epoch   2/100 Batch 7710/14303 - Loss:  0.090, Seconds: 9.85\n",
            "Epoch   2/100 Batch 7740/14303 - Loss:  0.091, Seconds: 9.82\n",
            "Epoch   2/100 Batch 7770/14303 - Loss:  0.089, Seconds: 9.81\n",
            "Epoch   2/100 Batch 7800/14303 - Loss:  0.089, Seconds: 9.68\n",
            "Epoch   2/100 Batch 7830/14303 - Loss:  0.093, Seconds: 9.71\n",
            "Epoch   2/100 Batch 7860/14303 - Loss:  0.092, Seconds: 9.73\n",
            "Epoch   2/100 Batch 7890/14303 - Loss:  0.091, Seconds: 9.90\n",
            "Epoch   2/100 Batch 7920/14303 - Loss:  0.087, Seconds: 9.85\n",
            "Epoch   2/100 Batch 7950/14303 - Loss:  0.092, Seconds: 9.89\n",
            "Epoch   2/100 Batch 7980/14303 - Loss:  0.091, Seconds: 9.98\n",
            "Epoch   2/100 Batch 8010/14303 - Loss:  0.089, Seconds: 9.89\n",
            "Epoch   2/100 Batch 8040/14303 - Loss:  0.088, Seconds: 9.91\n",
            "Epoch   2/100 Batch 8070/14303 - Loss:  0.087, Seconds: 9.88\n",
            "Epoch   2/100 Batch 8100/14303 - Loss:  0.087, Seconds: 9.86\n",
            "Epoch   2/100 Batch 8130/14303 - Loss:  0.091, Seconds: 10.06\n",
            "Epoch   2/100 Batch 8160/14303 - Loss:  0.087, Seconds: 9.98\n",
            "Epoch   2/100 Batch 8190/14303 - Loss:  0.088, Seconds: 10.09\n",
            "Epoch   2/100 Batch 8220/14303 - Loss:  0.091, Seconds: 10.14\n",
            "Epoch   2/100 Batch 8250/14303 - Loss:  0.086, Seconds: 10.09\n",
            "Epoch   2/100 Batch 8280/14303 - Loss:  0.089, Seconds: 10.14\n",
            "Epoch   2/100 Batch 8310/14303 - Loss:  0.088, Seconds: 10.15\n",
            "Epoch   2/100 Batch 8340/14303 - Loss:  0.091, Seconds: 10.05\n",
            "Epoch   2/100 Batch 8370/14303 - Loss:  0.092, Seconds: 10.33\n",
            "Epoch   2/100 Batch 8400/14303 - Loss:  0.092, Seconds: 10.27\n",
            "Epoch   2/100 Batch 8430/14303 - Loss:  0.089, Seconds: 10.23\n",
            "Epoch   2/100 Batch 8460/14303 - Loss:  0.089, Seconds: 10.14\n",
            "Epoch   2/100 Batch 8490/14303 - Loss:  0.085, Seconds: 10.32\n",
            "Epoch   2/100 Batch 8520/14303 - Loss:  0.089, Seconds: 10.21\n",
            "Epoch   2/100 Batch 8550/14303 - Loss:  0.086, Seconds: 10.20\n",
            "Epoch   2/100 Batch 8580/14303 - Loss:  0.088, Seconds: 10.21\n",
            "Epoch   2/100 Batch 8610/14303 - Loss:  0.087, Seconds: 10.36\n",
            "Epoch   2/100 Batch 8640/14303 - Loss:  0.092, Seconds: 10.39\n",
            "Epoch   2/100 Batch 8670/14303 - Loss:  0.088, Seconds: 10.36\n",
            "Epoch   2/100 Batch 8700/14303 - Loss:  0.087, Seconds: 10.33\n",
            "Epoch   2/100 Batch 8730/14303 - Loss:  0.087, Seconds: 10.29\n",
            "Epoch   2/100 Batch 8760/14303 - Loss:  0.090, Seconds: 10.34\n",
            "Epoch   2/100 Batch 8790/14303 - Loss:  0.088, Seconds: 10.38\n",
            "Epoch   2/100 Batch 8820/14303 - Loss:  0.088, Seconds: 10.42\n",
            "Epoch   2/100 Batch 8850/14303 - Loss:  0.098, Seconds: 10.56\n",
            "Epoch   2/100 Batch 8880/14303 - Loss:  0.235, Seconds: 10.64\n",
            "Epoch   2/100 Batch 8910/14303 - Loss:  0.132, Seconds: 10.55\n",
            "Epoch   2/100 Batch 8940/14303 - Loss:  0.106, Seconds: 10.52\n",
            "Epoch   2/100 Batch 8970/14303 - Loss:  0.100, Seconds: 10.57\n",
            "Epoch   2/100 Batch 9000/14303 - Loss:  0.098, Seconds: 10.58\n",
            "Epoch   2/100 Batch 9030/14303 - Loss:  0.095, Seconds: 10.57\n",
            "Epoch   2/100 Batch 9060/14303 - Loss:  0.096, Seconds: 10.70\n",
            "Epoch   2/100 Batch 9090/14303 - Loss:  0.091, Seconds: 10.81\n",
            "Epoch   2/100 Batch 9120/14303 - Loss:  0.095, Seconds: 10.70\n",
            "Epoch   2/100 Batch 9150/14303 - Loss:  0.092, Seconds: 10.72\n",
            "Epoch   2/100 Batch 9180/14303 - Loss:  0.091, Seconds: 10.75\n",
            "Epoch   2/100 Batch 9210/14303 - Loss:  0.091, Seconds: 10.79\n",
            "Epoch   2/100 Batch 9240/14303 - Loss:  0.092, Seconds: 10.71\n",
            "Epoch   2/100 Batch 9270/14303 - Loss:  0.087, Seconds: 10.93\n",
            "Epoch   2/100 Batch 9300/14303 - Loss:  0.090, Seconds: 10.98\n",
            "Epoch   2/100 Batch 9330/14303 - Loss:  0.093, Seconds: 11.10\n",
            "Epoch   2/100 Batch 9360/14303 - Loss:  0.086, Seconds: 10.82\n",
            "Epoch   2/100 Batch 9390/14303 - Loss:  0.090, Seconds: 10.88\n",
            "Epoch   2/100 Batch 9420/14303 - Loss:  0.088, Seconds: 10.84\n",
            "Epoch   2/100 Batch 9450/14303 - Loss:  0.089, Seconds: 10.78\n",
            "Epoch   2/100 Batch 9480/14303 - Loss:  0.087, Seconds: 11.01\n",
            "Epoch   2/100 Batch 9510/14303 - Loss:  0.087, Seconds: 11.05\n",
            "Epoch   2/100 Batch 9540/14303 - Loss:  0.084, Seconds: 10.99\n",
            "Epoch   2/100 Batch 9570/14303 - Loss:  0.092, Seconds: 10.98\n",
            "Epoch   2/100 Batch 9600/14303 - Loss:  0.090, Seconds: 11.04\n",
            "Epoch   2/100 Batch 9630/14303 - Loss:  0.090, Seconds: 10.99\n",
            "Epoch   2/100 Batch 9660/14303 - Loss:  0.087, Seconds: 11.04\n",
            "Epoch   2/100 Batch 9690/14303 - Loss:  0.087, Seconds: 11.22\n",
            "Epoch   2/100 Batch 9720/14303 - Loss:  0.082, Seconds: 11.23\n",
            "Epoch   2/100 Batch 9750/14303 - Loss:  0.086, Seconds: 11.15\n",
            "Epoch   2/100 Batch 9780/14303 - Loss:  0.086, Seconds: 11.19\n",
            "Epoch   2/100 Batch 9810/14303 - Loss:  0.085, Seconds: 11.14\n",
            "Epoch   2/100 Batch 9840/14303 - Loss:  0.088, Seconds: 11.20\n",
            "Epoch   2/100 Batch 9870/14303 - Loss:  0.088, Seconds: 11.24\n",
            "Epoch   2/100 Batch 9900/14303 - Loss:  0.088, Seconds: 11.32\n",
            "Epoch   2/100 Batch 9930/14303 - Loss:  0.087, Seconds: 11.40\n",
            "Epoch   2/100 Batch 9960/14303 - Loss:  0.086, Seconds: 11.33\n",
            "Epoch   2/100 Batch 9990/14303 - Loss:  0.088, Seconds: 11.41\n",
            "Epoch   2/100 Batch 10020/14303 - Loss:  0.089, Seconds: 11.32\n",
            "Epoch   2/100 Batch 10050/14303 - Loss:  0.087, Seconds: 11.40\n",
            "Epoch   2/100 Batch 10080/14303 - Loss:  0.087, Seconds: 11.53\n",
            "Epoch   2/100 Batch 10110/14303 - Loss:  0.085, Seconds: 11.71\n",
            "Epoch   2/100 Batch 10140/14303 - Loss:  0.089, Seconds: 11.57\n",
            "Epoch   2/100 Batch 10170/14303 - Loss:  0.087, Seconds: 11.45\n",
            "Epoch   2/100 Batch 10200/14303 - Loss:  0.084, Seconds: 11.55\n",
            "Epoch   2/100 Batch 10230/14303 - Loss:  0.085, Seconds: 11.54\n",
            "Epoch   2/100 Batch 10260/14303 - Loss:  0.087, Seconds: 11.72\n",
            "Epoch   2/100 Batch 10290/14303 - Loss:  0.088, Seconds: 11.69\n",
            "Epoch   2/100 Batch 10320/14303 - Loss:  0.086, Seconds: 11.70\n",
            "Epoch   2/100 Batch 10350/14303 - Loss:  0.087, Seconds: 11.69\n",
            "Epoch   2/100 Batch 10380/14303 - Loss:  0.081, Seconds: 11.66\n",
            "Epoch   2/100 Batch 10410/14303 - Loss:  0.085, Seconds: 11.70\n",
            "Epoch   2/100 Batch 10440/14303 - Loss:  0.086, Seconds: 11.93\n",
            "Epoch   2/100 Batch 10470/14303 - Loss:  0.085, Seconds: 11.87\n",
            "Epoch   2/100 Batch 10500/14303 - Loss:  0.083, Seconds: 11.96\n",
            "Epoch   2/100 Batch 10530/14303 - Loss:  0.085, Seconds: 11.81\n",
            "Epoch   2/100 Batch 10560/14303 - Loss:  0.086, Seconds: 11.81\n",
            "Epoch   2/100 Batch 10590/14303 - Loss:  0.090, Seconds: 11.94\n",
            "Epoch   2/100 Batch 10620/14303 - Loss:  0.084, Seconds: 12.04\n",
            "Epoch   2/100 Batch 10650/14303 - Loss:  0.084, Seconds: 12.03\n",
            "Epoch   2/100 Batch 10680/14303 - Loss:  0.084, Seconds: 12.05\n",
            "Epoch   2/100 Batch 10710/14303 - Loss:  0.088, Seconds: 12.17\n",
            "Epoch   2/100 Batch 10740/14303 - Loss:  0.084, Seconds: 12.05\n",
            "Epoch   2/100 Batch 10770/14303 - Loss:  0.085, Seconds: 12.20\n",
            "Epoch   2/100 Batch 10800/14303 - Loss:  0.086, Seconds: 12.21\n",
            "Epoch   2/100 Batch 10830/14303 - Loss:  0.086, Seconds: 12.23\n",
            "Epoch   2/100 Batch 10860/14303 - Loss:  0.084, Seconds: 12.34\n",
            "Epoch   2/100 Batch 10890/14303 - Loss:  0.087, Seconds: 12.19\n",
            "Epoch   2/100 Batch 10920/14303 - Loss:  0.086, Seconds: 12.36\n",
            "Epoch   2/100 Batch 10950/14303 - Loss:  0.085, Seconds: 12.38\n",
            "Epoch   2/100 Batch 10980/14303 - Loss:  0.083, Seconds: 12.31\n",
            "Epoch   2/100 Batch 11010/14303 - Loss:  0.086, Seconds: 12.42\n",
            "Epoch   2/100 Batch 11040/14303 - Loss:  0.086, Seconds: 12.37\n",
            "Epoch   2/100 Batch 11070/14303 - Loss:  0.086, Seconds: 12.56\n",
            "Epoch   2/100 Batch 11100/14303 - Loss:  0.086, Seconds: 12.49\n",
            "Epoch   2/100 Batch 11130/14303 - Loss:  0.086, Seconds: 12.52\n",
            "Epoch   2/100 Batch 11160/14303 - Loss:  0.084, Seconds: 12.50\n",
            "Epoch   2/100 Batch 11190/14303 - Loss:  0.083, Seconds: 12.50\n",
            "Epoch   2/100 Batch 11220/14303 - Loss:  0.086, Seconds: 12.76\n",
            "Epoch   2/100 Batch 11250/14303 - Loss:  0.084, Seconds: 12.87\n",
            "Epoch   2/100 Batch 11280/14303 - Loss:  0.083, Seconds: 12.85\n",
            "Epoch   2/100 Batch 11310/14303 - Loss:  0.084, Seconds: 12.83\n",
            "Epoch   2/100 Batch 11340/14303 - Loss:  0.086, Seconds: 12.84\n",
            "Epoch   2/100 Batch 11370/14303 - Loss:  0.086, Seconds: 12.99\n",
            "Epoch   2/100 Batch 11400/14303 - Loss:  0.085, Seconds: 13.06\n",
            "Epoch   2/100 Batch 11430/14303 - Loss:  0.085, Seconds: 13.04\n",
            "Epoch   2/100 Batch 11460/14303 - Loss:  0.084, Seconds: 12.96\n",
            "Epoch   2/100 Batch 11490/14303 - Loss:  0.082, Seconds: 13.08\n",
            "Epoch   2/100 Batch 11520/14303 - Loss:  0.087, Seconds: 13.18\n",
            "Epoch   2/100 Batch 11550/14303 - Loss:  0.083, Seconds: 13.33\n",
            "Epoch   2/100 Batch 11580/14303 - Loss:  0.084, Seconds: 13.17\n",
            "Epoch   2/100 Batch 11610/14303 - Loss:  0.085, Seconds: 13.21\n",
            "Epoch   2/100 Batch 11640/14303 - Loss:  0.086, Seconds: 13.31\n",
            "Epoch   2/100 Batch 11670/14303 - Loss:  0.080, Seconds: 13.34\n",
            "Epoch   2/100 Batch 11700/14303 - Loss:  0.084, Seconds: 13.30\n",
            "Epoch   2/100 Batch 11730/14303 - Loss:  0.084, Seconds: 13.29\n",
            "Epoch   2/100 Batch 11760/14303 - Loss:  0.083, Seconds: 13.56\n",
            "Epoch   2/100 Batch 11790/14303 - Loss:  0.084, Seconds: 13.46\n",
            "Epoch   2/100 Batch 11820/14303 - Loss:  0.082, Seconds: 13.44\n",
            "Epoch   2/100 Batch 11850/14303 - Loss:  0.083, Seconds: 13.46\n",
            "Epoch   2/100 Batch 11880/14303 - Loss:  0.083, Seconds: 13.65\n",
            "Epoch   2/100 Batch 11910/14303 - Loss:  0.081, Seconds: 13.66\n",
            "Epoch   2/100 Batch 11940/14303 - Loss:  0.080, Seconds: 13.67\n",
            "Epoch   2/100 Batch 11970/14303 - Loss:  0.081, Seconds: 13.70\n",
            "Epoch   2/100 Batch 12000/14303 - Loss:  0.085, Seconds: 13.85\n",
            "Epoch   2/100 Batch 12030/14303 - Loss:  0.084, Seconds: 13.90\n",
            "Epoch   2/100 Batch 12060/14303 - Loss:  0.084, Seconds: 13.88\n",
            "Epoch   2/100 Batch 12090/14303 - Loss:  0.080, Seconds: 13.97\n",
            "Epoch   2/100 Batch 12120/14303 - Loss:  0.083, Seconds: 13.98\n",
            "Epoch   2/100 Batch 12150/14303 - Loss:  0.083, Seconds: 14.00\n",
            "Epoch   2/100 Batch 12180/14303 - Loss:  0.079, Seconds: 14.10\n",
            "Epoch   2/100 Batch 12210/14303 - Loss:  0.083, Seconds: 14.17\n",
            "Epoch   2/100 Batch 12240/14303 - Loss:  0.082, Seconds: 14.18\n",
            "Epoch   2/100 Batch 12270/14303 - Loss:  0.084, Seconds: 14.30\n",
            "Epoch   2/100 Batch 12300/14303 - Loss:  0.084, Seconds: 14.25\n",
            "Epoch   2/100 Batch 12330/14303 - Loss:  0.086, Seconds: 14.33\n",
            "Epoch   2/100 Batch 12360/14303 - Loss:  0.083, Seconds: 14.36\n",
            "Epoch   2/100 Batch 12390/14303 - Loss:  0.083, Seconds: 14.36\n",
            "Epoch   2/100 Batch 12420/14303 - Loss:  0.082, Seconds: 14.56\n",
            "Epoch   2/100 Batch 12450/14303 - Loss:  0.079, Seconds: 14.46\n",
            "Epoch   2/100 Batch 12480/14303 - Loss:  0.082, Seconds: 14.50\n",
            "Epoch   2/100 Batch 12510/14303 - Loss:  0.082, Seconds: 14.71\n",
            "Epoch   2/100 Batch 12540/14303 - Loss:  0.085, Seconds: 14.64\n",
            "Epoch   2/100 Batch 12570/14303 - Loss:  0.081, Seconds: 14.81\n",
            "Epoch   2/100 Batch 12600/14303 - Loss:  0.081, Seconds: 14.91\n",
            "Epoch   2/100 Batch 12630/14303 - Loss:  0.082, Seconds: 14.95\n",
            "Epoch   2/100 Batch 12660/14303 - Loss:  0.081, Seconds: 15.08\n",
            "Epoch   2/100 Batch 12690/14303 - Loss:  0.084, Seconds: 15.03\n",
            "Epoch   2/100 Batch 12720/14303 - Loss:  0.082, Seconds: 15.16\n",
            "Epoch   2/100 Batch 12750/14303 - Loss:  0.083, Seconds: 15.25\n",
            "Epoch   2/100 Batch 12780/14303 - Loss:  0.083, Seconds: 15.48\n",
            "Epoch   2/100 Batch 12810/14303 - Loss:  0.082, Seconds: 15.38\n",
            "Epoch   2/100 Batch 12840/14303 - Loss:  0.086, Seconds: 15.48\n",
            "Epoch   2/100 Batch 12870/14303 - Loss:  0.083, Seconds: 15.55\n",
            "Epoch   2/100 Batch 12900/14303 - Loss:  0.082, Seconds: 15.57\n",
            "Epoch   2/100 Batch 12930/14303 - Loss:  0.082, Seconds: 15.70\n",
            "Epoch   2/100 Batch 12960/14303 - Loss:  0.093, Seconds: 15.81\n",
            "Epoch   2/100 Batch 12990/14303 - Loss:  0.088, Seconds: 15.85\n",
            "Epoch   2/100 Batch 13020/14303 - Loss:  0.088, Seconds: 15.90\n",
            "Epoch   2/100 Batch 13050/14303 - Loss:  0.084, Seconds: 16.00\n",
            "Epoch   2/100 Batch 13080/14303 - Loss:  0.083, Seconds: 16.04\n",
            "Epoch   2/100 Batch 13110/14303 - Loss:  0.085, Seconds: 16.22\n",
            "Epoch   2/100 Batch 13140/14303 - Loss:  0.085, Seconds: 16.28\n",
            "Epoch   2/100 Batch 13170/14303 - Loss:  0.078, Seconds: 16.46\n",
            "Epoch   2/100 Batch 13200/14303 - Loss:  0.080, Seconds: 16.54\n",
            "Epoch   2/100 Batch 13230/14303 - Loss:  0.082, Seconds: 16.61\n",
            "Epoch   2/100 Batch 13260/14303 - Loss:  0.083, Seconds: 16.62\n",
            "Epoch   2/100 Batch 13290/14303 - Loss:  0.079, Seconds: 16.93\n",
            "Epoch   2/100 Batch 13320/14303 - Loss:  0.080, Seconds: 17.09\n",
            "Epoch   2/100 Batch 13350/14303 - Loss:  0.079, Seconds: 17.09\n",
            "Epoch   2/100 Batch 13380/14303 - Loss:  0.083, Seconds: 17.14\n",
            "Epoch   2/100 Batch 13410/14303 - Loss:  0.079, Seconds: 17.31\n",
            "Epoch   2/100 Batch 13440/14303 - Loss:  0.080, Seconds: 17.47\n",
            "Epoch   2/100 Batch 13470/14303 - Loss:  0.080, Seconds: 17.54\n",
            "Epoch   2/100 Batch 13500/14303 - Loss:  0.082, Seconds: 17.63\n",
            "Epoch   2/100 Batch 13530/14303 - Loss:  0.080, Seconds: 17.91\n",
            "Epoch   2/100 Batch 13560/14303 - Loss:  0.079, Seconds: 18.08\n",
            "Epoch   2/100 Batch 13590/14303 - Loss:  0.080, Seconds: 18.14\n",
            "Epoch   2/100 Batch 13620/14303 - Loss:  0.080, Seconds: 18.30\n",
            "Epoch   2/100 Batch 13650/14303 - Loss:  0.081, Seconds: 18.52\n",
            "Epoch   2/100 Batch 13680/14303 - Loss:  0.080, Seconds: 18.83\n",
            "Epoch   2/100 Batch 13710/14303 - Loss:  0.080, Seconds: 18.80\n",
            "Epoch   2/100 Batch 13740/14303 - Loss:  0.079, Seconds: 19.04\n",
            "Epoch   2/100 Batch 13770/14303 - Loss:  0.081, Seconds: 19.29\n",
            "Epoch   2/100 Batch 13800/14303 - Loss:  0.080, Seconds: 19.65\n",
            "Epoch   2/100 Batch 13830/14303 - Loss:  0.077, Seconds: 19.69\n",
            "Epoch   2/100 Batch 13860/14303 - Loss:  0.079, Seconds: 19.94\n",
            "Epoch   2/100 Batch 13890/14303 - Loss:  0.076, Seconds: 20.18\n",
            "Epoch   2/100 Batch 13920/14303 - Loss:  0.079, Seconds: 20.53\n",
            "Epoch   2/100 Batch 13950/14303 - Loss:  0.078, Seconds: 20.79\n",
            "Epoch   2/100 Batch 13980/14303 - Loss:  0.081, Seconds: 21.09\n",
            "Epoch   2/100 Batch 14010/14303 - Loss:  0.077, Seconds: 21.53\n",
            "Epoch   2/100 Batch 14040/14303 - Loss:  0.081, Seconds: 21.79\n",
            "Epoch   2/100 Batch 14070/14303 - Loss:  0.080, Seconds: 22.32\n",
            "Epoch   2/100 Batch 14100/14303 - Loss:  0.082, Seconds: 22.83\n",
            "Epoch   2/100 Batch 14130/14303 - Loss:  0.080, Seconds: 23.38\n",
            "Epoch   2/100 Batch 14160/14303 - Loss:  0.080, Seconds: 23.99\n",
            "Epoch   2/100 Batch 14190/14303 - Loss:  0.080, Seconds: 24.76\n",
            "Epoch   2/100 Batch 14220/14303 - Loss:  0.080, Seconds: 26.03\n",
            "Epoch   2/100 Batch 14250/14303 - Loss:  0.078, Seconds: 27.21\n",
            "Epoch   2/100 Batch 14280/14303 - Loss:  0.081, Seconds: 29.72\n",
            "  Validation Input: no aqs bad sa one sanadl think\n",
            "  Validation Output: now as base bad as bad as one san\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it ius lso calledg rant sahbi\n",
            "  Validation Output: it is also also called it is als\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: i is intende dfor adrults only\n",
            "  Validation Output: it is intended for its intended f\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reasone things orut for yuoself\n",
            "  Validation Output: reason reasoned things out for you\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it happened to work quite wexll\n",
            "  Validation Output: it happened to happened to happene\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: hef warnd i are doing very well\n",
            "  Validation Output: helf warned it are doing very doin\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: cnetecr for the perfaorming arts\n",
            "  Validation Output: center for the center for the perfo\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: sopon into a hot bowl to serve\n",
            "  Validation Output: spoon into a hospon into a hot bo\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: people qmete her nagd they swoon\n",
            "  Validation Output: people meet heapple meet her and th\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my poor little head sib oilingu\n",
            "  Validation Output: my poor little head my poor little\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this ah dbecome acommonb scene\n",
            "  Validation Output: this had become this had become\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: eve nwent through the spceials\n",
            "  Validation Output: even went through through the spe\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: prheta ax stexameron hignh heat\n",
            "  Validation Output: phretax ax stexame ax stexameron t\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lo well someone had to say it\n",
            "  Validation Output: low well someone had to someone\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he aws weavidng mts on the lst\n",
            "  Validation Output: he was weaving most weaving most\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: thi is wron for manyj reasos\n",
            "  Validation Output: this is wrong for many reasons\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: todya we heded againt o cuvbao\n",
            "  Validation Output: today we headed against to cuvab\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep and meber stauts required\n",
            "  Validation Output: rep and member and member status\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it snowed imdlding har o day\n",
            "  Validation Output: it snowed middle middling heart\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: threy bothv waerp leather sandals\n",
            "  Validation Output: thereby both water both water both w\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  2.309, Seconds: 0.00\n",
            "No Improvement.\n",
            "\n",
            "Training Model: 3\n",
            "Epoch   3/100 Batch   30/14303 - Loss:  0.363, Seconds: 5.69\n",
            "Epoch   3/100 Batch   60/14303 - Loss:  0.145, Seconds: 5.47\n",
            "Epoch   3/100 Batch   90/14303 - Loss:  0.120, Seconds: 5.47\n",
            "Epoch   3/100 Batch  120/14303 - Loss:  0.107, Seconds: 5.57\n",
            "Epoch   3/100 Batch  150/14303 - Loss:  0.110, Seconds: 5.50\n",
            "Epoch   3/100 Batch  180/14303 - Loss:  0.103, Seconds: 5.48\n",
            "Epoch   3/100 Batch  210/14303 - Loss:  0.102, Seconds: 5.51\n",
            "Epoch   3/100 Batch  240/14303 - Loss:  0.099, Seconds: 5.58\n",
            "Epoch   3/100 Batch  270/14303 - Loss:  0.100, Seconds: 5.59\n",
            "Epoch   3/100 Batch  300/14303 - Loss:  0.117, Seconds: 5.53\n",
            "Epoch   3/100 Batch  330/14303 - Loss:  0.097, Seconds: 5.68\n",
            "Epoch   3/100 Batch  360/14303 - Loss:  0.098, Seconds: 5.63\n",
            "Epoch   3/100 Batch  390/14303 - Loss:  0.096, Seconds: 5.64\n",
            "Epoch   3/100 Batch  420/14303 - Loss:  0.095, Seconds: 5.60\n",
            "Epoch   3/100 Batch  450/14303 - Loss:  0.091, Seconds: 5.68\n",
            "Epoch   3/100 Batch  480/14303 - Loss:  0.093, Seconds: 5.65\n",
            "Epoch   3/100 Batch  510/14303 - Loss:  0.092, Seconds: 5.71\n",
            "Epoch   3/100 Batch  540/14303 - Loss:  0.084, Seconds: 5.65\n",
            "Epoch   3/100 Batch  570/14303 - Loss:  0.095, Seconds: 5.66\n",
            "Epoch   3/100 Batch  600/14303 - Loss:  0.093, Seconds: 5.67\n",
            "Epoch   3/100 Batch  630/14303 - Loss:  0.092, Seconds: 5.85\n",
            "Epoch   3/100 Batch  660/14303 - Loss:  0.092, Seconds: 5.90\n",
            "Epoch   3/100 Batch  690/14303 - Loss:  0.086, Seconds: 5.81\n",
            "Epoch   3/100 Batch  720/14303 - Loss:  0.086, Seconds: 5.83\n",
            "Epoch   3/100 Batch  750/14303 - Loss:  0.087, Seconds: 5.85\n",
            "Epoch   3/100 Batch  780/14303 - Loss:  0.089, Seconds: 5.80\n",
            "Epoch   3/100 Batch  810/14303 - Loss:  0.093, Seconds: 5.94\n",
            "Epoch   3/100 Batch  840/14303 - Loss:  0.085, Seconds: 5.84\n",
            "Epoch   3/100 Batch  870/14303 - Loss:  0.087, Seconds: 5.81\n",
            "Epoch   3/100 Batch  900/14303 - Loss:  0.084, Seconds: 5.82\n",
            "Epoch   3/100 Batch  930/14303 - Loss:  0.094, Seconds: 5.94\n",
            "Epoch   3/100 Batch  960/14303 - Loss:  0.084, Seconds: 6.00\n",
            "Epoch   3/100 Batch  990/14303 - Loss:  0.086, Seconds: 5.99\n",
            "Epoch   3/100 Batch 1020/14303 - Loss:  0.088, Seconds: 6.06\n",
            "Epoch   3/100 Batch 1050/14303 - Loss:  0.088, Seconds: 6.03\n",
            "Epoch   3/100 Batch 1080/14303 - Loss:  0.084, Seconds: 6.00\n",
            "Epoch   3/100 Batch 1110/14303 - Loss:  0.084, Seconds: 5.98\n",
            "Epoch   3/100 Batch 1140/14303 - Loss:  0.087, Seconds: 5.99\n",
            "Epoch   3/100 Batch 1170/14303 - Loss:  0.083, Seconds: 6.03\n",
            "Epoch   3/100 Batch 1200/14303 - Loss:  0.088, Seconds: 5.96\n",
            "Epoch   3/100 Batch 1230/14303 - Loss:  0.082, Seconds: 6.04\n",
            "Epoch   3/100 Batch 1260/14303 - Loss:  0.084, Seconds: 6.24\n",
            "Epoch   3/100 Batch 1290/14303 - Loss:  0.087, Seconds: 6.29\n",
            "Epoch   3/100 Batch 1320/14303 - Loss:  0.083, Seconds: 6.19\n",
            "Epoch   3/100 Batch 1350/14303 - Loss:  0.084, Seconds: 6.16\n",
            "Epoch   3/100 Batch 1380/14303 - Loss:  0.081, Seconds: 6.21\n",
            "Epoch   3/100 Batch 1410/14303 - Loss:  0.084, Seconds: 6.21\n",
            "Epoch   3/100 Batch 1440/14303 - Loss:  0.083, Seconds: 6.15\n",
            "Epoch   3/100 Batch 1470/14303 - Loss:  0.084, Seconds: 6.17\n",
            "Epoch   3/100 Batch 1500/14303 - Loss:  0.083, Seconds: 6.17\n",
            "Epoch   3/100 Batch 1530/14303 - Loss:  0.085, Seconds: 6.14\n",
            "Epoch   3/100 Batch 1560/14303 - Loss:  0.086, Seconds: 6.21\n",
            "Epoch   3/100 Batch 1590/14303 - Loss:  0.080, Seconds: 6.31\n",
            "Epoch   3/100 Batch 1620/14303 - Loss:  0.082, Seconds: 6.28\n",
            "Epoch   3/100 Batch 1650/14303 - Loss:  0.083, Seconds: 6.33\n",
            "Epoch   3/100 Batch 1680/14303 - Loss:  0.080, Seconds: 6.33\n",
            "Epoch   3/100 Batch 1710/14303 - Loss:  0.082, Seconds: 6.31\n",
            "Epoch   3/100 Batch 1740/14303 - Loss:  0.085, Seconds: 6.39\n",
            "Epoch   3/100 Batch 1770/14303 - Loss:  0.081, Seconds: 6.38\n",
            "Epoch   3/100 Batch 1800/14303 - Loss:  0.083, Seconds: 6.34\n",
            "Epoch   3/100 Batch 1830/14303 - Loss:  0.085, Seconds: 6.34\n",
            "Epoch   3/100 Batch 1860/14303 - Loss:  0.085, Seconds: 6.30\n",
            "Epoch   3/100 Batch 1890/14303 - Loss:  0.083, Seconds: 6.51\n",
            "Epoch   3/100 Batch 1920/14303 - Loss:  0.083, Seconds: 6.51\n",
            "Epoch   3/100 Batch 1950/14303 - Loss:  0.083, Seconds: 6.54\n",
            "Epoch   3/100 Batch 1980/14303 - Loss:  0.082, Seconds: 6.46\n",
            "Epoch   3/100 Batch 2010/14303 - Loss:  0.079, Seconds: 6.49\n",
            "Epoch   3/100 Batch 2040/14303 - Loss:  0.082, Seconds: 6.51\n",
            "Epoch   3/100 Batch 2070/14303 - Loss:  0.080, Seconds: 6.52\n",
            "Epoch   3/100 Batch 2100/14303 - Loss:  0.080, Seconds: 6.58\n",
            "Epoch   3/100 Batch 2130/14303 - Loss:  0.077, Seconds: 6.48\n",
            "Epoch   3/100 Batch 2160/14303 - Loss:  0.082, Seconds: 6.49\n",
            "Epoch   3/100 Batch 2190/14303 - Loss:  0.082, Seconds: 6.63\n",
            "Epoch   3/100 Batch 2220/14303 - Loss:  0.083, Seconds: 6.65\n",
            "Epoch   3/100 Batch 2250/14303 - Loss:  0.087, Seconds: 6.69\n",
            "Epoch   3/100 Batch 2280/14303 - Loss:  0.080, Seconds: 6.71\n",
            "Epoch   3/100 Batch 2310/14303 - Loss:  0.083, Seconds: 6.66\n",
            "Epoch   3/100 Batch 2340/14303 - Loss:  0.088, Seconds: 6.67\n",
            "Epoch   3/100 Batch 2370/14303 - Loss:  0.085, Seconds: 6.70\n",
            "Epoch   3/100 Batch 2400/14303 - Loss:  0.080, Seconds: 6.64\n",
            "Epoch   3/100 Batch 2430/14303 - Loss:  0.081, Seconds: 6.71\n",
            "Epoch   3/100 Batch 2460/14303 - Loss:  0.079, Seconds: 6.72\n",
            "Epoch   3/100 Batch 2490/14303 - Loss:  0.079, Seconds: 6.67\n",
            "Epoch   3/100 Batch 2520/14303 - Loss:  0.084, Seconds: 6.75\n",
            "Epoch   3/100 Batch 2550/14303 - Loss:  0.084, Seconds: 6.84\n",
            "Epoch   3/100 Batch 2580/14303 - Loss:  0.079, Seconds: 6.81\n",
            "Epoch   3/100 Batch 2610/14303 - Loss:  0.084, Seconds: 6.90\n",
            "Epoch   3/100 Batch 2640/14303 - Loss:  0.082, Seconds: 6.93\n",
            "Epoch   3/100 Batch 2670/14303 - Loss:  0.084, Seconds: 6.88\n",
            "Epoch   3/100 Batch 2700/14303 - Loss:  0.080, Seconds: 6.85\n",
            "Epoch   3/100 Batch 2730/14303 - Loss:  0.082, Seconds: 6.84\n",
            "Epoch   3/100 Batch 2760/14303 - Loss:  0.084, Seconds: 6.86\n",
            "Epoch   3/100 Batch 2790/14303 - Loss:  0.081, Seconds: 6.88\n",
            "Epoch   3/100 Batch 2820/14303 - Loss:  0.083, Seconds: 6.85\n",
            "Epoch   3/100 Batch 2850/14303 - Loss:  0.084, Seconds: 7.05\n",
            "Epoch   3/100 Batch 2880/14303 - Loss:  0.081, Seconds: 6.99\n",
            "Epoch   3/100 Batch 2910/14303 - Loss:  0.086, Seconds: 7.00\n",
            "Epoch   3/100 Batch 2940/14303 - Loss:  0.074, Seconds: 7.03\n",
            "Epoch   3/100 Batch 2970/14303 - Loss:  0.084, Seconds: 7.01\n",
            "Epoch   3/100 Batch 3000/14303 - Loss:  0.081, Seconds: 7.04\n",
            "Epoch   3/100 Batch 3030/14303 - Loss:  0.083, Seconds: 7.02\n",
            "Epoch   3/100 Batch 3060/14303 - Loss:  0.080, Seconds: 6.99\n",
            "Epoch   3/100 Batch 3090/14303 - Loss:  0.081, Seconds: 7.11\n",
            "Epoch   3/100 Batch 3120/14303 - Loss:  0.083, Seconds: 7.07\n",
            "Epoch   3/100 Batch 3150/14303 - Loss:  0.079, Seconds: 7.00\n",
            "Epoch   3/100 Batch 3180/14303 - Loss:  0.080, Seconds: 7.15\n",
            "Epoch   3/100 Batch 3210/14303 - Loss:  0.083, Seconds: 7.21\n",
            "Epoch   3/100 Batch 3240/14303 - Loss:  0.080, Seconds: 7.16\n",
            "Epoch   3/100 Batch 3270/14303 - Loss:  0.078, Seconds: 7.22\n",
            "Epoch   3/100 Batch 3300/14303 - Loss:  0.082, Seconds: 7.14\n",
            "Epoch   3/100 Batch 3330/14303 - Loss:  0.077, Seconds: 7.18\n",
            "Epoch   3/100 Batch 3360/14303 - Loss:  0.083, Seconds: 7.16\n",
            "Epoch   3/100 Batch 3390/14303 - Loss:  0.088, Seconds: 7.15\n",
            "Epoch   3/100 Batch 3420/14303 - Loss:  0.085, Seconds: 7.28\n",
            "Epoch   3/100 Batch 3450/14303 - Loss:  0.082, Seconds: 7.17\n",
            "Epoch   3/100 Batch 3480/14303 - Loss:  0.082, Seconds: 7.19\n",
            "Epoch   3/100 Batch 3510/14303 - Loss:  0.081, Seconds: 7.32\n",
            "Epoch   3/100 Batch 3540/14303 - Loss:  0.080, Seconds: 7.37\n",
            "Epoch   3/100 Batch 3570/14303 - Loss:  0.082, Seconds: 7.35\n",
            "Epoch   3/100 Batch 3600/14303 - Loss:  0.082, Seconds: 7.31\n",
            "Epoch   3/100 Batch 3630/14303 - Loss:  0.083, Seconds: 7.32\n",
            "Epoch   3/100 Batch 3660/14303 - Loss:  0.080, Seconds: 7.31\n",
            "Epoch   3/100 Batch 3690/14303 - Loss:  0.081, Seconds: 7.37\n",
            "Epoch   3/100 Batch 3720/14303 - Loss:  0.078, Seconds: 7.38\n",
            "Epoch   3/100 Batch 3750/14303 - Loss:  0.080, Seconds: 7.38\n",
            "Epoch   3/100 Batch 3780/14303 - Loss:  0.081, Seconds: 7.33\n",
            "Epoch   3/100 Batch 3810/14303 - Loss:  0.084, Seconds: 7.47\n",
            "Epoch   3/100 Batch 3840/14303 - Loss:  0.095, Seconds: 7.67\n",
            "Epoch   3/100 Batch 3870/14303 - Loss:  0.088, Seconds: 7.55\n",
            "Epoch   3/100 Batch 3900/14303 - Loss:  0.085, Seconds: 7.55\n",
            "Epoch   3/100 Batch 3930/14303 - Loss:  0.088, Seconds: 7.49\n",
            "Epoch   3/100 Batch 3960/14303 - Loss:  0.084, Seconds: 7.48\n",
            "Epoch   3/100 Batch 3990/14303 - Loss:  0.081, Seconds: 7.51\n",
            "Epoch   3/100 Batch 4020/14303 - Loss:  0.082, Seconds: 7.56\n",
            "Epoch   3/100 Batch 4050/14303 - Loss:  0.077, Seconds: 7.54\n",
            "Epoch   3/100 Batch 4080/14303 - Loss:  0.081, Seconds: 7.49\n",
            "Epoch   3/100 Batch 4110/14303 - Loss:  0.080, Seconds: 7.47\n",
            "Epoch   3/100 Batch 4140/14303 - Loss:  0.081, Seconds: 7.69\n",
            "Epoch   3/100 Batch 4170/14303 - Loss:  0.078, Seconds: 7.65\n",
            "Epoch   3/100 Batch 4200/14303 - Loss:  0.079, Seconds: 7.69\n",
            "Epoch   3/100 Batch 4230/14303 - Loss:  0.081, Seconds: 7.66\n",
            "Epoch   3/100 Batch 4260/14303 - Loss:  0.082, Seconds: 7.71\n",
            "Epoch   3/100 Batch 4290/14303 - Loss:  0.082, Seconds: 7.65\n",
            "Epoch   3/100 Batch 4320/14303 - Loss:  0.080, Seconds: 7.77\n",
            "Epoch   3/100 Batch 4350/14303 - Loss:  0.079, Seconds: 7.71\n",
            "Epoch   3/100 Batch 4380/14303 - Loss:  0.080, Seconds: 7.69\n",
            "Epoch   3/100 Batch 4410/14303 - Loss:  0.082, Seconds: 7.69\n",
            "Epoch   3/100 Batch 4440/14303 - Loss:  0.083, Seconds: 7.76\n",
            "Epoch   3/100 Batch 4470/14303 - Loss:  0.080, Seconds: 7.88\n",
            "Epoch   3/100 Batch 4500/14303 - Loss:  0.083, Seconds: 7.84\n",
            "Epoch   3/100 Batch 4530/14303 - Loss:  0.081, Seconds: 7.80\n",
            "Epoch   3/100 Batch 4560/14303 - Loss:  0.081, Seconds: 7.96\n",
            "Epoch   3/100 Batch 4590/14303 - Loss:  0.081, Seconds: 7.89\n",
            "Epoch   3/100 Batch 4620/14303 - Loss:  0.080, Seconds: 7.93\n",
            "Epoch   3/100 Batch 4650/14303 - Loss:  0.080, Seconds: 7.79\n",
            "Epoch   3/100 Batch 4680/14303 - Loss:  0.079, Seconds: 7.91\n",
            "Epoch   3/100 Batch 4710/14303 - Loss:  0.079, Seconds: 7.87\n",
            "Epoch   3/100 Batch 4740/14303 - Loss:  0.082, Seconds: 7.88\n",
            "Epoch   3/100 Batch 4770/14303 - Loss:  0.077, Seconds: 8.04\n",
            "Epoch   3/100 Batch 4800/14303 - Loss:  0.077, Seconds: 8.00\n",
            "Epoch   3/100 Batch 4830/14303 - Loss:  0.080, Seconds: 8.03\n",
            "Epoch   3/100 Batch 4860/14303 - Loss:  0.081, Seconds: 8.02\n",
            "Epoch   3/100 Batch 4890/14303 - Loss:  0.080, Seconds: 8.01\n",
            "Epoch   3/100 Batch 4920/14303 - Loss:  0.077, Seconds: 8.05\n",
            "Epoch   3/100 Batch 4950/14303 - Loss:  0.081, Seconds: 8.13\n",
            "Epoch   3/100 Batch 4980/14303 - Loss:  0.079, Seconds: 8.16\n",
            "Epoch   3/100 Batch 5010/14303 - Loss:  0.081, Seconds: 8.00\n",
            "Epoch   3/100 Batch 5040/14303 - Loss:  0.079, Seconds: 7.99\n",
            "Epoch   3/100 Batch 5070/14303 - Loss:  0.082, Seconds: 8.21\n",
            "Epoch   3/100 Batch 5100/14303 - Loss:  0.080, Seconds: 8.21\n",
            "Epoch   3/100 Batch 5130/14303 - Loss:  0.081, Seconds: 8.13\n",
            "Epoch   3/100 Batch 5160/14303 - Loss:  0.081, Seconds: 8.26\n",
            "Epoch   3/100 Batch 5190/14303 - Loss:  0.079, Seconds: 8.24\n",
            "Epoch   3/100 Batch 5220/14303 - Loss:  0.079, Seconds: 8.16\n",
            "Epoch   3/100 Batch 5250/14303 - Loss:  0.077, Seconds: 8.15\n",
            "Epoch   3/100 Batch 5280/14303 - Loss:  0.083, Seconds: 8.16\n",
            "Epoch   3/100 Batch 5310/14303 - Loss:  0.080, Seconds: 8.20\n",
            "Epoch   3/100 Batch 5340/14303 - Loss:  0.079, Seconds: 8.13\n",
            "Epoch   3/100 Batch 5370/14303 - Loss:  0.084, Seconds: 8.31\n",
            "Epoch   3/100 Batch 5400/14303 - Loss:  0.084, Seconds: 8.36\n",
            "Epoch   3/100 Batch 5430/14303 - Loss:  0.078, Seconds: 8.37\n",
            "Epoch   3/100 Batch 5460/14303 - Loss:  0.078, Seconds: 8.44\n",
            "Epoch   3/100 Batch 5490/14303 - Loss:  0.079, Seconds: 8.36\n",
            "Epoch   3/100 Batch 5520/14303 - Loss:  0.085, Seconds: 8.32\n",
            "Epoch   3/100 Batch 5550/14303 - Loss:  0.079, Seconds: 8.34\n",
            "Epoch   3/100 Batch 5580/14303 - Loss:  0.081, Seconds: 8.34\n",
            "Epoch   3/100 Batch 5610/14303 - Loss:  0.077, Seconds: 8.46\n",
            "Epoch   3/100 Batch 5640/14303 - Loss:  0.080, Seconds: 8.38\n",
            "Epoch   3/100 Batch 5670/14303 - Loss:  0.080, Seconds: 8.49\n",
            "Epoch   3/100 Batch 5700/14303 - Loss:  0.079, Seconds: 8.51\n",
            "Epoch   3/100 Batch 5730/14303 - Loss:  0.081, Seconds: 8.59\n",
            "Epoch   3/100 Batch 5760/14303 - Loss:  0.078, Seconds: 8.50\n",
            "Epoch   3/100 Batch 5790/14303 - Loss:  0.079, Seconds: 8.49\n",
            "Epoch   3/100 Batch 5820/14303 - Loss:  0.083, Seconds: 8.53\n",
            "Epoch   3/100 Batch 5850/14303 - Loss:  0.079, Seconds: 8.46\n",
            "Epoch   3/100 Batch 5880/14303 - Loss:  0.081, Seconds: 8.50\n",
            "Epoch   3/100 Batch 5910/14303 - Loss:  0.079, Seconds: 8.53\n",
            "Epoch   3/100 Batch 5940/14303 - Loss:  0.079, Seconds: 8.54\n",
            "Epoch   3/100 Batch 5970/14303 - Loss:  0.079, Seconds: 8.66\n",
            "Epoch   3/100 Batch 6000/14303 - Loss:  0.083, Seconds: 8.84\n",
            "Epoch   3/100 Batch 6030/14303 - Loss:  0.079, Seconds: 8.78\n",
            "Epoch   3/100 Batch 6060/14303 - Loss:  0.079, Seconds: 8.71\n",
            "Epoch   3/100 Batch 6090/14303 - Loss:  0.082, Seconds: 8.67\n",
            "Epoch   3/100 Batch 6120/14303 - Loss:  0.078, Seconds: 8.67\n",
            "Epoch   3/100 Batch 6150/14303 - Loss:  0.080, Seconds: 8.72\n",
            "Epoch   3/100 Batch 6180/14303 - Loss:  0.082, Seconds: 8.63\n",
            "Epoch   3/100 Batch 6210/14303 - Loss:  0.081, Seconds: 8.73\n",
            "Epoch   3/100 Batch 6240/14303 - Loss:  0.078, Seconds: 8.92\n",
            "Epoch   3/100 Batch 6270/14303 - Loss:  0.079, Seconds: 8.89\n",
            "Epoch   3/100 Batch 6300/14303 - Loss:  0.078, Seconds: 8.99\n",
            "Epoch   3/100 Batch 6330/14303 - Loss:  0.077, Seconds: 8.85\n",
            "Epoch   3/100 Batch 6360/14303 - Loss:  0.077, Seconds: 8.90\n",
            "Epoch   3/100 Batch 6390/14303 - Loss:  0.074, Seconds: 8.93\n",
            "Epoch   3/100 Batch 6420/14303 - Loss:  0.080, Seconds: 8.88\n",
            "Epoch   3/100 Batch 6450/14303 - Loss:  0.080, Seconds: 8.90\n",
            "Epoch   3/100 Batch 6480/14303 - Loss:  0.082, Seconds: 8.91\n",
            "Epoch   3/100 Batch 6510/14303 - Loss:  0.080, Seconds: 8.97\n",
            "Epoch   3/100 Batch 6540/14303 - Loss:  0.077, Seconds: 9.05\n",
            "Epoch   3/100 Batch 6570/14303 - Loss:  0.077, Seconds: 9.14\n",
            "Epoch   3/100 Batch 6600/14303 - Loss:  0.078, Seconds: 9.22\n",
            "Epoch   3/100 Batch 6630/14303 - Loss:  0.082, Seconds: 9.06\n",
            "Epoch   3/100 Batch 6660/14303 - Loss:  0.081, Seconds: 9.03\n",
            "Epoch   3/100 Batch 6690/14303 - Loss:  0.077, Seconds: 9.07\n",
            "Epoch   3/100 Batch 6720/14303 - Loss:  0.078, Seconds: 9.05\n",
            "Epoch   3/100 Batch 6750/14303 - Loss:  0.083, Seconds: 9.08\n",
            "Epoch   3/100 Batch 6780/14303 - Loss:  0.078, Seconds: 9.08\n",
            "Epoch   3/100 Batch 6810/14303 - Loss:  0.080, Seconds: 9.21\n",
            "Epoch   3/100 Batch 6840/14303 - Loss:  0.080, Seconds: 9.22\n",
            "Epoch   3/100 Batch 6870/14303 - Loss:  0.080, Seconds: 9.27\n",
            "Epoch   3/100 Batch 6900/14303 - Loss:  0.078, Seconds: 9.24\n",
            "Epoch   3/100 Batch 6930/14303 - Loss:  0.080, Seconds: 9.16\n",
            "Epoch   3/100 Batch 6960/14303 - Loss:  0.080, Seconds: 9.28\n",
            "Epoch   3/100 Batch 6990/14303 - Loss:  0.080, Seconds: 9.41\n",
            "Epoch   3/100 Batch 7020/14303 - Loss:  0.080, Seconds: 9.23\n",
            "Epoch   3/100 Batch 7050/14303 - Loss:  0.080, Seconds: 9.23\n",
            "Epoch   3/100 Batch 7080/14303 - Loss:  0.077, Seconds: 9.34\n",
            "Epoch   3/100 Batch 7110/14303 - Loss:  0.079, Seconds: 9.39\n",
            "Epoch   3/100 Batch 7140/14303 - Loss:  0.077, Seconds: 9.41\n",
            "Epoch   3/100 Batch 7170/14303 - Loss:  0.079, Seconds: 9.41\n",
            "Epoch   3/100 Batch 7200/14303 - Loss:  0.077, Seconds: 9.38\n",
            "Epoch   3/100 Batch 7230/14303 - Loss:  0.080, Seconds: 9.49\n",
            "Epoch   3/100 Batch 7260/14303 - Loss:  0.077, Seconds: 9.45\n",
            "Epoch   3/100 Batch 7290/14303 - Loss:  0.080, Seconds: 9.40\n",
            "Epoch   3/100 Batch 7320/14303 - Loss:  0.082, Seconds: 9.42\n",
            "Epoch   3/100 Batch 7350/14303 - Loss:  0.079, Seconds: 9.56\n",
            "Epoch   3/100 Batch 7380/14303 - Loss:  0.079, Seconds: 9.65\n",
            "Epoch   3/100 Batch 7410/14303 - Loss:  0.081, Seconds: 9.59\n",
            "Epoch   3/100 Batch 7440/14303 - Loss:  0.080, Seconds: 9.54\n",
            "Epoch   3/100 Batch 7470/14303 - Loss:  0.079, Seconds: 9.57\n",
            "Epoch   3/100 Batch 7500/14303 - Loss:  0.075, Seconds: 9.65\n",
            "Epoch   3/100 Batch 7530/14303 - Loss:  0.078, Seconds: 9.71\n",
            "Epoch   3/100 Batch 7560/14303 - Loss:  0.078, Seconds: 9.59\n",
            "Epoch   3/100 Batch 7590/14303 - Loss:  0.075, Seconds: 9.56\n",
            "Epoch   3/100 Batch 7620/14303 - Loss:  0.078, Seconds: 9.69\n",
            "Epoch   3/100 Batch 7650/14303 - Loss:  0.079, Seconds: 9.79\n",
            "Epoch   3/100 Batch 7680/14303 - Loss:  0.081, Seconds: 9.73\n",
            "Epoch   3/100 Batch 7710/14303 - Loss:  0.077, Seconds: 9.76\n",
            "Epoch   3/100 Batch 7740/14303 - Loss:  0.079, Seconds: 9.72\n",
            "Epoch   3/100 Batch 7770/14303 - Loss:  0.078, Seconds: 9.75\n",
            "Epoch   3/100 Batch 7800/14303 - Loss:  0.078, Seconds: 9.74\n",
            "Epoch   3/100 Batch 7830/14303 - Loss:  0.081, Seconds: 9.69\n",
            "Epoch   3/100 Batch 7860/14303 - Loss:  0.077, Seconds: 9.80\n",
            "Epoch   3/100 Batch 7890/14303 - Loss:  0.077, Seconds: 9.99\n",
            "Epoch   3/100 Batch 7920/14303 - Loss:  0.076, Seconds: 10.02\n",
            "Epoch   3/100 Batch 7950/14303 - Loss:  0.084, Seconds: 9.95\n",
            "Epoch   3/100 Batch 7980/14303 - Loss:  0.077, Seconds: 9.88\n",
            "Epoch   3/100 Batch 8010/14303 - Loss:  0.076, Seconds: 9.97\n",
            "Epoch   3/100 Batch 8040/14303 - Loss:  0.079, Seconds: 9.91\n",
            "Epoch   3/100 Batch 8070/14303 - Loss:  0.072, Seconds: 9.94\n",
            "Epoch   3/100 Batch 8100/14303 - Loss:  0.076, Seconds: 9.95\n",
            "Epoch   3/100 Batch 8130/14303 - Loss:  0.079, Seconds: 10.03\n",
            "Epoch   3/100 Batch 8160/14303 - Loss:  0.076, Seconds: 10.17\n",
            "Epoch   3/100 Batch 8190/14303 - Loss:  0.080, Seconds: 10.10\n",
            "Epoch   3/100 Batch 8220/14303 - Loss:  0.077, Seconds: 10.13\n",
            "Epoch   3/100 Batch 8250/14303 - Loss:  0.078, Seconds: 10.05\n",
            "Epoch   3/100 Batch 8280/14303 - Loss:  0.077, Seconds: 10.06\n",
            "Epoch   3/100 Batch 8310/14303 - Loss:  0.077, Seconds: 10.07\n",
            "Epoch   3/100 Batch 8340/14303 - Loss:  0.080, Seconds: 10.06\n",
            "Epoch   3/100 Batch 8370/14303 - Loss:  0.080, Seconds: 10.29\n",
            "Epoch   3/100 Batch 8400/14303 - Loss:  0.078, Seconds: 10.37\n",
            "Epoch   3/100 Batch 8430/14303 - Loss:  0.078, Seconds: 10.27\n",
            "Epoch   3/100 Batch 8460/14303 - Loss:  0.076, Seconds: 10.24\n",
            "Epoch   3/100 Batch 8490/14303 - Loss:  0.081, Seconds: 10.27\n",
            "Epoch   3/100 Batch 8520/14303 - Loss:  0.077, Seconds: 10.23\n",
            "Epoch   3/100 Batch 8550/14303 - Loss:  0.073, Seconds: 10.25\n",
            "Epoch   3/100 Batch 8580/14303 - Loss:  0.076, Seconds: 10.28\n",
            "Epoch   3/100 Batch 8610/14303 - Loss:  0.078, Seconds: 10.52\n",
            "Epoch   3/100 Batch 8640/14303 - Loss:  0.078, Seconds: 10.43\n",
            "Epoch   3/100 Batch 8670/14303 - Loss:  0.077, Seconds: 10.45\n",
            "Epoch   3/100 Batch 8700/14303 - Loss:  0.075, Seconds: 10.46\n",
            "Epoch   3/100 Batch 8730/14303 - Loss:  0.078, Seconds: 10.44\n",
            "Epoch   3/100 Batch 8760/14303 - Loss:  0.075, Seconds: 10.63\n",
            "Epoch   3/100 Batch 8790/14303 - Loss:  0.077, Seconds: 10.51\n",
            "Epoch   3/100 Batch 8820/14303 - Loss:  0.077, Seconds: 10.52\n",
            "Epoch   3/100 Batch 8850/14303 - Loss:  0.078, Seconds: 10.66\n",
            "Epoch   3/100 Batch 8880/14303 - Loss:  0.081, Seconds: 10.63\n",
            "Epoch   3/100 Batch 8910/14303 - Loss:  0.078, Seconds: 10.56\n",
            "Epoch   3/100 Batch 8940/14303 - Loss:  0.080, Seconds: 10.59\n",
            "Epoch   3/100 Batch 8970/14303 - Loss:  0.075, Seconds: 10.63\n",
            "Epoch   3/100 Batch 9000/14303 - Loss:  0.078, Seconds: 10.62\n",
            "Epoch   3/100 Batch 9030/14303 - Loss:  0.078, Seconds: 10.67\n",
            "Epoch   3/100 Batch 9060/14303 - Loss:  0.076, Seconds: 10.89\n",
            "Epoch   3/100 Batch 9090/14303 - Loss:  0.075, Seconds: 10.81\n",
            "Epoch   3/100 Batch 9120/14303 - Loss:  0.079, Seconds: 10.81\n",
            "Epoch   3/100 Batch 9150/14303 - Loss:  0.076, Seconds: 10.79\n",
            "Epoch   3/100 Batch 9180/14303 - Loss:  0.075, Seconds: 10.77\n",
            "Epoch   3/100 Batch 9210/14303 - Loss:  0.075, Seconds: 10.91\n",
            "Epoch   3/100 Batch 9240/14303 - Loss:  0.074, Seconds: 10.85\n",
            "Epoch   3/100 Batch 9270/14303 - Loss:  0.077, Seconds: 10.99\n",
            "Epoch   3/100 Batch 9300/14303 - Loss:  0.074, Seconds: 11.02\n",
            "Epoch   3/100 Batch 9330/14303 - Loss:  0.073, Seconds: 11.01\n",
            "Epoch   3/100 Batch 9360/14303 - Loss:  0.078, Seconds: 10.96\n",
            "Epoch   3/100 Batch 9390/14303 - Loss:  0.079, Seconds: 11.00\n",
            "Epoch   3/100 Batch 9420/14303 - Loss:  0.074, Seconds: 10.96\n",
            "Epoch   3/100 Batch 9450/14303 - Loss:  0.077, Seconds: 11.06\n",
            "Epoch   3/100 Batch 9480/14303 - Loss:  0.072, Seconds: 11.18\n",
            "Epoch   3/100 Batch 9510/14303 - Loss:  0.076, Seconds: 11.16\n",
            "Epoch   3/100 Batch 9540/14303 - Loss:  0.076, Seconds: 11.17\n",
            "Epoch   3/100 Batch 9570/14303 - Loss:  0.076, Seconds: 11.34\n",
            "Epoch   3/100 Batch 9600/14303 - Loss:  0.078, Seconds: 11.22\n",
            "Epoch   3/100 Batch 9630/14303 - Loss:  0.080, Seconds: 11.18\n",
            "Epoch   3/100 Batch 9660/14303 - Loss:  0.076, Seconds: 11.26\n",
            "Epoch   3/100 Batch 9690/14303 - Loss:  0.077, Seconds: 11.33\n",
            "Epoch   3/100 Batch 9720/14303 - Loss:  0.075, Seconds: 11.32\n",
            "Epoch   3/100 Batch 9750/14303 - Loss:  0.074, Seconds: 11.36\n",
            "Epoch   3/100 Batch 9780/14303 - Loss:  0.075, Seconds: 11.27\n",
            "Epoch   3/100 Batch 9810/14303 - Loss:  0.075, Seconds: 11.33\n",
            "Epoch   3/100 Batch 9840/14303 - Loss:  0.075, Seconds: 11.48\n",
            "Epoch   3/100 Batch 9870/14303 - Loss:  0.079, Seconds: 11.55\n",
            "Epoch   3/100 Batch 9900/14303 - Loss:  0.078, Seconds: 11.52\n",
            "Epoch   3/100 Batch 9930/14303 - Loss:  0.077, Seconds: 11.53\n",
            "Epoch   3/100 Batch 9960/14303 - Loss:  0.077, Seconds: 11.55\n",
            "Epoch   3/100 Batch 9990/14303 - Loss:  0.079, Seconds: 11.51\n",
            "Epoch   3/100 Batch 10020/14303 - Loss:  0.075, Seconds: 11.52\n",
            "Epoch   3/100 Batch 10050/14303 - Loss:  0.077, Seconds: 11.60\n",
            "Epoch   3/100 Batch 10080/14303 - Loss:  0.075, Seconds: 11.71\n",
            "Epoch   3/100 Batch 10110/14303 - Loss:  0.073, Seconds: 11.67\n",
            "Epoch   3/100 Batch 10140/14303 - Loss:  0.073, Seconds: 11.71\n",
            "Epoch   3/100 Batch 10170/14303 - Loss:  0.075, Seconds: 11.67\n",
            "Epoch   3/100 Batch 10200/14303 - Loss:  0.071, Seconds: 11.60\n",
            "Epoch   3/100 Batch 10230/14303 - Loss:  0.076, Seconds: 11.68\n",
            "Epoch   3/100 Batch 10260/14303 - Loss:  0.075, Seconds: 11.93\n",
            "Epoch   3/100 Batch 10290/14303 - Loss:  0.075, Seconds: 11.88\n",
            "Epoch   3/100 Batch 10320/14303 - Loss:  0.073, Seconds: 12.07\n",
            "Epoch   3/100 Batch 10350/14303 - Loss:  0.075, Seconds: 11.90\n",
            "Epoch   3/100 Batch 10380/14303 - Loss:  0.075, Seconds: 11.83\n",
            "Epoch   3/100 Batch 10410/14303 - Loss:  0.073, Seconds: 11.89\n",
            "Epoch   3/100 Batch 10440/14303 - Loss:  0.077, Seconds: 12.08\n",
            "Epoch   3/100 Batch 10470/14303 - Loss:  0.074, Seconds: 12.05\n",
            "Epoch   3/100 Batch 10500/14303 - Loss:  0.075, Seconds: 12.03\n",
            "Epoch   3/100 Batch 10530/14303 - Loss:  0.074, Seconds: 12.02\n",
            "Epoch   3/100 Batch 10560/14303 - Loss:  0.077, Seconds: 11.97\n",
            "Epoch   3/100 Batch 10590/14303 - Loss:  0.077, Seconds: 12.12\n",
            "Epoch   3/100 Batch 10620/14303 - Loss:  0.075, Seconds: 12.21\n",
            "Epoch   3/100 Batch 10650/14303 - Loss:  0.078, Seconds: 12.26\n",
            "Epoch   3/100 Batch 10680/14303 - Loss:  0.077, Seconds: 12.23\n",
            "Epoch   3/100 Batch 10710/14303 - Loss:  0.078, Seconds: 12.24\n",
            "Epoch   3/100 Batch 10740/14303 - Loss:  0.076, Seconds: 12.20\n",
            "Epoch   3/100 Batch 10770/14303 - Loss:  0.077, Seconds: 12.30\n",
            "Epoch   3/100 Batch 10800/14303 - Loss:  0.076, Seconds: 12.38\n",
            "Epoch   3/100 Batch 10830/14303 - Loss:  0.074, Seconds: 12.44\n",
            "Epoch   3/100 Batch 10860/14303 - Loss:  0.075, Seconds: 12.38\n",
            "Epoch   3/100 Batch 10890/14303 - Loss:  0.076, Seconds: 12.37\n",
            "Epoch   3/100 Batch 10920/14303 - Loss:  0.074, Seconds: 12.51\n",
            "Epoch   3/100 Batch 10950/14303 - Loss:  0.076, Seconds: 12.54\n",
            "Epoch   3/100 Batch 10980/14303 - Loss:  0.074, Seconds: 12.54\n",
            "Epoch   3/100 Batch 11010/14303 - Loss:  0.078, Seconds: 12.55\n",
            "Epoch   3/100 Batch 11040/14303 - Loss:  0.077, Seconds: 12.76\n",
            "Epoch   3/100 Batch 11070/14303 - Loss:  0.076, Seconds: 12.63\n",
            "Epoch   3/100 Batch 11100/14303 - Loss:  0.078, Seconds: 12.78\n",
            "Epoch   3/100 Batch 11130/14303 - Loss:  0.079, Seconds: 12.69\n",
            "Epoch   3/100 Batch 11160/14303 - Loss:  0.076, Seconds: 12.77\n",
            "Epoch   3/100 Batch 11190/14303 - Loss:  0.074, Seconds: 12.77\n",
            "Epoch   3/100 Batch 11220/14303 - Loss:  0.077, Seconds: 12.83\n",
            "Epoch   3/100 Batch 11250/14303 - Loss:  0.076, Seconds: 12.90\n",
            "Epoch   3/100 Batch 11280/14303 - Loss:  0.076, Seconds: 12.86\n",
            "Epoch   3/100 Batch 11310/14303 - Loss:  0.079, Seconds: 12.89\n",
            "Epoch   3/100 Batch 11340/14303 - Loss:  0.075, Seconds: 12.94\n",
            "Epoch   3/100 Batch 11370/14303 - Loss:  0.077, Seconds: 13.23\n",
            "Epoch   3/100 Batch 11400/14303 - Loss:  0.072, Seconds: 13.07\n",
            "Epoch   3/100 Batch 11430/14303 - Loss:  0.073, Seconds: 13.04\n",
            "Epoch   3/100 Batch 11460/14303 - Loss:  0.076, Seconds: 13.08\n",
            "Epoch   3/100 Batch 11490/14303 - Loss:  0.072, Seconds: 13.15\n",
            "Epoch   3/100 Batch 11520/14303 - Loss:  0.074, Seconds: 13.23\n",
            "Epoch   3/100 Batch 11550/14303 - Loss:  0.077, Seconds: 13.19\n",
            "Epoch   3/100 Batch 11580/14303 - Loss:  0.073, Seconds: 13.26\n",
            "Epoch   3/100 Batch 11610/14303 - Loss:  0.075, Seconds: 13.27\n",
            "Epoch   3/100 Batch 11640/14303 - Loss:  0.076, Seconds: 13.40\n",
            "Epoch   3/100 Batch 11670/14303 - Loss:  0.072, Seconds: 13.34\n",
            "Epoch   3/100 Batch 11700/14303 - Loss:  0.076, Seconds: 14.22\n",
            "Epoch   3/100 Batch 11730/14303 - Loss:  0.076, Seconds: 13.62\n",
            "Epoch   3/100 Batch 11760/14303 - Loss:  0.074, Seconds: 13.63\n",
            "Epoch   3/100 Batch 11790/14303 - Loss:  0.076, Seconds: 13.56\n",
            "Epoch   3/100 Batch 11820/14303 - Loss:  0.074, Seconds: 13.58\n",
            "Epoch   3/100 Batch 11850/14303 - Loss:  0.079, Seconds: 13.61\n",
            "Epoch   3/100 Batch 11880/14303 - Loss:  0.075, Seconds: 13.79\n",
            "Epoch   3/100 Batch 11910/14303 - Loss:  0.072, Seconds: 13.77\n",
            "Epoch   3/100 Batch 11940/14303 - Loss:  0.072, Seconds: 13.72\n",
            "Epoch   3/100 Batch 11970/14303 - Loss:  0.074, Seconds: 13.78\n",
            "Epoch   3/100 Batch 12000/14303 - Loss:  0.075, Seconds: 14.00\n",
            "Epoch   3/100 Batch 12030/14303 - Loss:  0.073, Seconds: 14.08\n",
            "Epoch   3/100 Batch 12060/14303 - Loss:  0.074, Seconds: 13.91\n",
            "Epoch   3/100 Batch 12090/14303 - Loss:  0.072, Seconds: 14.04\n",
            "Epoch   3/100 Batch 12120/14303 - Loss:  0.074, Seconds: 14.06\n",
            "Epoch   3/100 Batch 12150/14303 - Loss:  0.077, Seconds: 14.10\n",
            "Epoch   3/100 Batch 12180/14303 - Loss:  0.076, Seconds: 14.17\n",
            "Epoch   3/100 Batch 12210/14303 - Loss:  0.072, Seconds: 14.35\n",
            "Epoch   3/100 Batch 12240/14303 - Loss:  0.074, Seconds: 14.30\n",
            "Epoch   3/100 Batch 12270/14303 - Loss:  0.075, Seconds: 14.33\n",
            "Epoch   3/100 Batch 12300/14303 - Loss:  0.075, Seconds: 14.42\n",
            "Epoch   3/100 Batch 12330/14303 - Loss:  0.072, Seconds: 14.63\n",
            "Epoch   3/100 Batch 12360/14303 - Loss:  0.073, Seconds: 14.56\n",
            "Epoch   3/100 Batch 12390/14303 - Loss:  0.073, Seconds: 14.56\n",
            "Epoch   3/100 Batch 12420/14303 - Loss:  0.074, Seconds: 14.67\n",
            "Epoch   3/100 Batch 12450/14303 - Loss:  0.072, Seconds: 14.68\n",
            "Epoch   3/100 Batch 12480/14303 - Loss:  0.074, Seconds: 14.71\n",
            "Epoch   3/100 Batch 12510/14303 - Loss:  0.073, Seconds: 14.92\n",
            "Epoch   3/100 Batch 12540/14303 - Loss:  0.075, Seconds: 14.91\n",
            "Epoch   3/100 Batch 12570/14303 - Loss:  0.073, Seconds: 14.83\n",
            "Epoch   3/100 Batch 12600/14303 - Loss:  0.073, Seconds: 15.16\n",
            "Epoch   3/100 Batch 12630/14303 - Loss:  0.074, Seconds: 15.04\n",
            "Epoch   3/100 Batch 12660/14303 - Loss:  0.073, Seconds: 15.19\n",
            "Epoch   3/100 Batch 12690/14303 - Loss:  0.072, Seconds: 15.24\n",
            "Epoch   3/100 Batch 12720/14303 - Loss:  0.071, Seconds: 15.24\n",
            "Epoch   3/100 Batch 12750/14303 - Loss:  0.072, Seconds: 15.36\n",
            "Epoch   3/100 Batch 12780/14303 - Loss:  0.072, Seconds: 15.48\n",
            "Epoch   3/100 Batch 12810/14303 - Loss:  0.072, Seconds: 15.56\n",
            "Epoch   3/100 Batch 12840/14303 - Loss:  0.076, Seconds: 15.60\n",
            "Epoch   3/100 Batch 12870/14303 - Loss:  0.072, Seconds: 15.57\n",
            "Epoch   3/100 Batch 12900/14303 - Loss:  0.071, Seconds: 15.81\n",
            "Epoch   3/100 Batch 12930/14303 - Loss:  0.072, Seconds: 15.98\n",
            "Epoch   3/100 Batch 12960/14303 - Loss:  0.078, Seconds: 15.89\n",
            "Epoch   3/100 Batch 12990/14303 - Loss:  0.078, Seconds: 16.04\n",
            "Epoch   3/100 Batch 13020/14303 - Loss:  0.266, Seconds: 15.99\n",
            "Epoch   3/100 Batch 13050/14303 - Loss:  0.133, Seconds: 16.24\n",
            "Epoch   3/100 Batch 13080/14303 - Loss:  0.108, Seconds: 16.28\n",
            "Epoch   3/100 Batch 13110/14303 - Loss:  0.099, Seconds: 16.36\n",
            "Epoch   3/100 Batch 13140/14303 - Loss:  0.095, Seconds: 16.42\n",
            "Epoch   3/100 Batch 13170/14303 - Loss:  0.089, Seconds: 16.71\n",
            "Epoch   3/100 Batch 13200/14303 - Loss:  0.088, Seconds: 16.71\n",
            "Epoch   3/100 Batch 13230/14303 - Loss:  0.087, Seconds: 16.91\n",
            "Epoch   3/100 Batch 13260/14303 - Loss:  0.085, Seconds: 16.86\n",
            "Epoch   3/100 Batch 13290/14303 - Loss:  0.082, Seconds: 17.06\n",
            "Epoch   3/100 Batch 13320/14303 - Loss:  0.082, Seconds: 17.12\n",
            "Epoch   3/100 Batch 13350/14303 - Loss:  0.080, Seconds: 17.25\n",
            "Epoch   3/100 Batch 13380/14303 - Loss:  0.081, Seconds: 17.27\n",
            "Epoch   3/100 Batch 13410/14303 - Loss:  0.079, Seconds: 17.43\n",
            "Epoch   3/100 Batch 13440/14303 - Loss:  0.077, Seconds: 17.73\n",
            "Epoch   3/100 Batch 13470/14303 - Loss:  0.078, Seconds: 17.72\n",
            "Epoch   3/100 Batch 13500/14303 - Loss:  0.081, Seconds: 17.89\n",
            "Epoch   3/100 Batch 13530/14303 - Loss:  0.078, Seconds: 18.01\n",
            "Epoch   3/100 Batch 13560/14303 - Loss:  0.077, Seconds: 18.23\n",
            "Epoch   3/100 Batch 13590/14303 - Loss:  0.077, Seconds: 18.30\n",
            "Epoch   3/100 Batch 13620/14303 - Loss:  0.076, Seconds: 18.55\n",
            "Epoch   3/100 Batch 13650/14303 - Loss:  0.077, Seconds: 18.73\n",
            "Epoch   3/100 Batch 13680/14303 - Loss:  0.077, Seconds: 18.84\n",
            "Epoch   3/100 Batch 13710/14303 - Loss:  0.076, Seconds: 19.05\n",
            "Epoch   3/100 Batch 13740/14303 - Loss:  0.075, Seconds: 19.31\n",
            "Epoch   3/100 Batch 13770/14303 - Loss:  0.074, Seconds: 19.41\n",
            "Epoch   3/100 Batch 13800/14303 - Loss:  0.076, Seconds: 19.71\n",
            "Epoch   3/100 Batch 13830/14303 - Loss:  0.075, Seconds: 19.86\n",
            "Epoch   3/100 Batch 13860/14303 - Loss:  0.075, Seconds: 20.21\n",
            "Epoch   3/100 Batch 13890/14303 - Loss:  0.071, Seconds: 20.37\n",
            "Epoch   3/100 Batch 13920/14303 - Loss:  0.073, Seconds: 20.74\n",
            "Epoch   3/100 Batch 13950/14303 - Loss:  0.075, Seconds: 21.04\n",
            "Epoch   3/100 Batch 13980/14303 - Loss:  0.078, Seconds: 21.35\n",
            "Epoch   3/100 Batch 14010/14303 - Loss:  0.071, Seconds: 21.59\n",
            "Epoch   3/100 Batch 14040/14303 - Loss:  0.073, Seconds: 22.02\n",
            "Epoch   3/100 Batch 14070/14303 - Loss:  0.072, Seconds: 22.63\n",
            "Epoch   3/100 Batch 14100/14303 - Loss:  0.075, Seconds: 22.97\n",
            "Epoch   3/100 Batch 14130/14303 - Loss:  0.074, Seconds: 23.57\n",
            "Epoch   3/100 Batch 14160/14303 - Loss:  0.076, Seconds: 24.24\n",
            "Epoch   3/100 Batch 14190/14303 - Loss:  0.073, Seconds: 24.95\n",
            "Epoch   3/100 Batch 14220/14303 - Loss:  0.074, Seconds: 26.12\n",
            "Epoch   3/100 Batch 14250/14303 - Loss:  0.074, Seconds: 27.57\n",
            "Epoch   3/100 Batch 14280/14303 - Loss:  0.075, Seconds: 30.15\n",
            "  Validation Input: no tas bad as one sandal think\n",
            "  Validation Output: not as bad as bad as one sandal as\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it si alsou called granth sahib\n",
            "  Validation Output: it is also also called grant has ha\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: i is itnendetd fo radults onlyb\n",
            "  Validation Output: it is in its intended for adults on\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reason things outf or yoursef\n",
            "  Validation Output: reason reason things out things o\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it happened to wor quite well\n",
            "  Validation Output: it happened to happened to work q\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: chef and i are doing ery well\n",
            "  Validation Output: chef and i are doing their are do\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: center fro the performing arts\n",
            "  Validation Output: center for the center for the perf\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: sponintoj a hot bowrl o serve\n",
            "  Validation Output: spon into a hot bowlr of toward t\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: peolemeet hern and theyswoon\n",
            "  Validation Output: people meet her here and then an\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my poorl itte head iis boniuling\n",
            "  Validation Output: my poor little poor little head is b\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: trhis had biecme a common scene\n",
            "  Validation Output: this had become a common scene a co\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: evenb went throug hthe speials\n",
            "  Validation Output: even be went went through the spec\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rpeheata  steamer on high heat\n",
            "  Validation Output: repheat a repeat a steamer on high\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lol well gsomoene had tos ay t\n",
            "  Validation Output: loll well someone had to someone h\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he wsa weaving mats on the lst\n",
            "  Validation Output: he was a weaking mats on the last\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: thi sis wrong fotr mnay reasons\n",
            "  Validation Output: this is wrong for many for many rea\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: today wel headued again to cubao\n",
            "  Validation Output: today well headed again to cubaging\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: r pad member status requird\n",
            "  Validation Output: ror pad member status require\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it nsowed midlineg hard to day\n",
            "  Validation Output: it snowed minds wed middling har\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: they bothw ear letaher sanadls\n",
            "  Validation Output: they both both wear both wear leat\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  1.016, Seconds: 0.00\n",
            "No Improvement.\n",
            "\n",
            "Training Model: 4\n",
            "Epoch   4/100 Batch   30/14303 - Loss:  0.176, Seconds: 5.94\n",
            "Epoch   4/100 Batch   60/14303 - Loss:  0.097, Seconds: 5.83\n",
            "Epoch   4/100 Batch   90/14303 - Loss:  0.086, Seconds: 5.72\n",
            "Epoch   4/100 Batch  120/14303 - Loss:  0.087, Seconds: 5.68\n",
            "Epoch   4/100 Batch  150/14303 - Loss:  0.086, Seconds: 5.66\n",
            "Epoch   4/100 Batch  180/14303 - Loss:  0.086, Seconds: 5.62\n",
            "Epoch   4/100 Batch  210/14303 - Loss:  0.085, Seconds: 5.68\n",
            "Epoch   4/100 Batch  240/14303 - Loss:  0.086, Seconds: 5.70\n",
            "Epoch   4/100 Batch  270/14303 - Loss:  0.083, Seconds: 5.69\n",
            "Epoch   4/100 Batch  300/14303 - Loss:  0.110, Seconds: 5.69\n",
            "Epoch   4/100 Batch  330/14303 - Loss:  0.079, Seconds: 5.85\n",
            "Epoch   4/100 Batch  360/14303 - Loss:  0.081, Seconds: 5.89\n",
            "Epoch   4/100 Batch  390/14303 - Loss:  0.080, Seconds: 5.90\n",
            "Epoch   4/100 Batch  420/14303 - Loss:  0.075, Seconds: 5.85\n",
            "Epoch   4/100 Batch  450/14303 - Loss:  0.078, Seconds: 5.83\n",
            "Epoch   4/100 Batch  480/14303 - Loss:  0.077, Seconds: 5.85\n",
            "Epoch   4/100 Batch  510/14303 - Loss:  0.082, Seconds: 5.85\n",
            "Epoch   4/100 Batch  540/14303 - Loss:  0.076, Seconds: 5.85\n",
            "Epoch   4/100 Batch  570/14303 - Loss:  0.078, Seconds: 5.83\n",
            "Epoch   4/100 Batch  600/14303 - Loss:  0.091, Seconds: 5.85\n",
            "Epoch   4/100 Batch  630/14303 - Loss:  0.078, Seconds: 6.10\n",
            "Epoch   4/100 Batch  660/14303 - Loss:  0.077, Seconds: 6.03\n",
            "Epoch   4/100 Batch  690/14303 - Loss:  0.076, Seconds: 6.03\n",
            "Epoch   4/100 Batch  720/14303 - Loss:  0.081, Seconds: 5.97\n",
            "Epoch   4/100 Batch  750/14303 - Loss:  0.076, Seconds: 6.08\n",
            "Epoch   4/100 Batch  780/14303 - Loss:  0.076, Seconds: 6.04\n",
            "Epoch   4/100 Batch  810/14303 - Loss:  0.078, Seconds: 6.01\n",
            "Epoch   4/100 Batch  840/14303 - Loss:  0.077, Seconds: 5.98\n",
            "Epoch   4/100 Batch  870/14303 - Loss:  0.079, Seconds: 6.01\n",
            "Epoch   4/100 Batch  900/14303 - Loss:  0.079, Seconds: 5.99\n",
            "Epoch   4/100 Batch  930/14303 - Loss:  0.094, Seconds: 6.07\n",
            "Epoch   4/100 Batch  960/14303 - Loss:  0.078, Seconds: 6.20\n",
            "Epoch   4/100 Batch  990/14303 - Loss:  0.077, Seconds: 6.16\n",
            "Epoch   4/100 Batch 1020/14303 - Loss:  0.079, Seconds: 6.20\n",
            "Epoch   4/100 Batch 1050/14303 - Loss:  0.081, Seconds: 6.19\n",
            "Epoch   4/100 Batch 1080/14303 - Loss:  0.075, Seconds: 6.15\n",
            "Epoch   4/100 Batch 1110/14303 - Loss:  0.073, Seconds: 6.26\n",
            "Epoch   4/100 Batch 1140/14303 - Loss:  0.073, Seconds: 6.18\n",
            "Epoch   4/100 Batch 1170/14303 - Loss:  0.078, Seconds: 6.22\n",
            "Epoch   4/100 Batch 1200/14303 - Loss:  0.075, Seconds: 6.22\n",
            "Epoch   4/100 Batch 1230/14303 - Loss:  0.081, Seconds: 6.22\n",
            "Epoch   4/100 Batch 1260/14303 - Loss:  0.075, Seconds: 6.34\n",
            "Epoch   4/100 Batch 1290/14303 - Loss:  0.076, Seconds: 6.33\n",
            "Epoch   4/100 Batch 1320/14303 - Loss:  0.080, Seconds: 6.31\n",
            "Epoch   4/100 Batch 1350/14303 - Loss:  0.075, Seconds: 6.35\n",
            "Epoch   4/100 Batch 1380/14303 - Loss:  0.073, Seconds: 6.34\n",
            "Epoch   4/100 Batch 1410/14303 - Loss:  0.075, Seconds: 6.31\n",
            "Epoch   4/100 Batch 1440/14303 - Loss:  0.075, Seconds: 6.36\n",
            "Epoch   4/100 Batch 1470/14303 - Loss:  0.075, Seconds: 6.44\n",
            "Epoch   4/100 Batch 1500/14303 - Loss:  0.075, Seconds: 6.51\n",
            "Epoch   4/100 Batch 1530/14303 - Loss:  0.074, Seconds: 6.37\n",
            "Epoch   4/100 Batch 1560/14303 - Loss:  0.086, Seconds: 6.42\n",
            "Epoch   4/100 Batch 1590/14303 - Loss:  0.073, Seconds: 6.48\n",
            "Epoch   4/100 Batch 1620/14303 - Loss:  0.075, Seconds: 6.57\n",
            "Epoch   4/100 Batch 1650/14303 - Loss:  0.073, Seconds: 6.56\n",
            "Epoch   4/100 Batch 1680/14303 - Loss:  0.074, Seconds: 6.54\n",
            "Epoch   4/100 Batch 1710/14303 - Loss:  0.073, Seconds: 6.50\n",
            "Epoch   4/100 Batch 1740/14303 - Loss:  0.076, Seconds: 6.53\n",
            "Epoch   4/100 Batch 1770/14303 - Loss:  0.074, Seconds: 6.48\n",
            "Epoch   4/100 Batch 1800/14303 - Loss:  0.076, Seconds: 6.54\n",
            "Epoch   4/100 Batch 1830/14303 - Loss:  0.073, Seconds: 6.58\n",
            "Epoch   4/100 Batch 1860/14303 - Loss:  0.076, Seconds: 6.52\n",
            "Epoch   4/100 Batch 1890/14303 - Loss:  0.077, Seconds: 6.62\n",
            "Epoch   4/100 Batch 1920/14303 - Loss:  0.072, Seconds: 6.70\n",
            "Epoch   4/100 Batch 1950/14303 - Loss:  0.072, Seconds: 6.70\n",
            "Epoch   4/100 Batch 1980/14303 - Loss:  0.076, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2010/14303 - Loss:  0.071, Seconds: 6.66\n",
            "Epoch   4/100 Batch 2040/14303 - Loss:  0.073, Seconds: 6.68\n",
            "Epoch   4/100 Batch 2070/14303 - Loss:  0.071, Seconds: 6.65\n",
            "Epoch   4/100 Batch 2100/14303 - Loss:  0.072, Seconds: 6.67\n",
            "Epoch   4/100 Batch 2130/14303 - Loss:  0.070, Seconds: 6.72\n",
            "Epoch   4/100 Batch 2160/14303 - Loss:  0.076, Seconds: 6.72\n",
            "Epoch   4/100 Batch 2190/14303 - Loss:  0.082, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2220/14303 - Loss:  0.074, Seconds: 6.85\n",
            "Epoch   4/100 Batch 2250/14303 - Loss:  0.075, Seconds: 6.82\n",
            "Epoch   4/100 Batch 2280/14303 - Loss:  0.071, Seconds: 6.82\n",
            "Epoch   4/100 Batch 2310/14303 - Loss:  0.075, Seconds: 6.86\n",
            "Epoch   4/100 Batch 2340/14303 - Loss:  0.075, Seconds: 6.83\n",
            "Epoch   4/100 Batch 2370/14303 - Loss:  0.074, Seconds: 6.81\n",
            "Epoch   4/100 Batch 2400/14303 - Loss:  0.072, Seconds: 6.83\n",
            "Epoch   4/100 Batch 2430/14303 - Loss:  0.072, Seconds: 6.81\n",
            "Epoch   4/100 Batch 2460/14303 - Loss:  0.075, Seconds: 6.93\n",
            "Epoch   4/100 Batch 2490/14303 - Loss:  0.076, Seconds: 6.89\n",
            "Epoch   4/100 Batch 2520/14303 - Loss:  0.075, Seconds: 6.97\n",
            "Epoch   4/100 Batch 2550/14303 - Loss:  0.073, Seconds: 6.98\n",
            "Epoch   4/100 Batch 2580/14303 - Loss:  0.076, Seconds: 6.99\n",
            "Epoch   4/100 Batch 2610/14303 - Loss:  0.074, Seconds: 6.99\n",
            "Epoch   4/100 Batch 2640/14303 - Loss:  0.072, Seconds: 7.01\n",
            "Epoch   4/100 Batch 2670/14303 - Loss:  0.074, Seconds: 6.98\n",
            "Epoch   4/100 Batch 2700/14303 - Loss:  0.074, Seconds: 7.02\n",
            "Epoch   4/100 Batch 2730/14303 - Loss:  0.070, Seconds: 6.92\n",
            "Epoch   4/100 Batch 2760/14303 - Loss:  0.075, Seconds: 7.01\n",
            "Epoch   4/100 Batch 2790/14303 - Loss:  0.075, Seconds: 7.09\n",
            "Epoch   4/100 Batch 2820/14303 - Loss:  0.072, Seconds: 7.05\n",
            "Epoch   4/100 Batch 2850/14303 - Loss:  0.084, Seconds: 7.18\n",
            "Epoch   4/100 Batch 2880/14303 - Loss:  0.073, Seconds: 7.15\n",
            "Epoch   4/100 Batch 2910/14303 - Loss:  0.073, Seconds: 7.14\n",
            "Epoch   4/100 Batch 2940/14303 - Loss:  0.068, Seconds: 7.19\n",
            "Epoch   4/100 Batch 2970/14303 - Loss:  0.071, Seconds: 7.25\n",
            "Epoch   4/100 Batch 3000/14303 - Loss:  0.071, Seconds: 7.18\n",
            "Epoch   4/100 Batch 3030/14303 - Loss:  0.070, Seconds: 7.17\n",
            "Epoch   4/100 Batch 3060/14303 - Loss:  0.072, Seconds: 7.21\n",
            "Epoch   4/100 Batch 3090/14303 - Loss:  0.071, Seconds: 7.19\n",
            "Epoch   4/100 Batch 3120/14303 - Loss:  0.069, Seconds: 7.21\n",
            "Epoch   4/100 Batch 3150/14303 - Loss:  0.072, Seconds: 7.22\n",
            "Epoch   4/100 Batch 3180/14303 - Loss:  0.080, Seconds: 7.34\n",
            "Epoch   4/100 Batch 3210/14303 - Loss:  0.075, Seconds: 7.47\n",
            "Epoch   4/100 Batch 3240/14303 - Loss:  0.074, Seconds: 7.39\n",
            "Epoch   4/100 Batch 3270/14303 - Loss:  0.069, Seconds: 7.36\n",
            "Epoch   4/100 Batch 3300/14303 - Loss:  0.075, Seconds: 7.36\n",
            "Epoch   4/100 Batch 3330/14303 - Loss:  0.073, Seconds: 7.34\n",
            "Epoch   4/100 Batch 3360/14303 - Loss:  0.074, Seconds: 7.34\n",
            "Epoch   4/100 Batch 3390/14303 - Loss:  0.076, Seconds: 7.32\n",
            "Epoch   4/100 Batch 3420/14303 - Loss:  0.074, Seconds: 7.32\n",
            "Epoch   4/100 Batch 3450/14303 - Loss:  0.076, Seconds: 7.43\n",
            "Epoch   4/100 Batch 3480/14303 - Loss:  0.077, Seconds: 7.41\n",
            "Epoch   4/100 Batch 3510/14303 - Loss:  0.073, Seconds: 7.49\n",
            "Epoch   4/100 Batch 3540/14303 - Loss:  0.073, Seconds: 7.51\n",
            "Epoch   4/100 Batch 3570/14303 - Loss:  0.072, Seconds: 7.52\n",
            "Epoch   4/100 Batch 3600/14303 - Loss:  0.072, Seconds: 7.49\n",
            "Epoch   4/100 Batch 3630/14303 - Loss:  0.078, Seconds: 7.49\n",
            "Epoch   4/100 Batch 3660/14303 - Loss:  0.071, Seconds: 7.49\n",
            "Epoch   4/100 Batch 3690/14303 - Loss:  0.073, Seconds: 7.56\n",
            "Epoch   4/100 Batch 3720/14303 - Loss:  0.074, Seconds: 7.51\n",
            "Epoch   4/100 Batch 3750/14303 - Loss:  0.071, Seconds: 7.58\n",
            "Epoch   4/100 Batch 3780/14303 - Loss:  0.070, Seconds: 7.50\n",
            "Epoch   4/100 Batch 3810/14303 - Loss:  0.077, Seconds: 7.61\n",
            "Epoch   4/100 Batch 3840/14303 - Loss:  0.071, Seconds: 7.67\n",
            "Epoch   4/100 Batch 3870/14303 - Loss:  0.071, Seconds: 7.64\n",
            "Epoch   4/100 Batch 3900/14303 - Loss:  0.074, Seconds: 7.71\n",
            "Epoch   4/100 Batch 3930/14303 - Loss:  0.072, Seconds: 7.72\n",
            "Epoch   4/100 Batch 3960/14303 - Loss:  0.072, Seconds: 7.79\n",
            "Epoch   4/100 Batch 3990/14303 - Loss:  0.073, Seconds: 7.76\n",
            "Epoch   4/100 Batch 4020/14303 - Loss:  0.074, Seconds: 7.73\n",
            "Epoch   4/100 Batch 4050/14303 - Loss:  0.072, Seconds: 7.78\n",
            "Epoch   4/100 Batch 4080/14303 - Loss:  0.070, Seconds: 7.60\n",
            "Epoch   4/100 Batch 4110/14303 - Loss:  0.068, Seconds: 7.67\n",
            "Epoch   4/100 Batch 4140/14303 - Loss:  0.080, Seconds: 7.82\n",
            "Epoch   4/100 Batch 4170/14303 - Loss:  0.073, Seconds: 7.80\n",
            "Epoch   4/100 Batch 4200/14303 - Loss:  0.069, Seconds: 7.83\n",
            "Epoch   4/100 Batch 4230/14303 - Loss:  0.076, Seconds: 7.83\n",
            "Epoch   4/100 Batch 4260/14303 - Loss:  0.075, Seconds: 7.83\n",
            "Epoch   4/100 Batch 4290/14303 - Loss:  0.071, Seconds: 7.84\n",
            "Epoch   4/100 Batch 4320/14303 - Loss:  0.072, Seconds: 7.90\n",
            "Epoch   4/100 Batch 4350/14303 - Loss:  0.073, Seconds: 7.90\n",
            "Epoch   4/100 Batch 4380/14303 - Loss:  0.071, Seconds: 7.79\n",
            "Epoch   4/100 Batch 4410/14303 - Loss:  0.074, Seconds: 7.81\n",
            "Epoch   4/100 Batch 4440/14303 - Loss:  0.080, Seconds: 7.88\n",
            "Epoch   4/100 Batch 4470/14303 - Loss:  0.071, Seconds: 7.98\n",
            "Epoch   4/100 Batch 4500/14303 - Loss:  0.075, Seconds: 7.95\n",
            "Epoch   4/100 Batch 4530/14303 - Loss:  0.072, Seconds: 8.02\n",
            "Epoch   4/100 Batch 4560/14303 - Loss:  0.077, Seconds: 8.03\n",
            "Epoch   4/100 Batch 4590/14303 - Loss:  0.072, Seconds: 8.08\n",
            "Epoch   4/100 Batch 4620/14303 - Loss:  0.070, Seconds: 8.09\n",
            "Epoch   4/100 Batch 4650/14303 - Loss:  0.076, Seconds: 7.97\n",
            "Epoch   4/100 Batch 4680/14303 - Loss:  0.070, Seconds: 7.98\n",
            "Epoch   4/100 Batch 4710/14303 - Loss:  0.076, Seconds: 7.98\n",
            "Epoch   4/100 Batch 4740/14303 - Loss:  0.074, Seconds: 8.00\n",
            "Epoch   4/100 Batch 4770/14303 - Loss:  0.070, Seconds: 8.18\n",
            "Epoch   4/100 Batch 4800/14303 - Loss:  0.072, Seconds: 8.15\n",
            "Epoch   4/100 Batch 4830/14303 - Loss:  0.073, Seconds: 8.12\n",
            "Epoch   4/100 Batch 4860/14303 - Loss:  0.074, Seconds: 8.21\n",
            "Epoch   4/100 Batch 4890/14303 - Loss:  0.070, Seconds: 8.25\n",
            "Epoch   4/100 Batch 4920/14303 - Loss:  0.072, Seconds: 8.23\n",
            "Epoch   4/100 Batch 4950/14303 - Loss:  0.079, Seconds: 8.24\n",
            "Epoch   4/100 Batch 4980/14303 - Loss:  0.073, Seconds: 8.16\n",
            "Epoch   4/100 Batch 5010/14303 - Loss:  0.073, Seconds: 8.12\n",
            "Epoch   4/100 Batch 5040/14303 - Loss:  0.071, Seconds: 8.15\n",
            "Epoch   4/100 Batch 5070/14303 - Loss:  0.074, Seconds: 8.48\n",
            "Epoch   4/100 Batch 5100/14303 - Loss:  0.070, Seconds: 8.38\n",
            "Epoch   4/100 Batch 5130/14303 - Loss:  0.073, Seconds: 8.35\n",
            "Epoch   4/100 Batch 5160/14303 - Loss:  0.073, Seconds: 8.37\n",
            "Epoch   4/100 Batch 5190/14303 - Loss:  0.070, Seconds: 8.40\n",
            "Epoch   4/100 Batch 5220/14303 - Loss:  0.068, Seconds: 8.32\n",
            "Epoch   4/100 Batch 5250/14303 - Loss:  0.073, Seconds: 8.32\n",
            "Epoch   4/100 Batch 5280/14303 - Loss:  0.072, Seconds: 8.28\n",
            "Epoch   4/100 Batch 5310/14303 - Loss:  0.071, Seconds: 8.37\n",
            "Epoch   4/100 Batch 5340/14303 - Loss:  0.072, Seconds: 8.34\n",
            "Epoch   4/100 Batch 5370/14303 - Loss:  0.078, Seconds: 8.51\n",
            "Epoch   4/100 Batch 5400/14303 - Loss:  0.076, Seconds: 8.58\n",
            "Epoch   4/100 Batch 5430/14303 - Loss:  0.075, Seconds: 8.56\n",
            "Epoch   4/100 Batch 5460/14303 - Loss:  0.071, Seconds: 8.55\n",
            "Epoch   4/100 Batch 5490/14303 - Loss:  0.075, Seconds: 8.45\n",
            "Epoch   4/100 Batch 5520/14303 - Loss:  0.077, Seconds: 8.46\n",
            "Epoch   4/100 Batch 5550/14303 - Loss:  0.072, Seconds: 8.49\n",
            "Epoch   4/100 Batch 5580/14303 - Loss:  0.073, Seconds: 8.53\n",
            "Epoch   4/100 Batch 5610/14303 - Loss:  0.070, Seconds: 8.45\n",
            "Epoch   4/100 Batch 5640/14303 - Loss:  0.071, Seconds: 8.48\n",
            "Epoch   4/100 Batch 5670/14303 - Loss:  0.070, Seconds: 8.64\n",
            "Epoch   4/100 Batch 5700/14303 - Loss:  0.073, Seconds: 8.75\n",
            "Epoch   4/100 Batch 5730/14303 - Loss:  0.073, Seconds: 8.68\n",
            "Epoch   4/100 Batch 5760/14303 - Loss:  0.073, Seconds: 8.68\n",
            "Epoch   4/100 Batch 5790/14303 - Loss:  0.070, Seconds: 8.62\n",
            "Epoch   4/100 Batch 5820/14303 - Loss:  0.072, Seconds: 8.63\n",
            "Epoch   4/100 Batch 5850/14303 - Loss:  0.070, Seconds: 8.64\n",
            "Epoch   4/100 Batch 5880/14303 - Loss:  0.074, Seconds: 8.65\n",
            "Epoch   4/100 Batch 5910/14303 - Loss:  0.072, Seconds: 8.68\n",
            "Epoch   4/100 Batch 5940/14303 - Loss:  0.074, Seconds: 8.69\n",
            "Epoch   4/100 Batch 5970/14303 - Loss:  0.075, Seconds: 8.89\n",
            "Epoch   4/100 Batch 6000/14303 - Loss:  0.073, Seconds: 8.88\n",
            "Epoch   4/100 Batch 6030/14303 - Loss:  0.073, Seconds: 8.84\n",
            "Epoch   4/100 Batch 6060/14303 - Loss:  0.069, Seconds: 8.84\n",
            "Epoch   4/100 Batch 6090/14303 - Loss:  0.071, Seconds: 8.93\n",
            "Epoch   4/100 Batch 6120/14303 - Loss:  0.073, Seconds: 8.82\n",
            "Epoch   4/100 Batch 6150/14303 - Loss:  0.073, Seconds: 8.83\n",
            "Epoch   4/100 Batch 6180/14303 - Loss:  0.072, Seconds: 8.86\n",
            "Epoch   4/100 Batch 6210/14303 - Loss:  0.070, Seconds: 8.83\n",
            "Epoch   4/100 Batch 6240/14303 - Loss:  0.074, Seconds: 8.97\n",
            "Epoch   4/100 Batch 6270/14303 - Loss:  0.075, Seconds: 9.03\n",
            "Epoch   4/100 Batch 6300/14303 - Loss:  0.074, Seconds: 9.02\n",
            "Epoch   4/100 Batch 6330/14303 - Loss:  0.072, Seconds: 9.07\n",
            "Epoch   4/100 Batch 6360/14303 - Loss:  0.071, Seconds: 9.08\n",
            "Epoch   4/100 Batch 6390/14303 - Loss:  0.071, Seconds: 9.04\n",
            "Epoch   4/100 Batch 6420/14303 - Loss:  0.071, Seconds: 8.94\n",
            "Epoch   4/100 Batch 6450/14303 - Loss:  0.071, Seconds: 9.01\n",
            "Epoch   4/100 Batch 6480/14303 - Loss:  0.072, Seconds: 9.10\n",
            "Epoch   4/100 Batch 6510/14303 - Loss:  0.070, Seconds: 8.99\n",
            "Epoch   4/100 Batch 6540/14303 - Loss:  0.069, Seconds: 9.18\n",
            "Epoch   4/100 Batch 6570/14303 - Loss:  0.070, Seconds: 9.19\n",
            "Epoch   4/100 Batch 6600/14303 - Loss:  0.071, Seconds: 9.16\n",
            "Epoch   4/100 Batch 6630/14303 - Loss:  0.071, Seconds: 9.15\n",
            "Epoch   4/100 Batch 6660/14303 - Loss:  0.075, Seconds: 9.19\n",
            "Epoch   4/100 Batch 6690/14303 - Loss:  0.071, Seconds: 9.21\n",
            "Epoch   4/100 Batch 6720/14303 - Loss:  0.069, Seconds: 9.25\n",
            "Epoch   4/100 Batch 6750/14303 - Loss:  0.069, Seconds: 9.19\n",
            "Epoch   4/100 Batch 6780/14303 - Loss:  0.070, Seconds: 9.17\n",
            "Epoch   4/100 Batch 6810/14303 - Loss:  0.073, Seconds: 9.26\n",
            "Epoch   4/100 Batch 6840/14303 - Loss:  0.072, Seconds: 9.37\n",
            "Epoch   4/100 Batch 6870/14303 - Loss:  0.067, Seconds: 9.37\n",
            "Epoch   4/100 Batch 6900/14303 - Loss:  0.075, Seconds: 9.36\n",
            "Epoch   4/100 Batch 6930/14303 - Loss:  0.072, Seconds: 9.36\n",
            "Epoch   4/100 Batch 6960/14303 - Loss:  0.073, Seconds: 9.40\n",
            "Epoch   4/100 Batch 6990/14303 - Loss:  0.075, Seconds: 9.36\n",
            "Epoch   4/100 Batch 7020/14303 - Loss:  0.077, Seconds: 9.44\n",
            "Epoch   4/100 Batch 7050/14303 - Loss:  0.071, Seconds: 9.45\n",
            "Epoch   4/100 Batch 7080/14303 - Loss:  0.072, Seconds: 9.45\n",
            "Epoch   4/100 Batch 7110/14303 - Loss:  0.073, Seconds: 9.52\n",
            "Epoch   4/100 Batch 7140/14303 - Loss:  0.074, Seconds: 9.56\n",
            "Epoch   4/100 Batch 7170/14303 - Loss:  0.069, Seconds: 9.56\n",
            "Epoch   4/100 Batch 7200/14303 - Loss:  0.071, Seconds: 9.61\n",
            "Epoch   4/100 Batch 7230/14303 - Loss:  0.070, Seconds: 9.55\n",
            "Epoch   4/100 Batch 7260/14303 - Loss:  0.067, Seconds: 9.49\n",
            "Epoch   4/100 Batch 7290/14303 - Loss:  0.068, Seconds: 9.62\n",
            "Epoch   4/100 Batch 7320/14303 - Loss:  0.071, Seconds: 9.47\n",
            "Epoch   4/100 Batch 7350/14303 - Loss:  0.074, Seconds: 9.62\n",
            "Epoch   4/100 Batch 7380/14303 - Loss:  0.071, Seconds: 9.67\n",
            "Epoch   4/100 Batch 7410/14303 - Loss:  0.073, Seconds: 9.63\n",
            "Epoch   4/100 Batch 7440/14303 - Loss:  0.071, Seconds: 9.79\n",
            "Epoch   4/100 Batch 7470/14303 - Loss:  0.072, Seconds: 9.70\n",
            "Epoch   4/100 Batch 7500/14303 - Loss:  0.071, Seconds: 9.69\n",
            "Epoch   4/100 Batch 7530/14303 - Loss:  0.071, Seconds: 9.67\n",
            "Epoch   4/100 Batch 7560/14303 - Loss:  0.069, Seconds: 9.67\n",
            "Epoch   4/100 Batch 7590/14303 - Loss:  0.072, Seconds: 9.64\n",
            "Epoch   4/100 Batch 7620/14303 - Loss:  0.075, Seconds: 9.86\n",
            "Epoch   4/100 Batch 7650/14303 - Loss:  0.075, Seconds: 9.90\n",
            "Epoch   4/100 Batch 7680/14303 - Loss:  0.072, Seconds: 9.94\n",
            "Epoch   4/100 Batch 7710/14303 - Loss:  0.068, Seconds: 9.89\n",
            "Epoch   4/100 Batch 7740/14303 - Loss:  0.075, Seconds: 9.81\n",
            "Epoch   4/100 Batch 7770/14303 - Loss:  0.071, Seconds: 9.87\n",
            "Epoch   4/100 Batch 7800/14303 - Loss:  0.072, Seconds: 9.87\n",
            "Epoch   4/100 Batch 7830/14303 - Loss:  0.071, Seconds: 9.83\n",
            "Epoch   4/100 Batch 7860/14303 - Loss:  0.070, Seconds: 9.96\n",
            "Epoch   4/100 Batch 7890/14303 - Loss:  0.070, Seconds: 10.13\n",
            "Epoch   4/100 Batch 7920/14303 - Loss:  0.071, Seconds: 10.09\n",
            "Epoch   4/100 Batch 7950/14303 - Loss:  0.072, Seconds: 10.11\n",
            "Epoch   4/100 Batch 7980/14303 - Loss:  0.072, Seconds: 10.04\n",
            "Epoch   4/100 Batch 8010/14303 - Loss:  0.069, Seconds: 10.06\n",
            "Epoch   4/100 Batch 8040/14303 - Loss:  0.068, Seconds: 10.06\n",
            "Epoch   4/100 Batch 8070/14303 - Loss:  0.068, Seconds: 10.09\n",
            "Epoch   4/100 Batch 8100/14303 - Loss:  0.073, Seconds: 10.07\n",
            "Epoch   4/100 Batch 8130/14303 - Loss:  0.072, Seconds: 10.32\n",
            "Epoch   4/100 Batch 8160/14303 - Loss:  0.069, Seconds: 10.29\n",
            "Epoch   4/100 Batch 8190/14303 - Loss:  0.070, Seconds: 10.22\n",
            "Epoch   4/100 Batch 8220/14303 - Loss:  0.070, Seconds: 10.27\n",
            "Epoch   4/100 Batch 8250/14303 - Loss:  0.069, Seconds: 10.19\n",
            "Epoch   4/100 Batch 8280/14303 - Loss:  0.071, Seconds: 10.21\n",
            "Epoch   4/100 Batch 8310/14303 - Loss:  0.070, Seconds: 10.24\n",
            "Epoch   4/100 Batch 8340/14303 - Loss:  0.070, Seconds: 10.25\n",
            "Epoch   4/100 Batch 8370/14303 - Loss:  0.072, Seconds: 10.42\n",
            "Epoch   4/100 Batch 8400/14303 - Loss:  0.074, Seconds: 10.42\n",
            "Epoch   4/100 Batch 8430/14303 - Loss:  0.071, Seconds: 10.36\n",
            "Epoch   4/100 Batch 8460/14303 - Loss:  0.071, Seconds: 10.42\n",
            "Epoch   4/100 Batch 8490/14303 - Loss:  0.073, Seconds: 10.39\n",
            "Epoch   4/100 Batch 8520/14303 - Loss:  0.069, Seconds: 10.37\n",
            "Epoch   4/100 Batch 8550/14303 - Loss:  0.068, Seconds: 10.40\n",
            "Epoch   4/100 Batch 8580/14303 - Loss:  0.070, Seconds: 10.46\n",
            "Epoch   4/100 Batch 8610/14303 - Loss:  0.068, Seconds: 10.64\n",
            "Epoch   4/100 Batch 8640/14303 - Loss:  0.073, Seconds: 10.62\n",
            "Epoch   4/100 Batch 8670/14303 - Loss:  0.068, Seconds: 10.58\n",
            "Epoch   4/100 Batch 8700/14303 - Loss:  0.069, Seconds: 10.56\n",
            "Epoch   4/100 Batch 8730/14303 - Loss:  0.069, Seconds: 10.57\n",
            "Epoch   4/100 Batch 8760/14303 - Loss:  0.070, Seconds: 10.59\n",
            "Epoch   4/100 Batch 8790/14303 - Loss:  0.068, Seconds: 10.80\n",
            "Epoch   4/100 Batch 8820/14303 - Loss:  0.070, Seconds: 10.64\n",
            "Epoch   4/100 Batch 8850/14303 - Loss:  0.069, Seconds: 10.77\n",
            "Epoch   4/100 Batch 8880/14303 - Loss:  0.070, Seconds: 10.79\n",
            "Epoch   4/100 Batch 8910/14303 - Loss:  0.070, Seconds: 10.81\n",
            "Epoch   4/100 Batch 8940/14303 - Loss:  0.070, Seconds: 10.73\n",
            "Epoch   4/100 Batch 8970/14303 - Loss:  0.068, Seconds: 10.80\n",
            "Epoch   4/100 Batch 9000/14303 - Loss:  0.069, Seconds: 10.83\n",
            "Epoch   4/100 Batch 9030/14303 - Loss:  0.070, Seconds: 10.81\n",
            "Epoch   4/100 Batch 9060/14303 - Loss:  0.067, Seconds: 10.94\n",
            "Epoch   4/100 Batch 9090/14303 - Loss:  0.068, Seconds: 10.86\n",
            "Epoch   4/100 Batch 9120/14303 - Loss:  0.070, Seconds: 10.93\n",
            "Epoch   4/100 Batch 9150/14303 - Loss:  0.066, Seconds: 10.92\n",
            "Epoch   4/100 Batch 9180/14303 - Loss:  0.070, Seconds: 10.98\n",
            "Epoch   4/100 Batch 9210/14303 - Loss:  0.066, Seconds: 10.97\n",
            "Epoch   4/100 Batch 9240/14303 - Loss:  0.068, Seconds: 10.90\n",
            "Epoch   4/100 Batch 9270/14303 - Loss:  0.069, Seconds: 11.03\n",
            "Epoch   4/100 Batch 9300/14303 - Loss:  0.071, Seconds: 11.12\n",
            "Epoch   4/100 Batch 9330/14303 - Loss:  0.070, Seconds: 11.15\n",
            "Epoch   4/100 Batch 9360/14303 - Loss:  0.072, Seconds: 11.11\n",
            "Epoch   4/100 Batch 9390/14303 - Loss:  0.075, Seconds: 11.18\n",
            "Epoch   4/100 Batch 9420/14303 - Loss:  0.070, Seconds: 11.17\n",
            "Epoch   4/100 Batch 9450/14303 - Loss:  0.069, Seconds: 11.14\n",
            "Epoch   4/100 Batch 9480/14303 - Loss:  0.067, Seconds: 11.29\n",
            "Epoch   4/100 Batch 9510/14303 - Loss:  0.071, Seconds: 11.32\n",
            "Epoch   4/100 Batch 9540/14303 - Loss:  0.069, Seconds: 11.32\n",
            "Epoch   4/100 Batch 9570/14303 - Loss:  0.070, Seconds: 11.46\n",
            "Epoch   4/100 Batch 9600/14303 - Loss:  0.070, Seconds: 11.45\n",
            "Epoch   4/100 Batch 9630/14303 - Loss:  0.071, Seconds: 11.29\n",
            "Epoch   4/100 Batch 9660/14303 - Loss:  0.069, Seconds: 11.31\n",
            "Epoch   4/100 Batch 9690/14303 - Loss:  0.070, Seconds: 11.45\n",
            "Epoch   4/100 Batch 9720/14303 - Loss:  0.070, Seconds: 11.46\n",
            "Epoch   4/100 Batch 9750/14303 - Loss:  0.067, Seconds: 11.54\n",
            "Epoch   4/100 Batch 9780/14303 - Loss:  0.069, Seconds: 11.52\n",
            "Epoch   4/100 Batch 9810/14303 - Loss:  0.068, Seconds: 11.55\n",
            "Epoch   4/100 Batch 9840/14303 - Loss:  0.069, Seconds: 11.47\n",
            "Epoch   4/100 Batch 9870/14303 - Loss:  0.072, Seconds: 11.63\n",
            "Epoch   4/100 Batch 9900/14303 - Loss:  0.070, Seconds: 11.62\n",
            "Epoch   4/100 Batch 9930/14303 - Loss:  0.073, Seconds: 11.62\n",
            "Epoch   4/100 Batch 9960/14303 - Loss:  0.070, Seconds: 11.63\n",
            "Epoch   4/100 Batch 9990/14303 - Loss:  0.071, Seconds: 11.67\n",
            "Epoch   4/100 Batch 10020/14303 - Loss:  0.070, Seconds: 11.66\n",
            "Epoch   4/100 Batch 10050/14303 - Loss:  0.071, Seconds: 11.68\n",
            "Epoch   4/100 Batch 10080/14303 - Loss:  0.068, Seconds: 11.84\n",
            "Epoch   4/100 Batch 10110/14303 - Loss:  0.070, Seconds: 11.85\n",
            "Epoch   4/100 Batch 10140/14303 - Loss:  0.070, Seconds: 11.85\n",
            "Epoch   4/100 Batch 10170/14303 - Loss:  0.068, Seconds: 11.77\n",
            "Epoch   4/100 Batch 10200/14303 - Loss:  0.066, Seconds: 11.88\n",
            "Epoch   4/100 Batch 10230/14303 - Loss:  0.067, Seconds: 11.82\n",
            "Epoch   4/100 Batch 10260/14303 - Loss:  0.071, Seconds: 12.01\n",
            "Epoch   4/100 Batch 10290/14303 - Loss:  0.068, Seconds: 11.99\n",
            "Epoch   4/100 Batch 10320/14303 - Loss:  0.069, Seconds: 12.06\n",
            "Epoch   4/100 Batch 10350/14303 - Loss:  0.070, Seconds: 12.07\n",
            "Epoch   4/100 Batch 10380/14303 - Loss:  0.069, Seconds: 12.09\n",
            "Epoch   4/100 Batch 10410/14303 - Loss:  0.068, Seconds: 12.03\n",
            "Epoch   4/100 Batch 10440/14303 - Loss:  0.073, Seconds: 12.22\n",
            "Epoch   4/100 Batch 10470/14303 - Loss:  0.071, Seconds: 12.20\n",
            "Epoch   4/100 Batch 10500/14303 - Loss:  0.068, Seconds: 12.23\n",
            "Epoch   4/100 Batch 10530/14303 - Loss:  0.068, Seconds: 12.16\n",
            "Epoch   4/100 Batch 10560/14303 - Loss:  0.070, Seconds: 12.23\n",
            "Epoch   4/100 Batch 10590/14303 - Loss:  0.069, Seconds: 12.30\n",
            "Epoch   4/100 Batch 10620/14303 - Loss:  0.067, Seconds: 12.35\n",
            "Epoch   4/100 Batch 10650/14303 - Loss:  0.069, Seconds: 12.30\n",
            "Epoch   4/100 Batch 10680/14303 - Loss:  0.067, Seconds: 12.36\n",
            "Epoch   4/100 Batch 10710/14303 - Loss:  0.071, Seconds: 12.38\n",
            "Epoch   4/100 Batch 10740/14303 - Loss:  0.070, Seconds: 12.47\n",
            "Epoch   4/100 Batch 10770/14303 - Loss:  0.077, Seconds: 12.58\n",
            "Epoch   4/100 Batch 10800/14303 - Loss:  0.073, Seconds: 12.56\n",
            "Epoch   4/100 Batch 10830/14303 - Loss:  0.071, Seconds: 12.57\n",
            "Epoch   4/100 Batch 10860/14303 - Loss:  0.070, Seconds: 12.55\n",
            "Epoch   4/100 Batch 10890/14303 - Loss:  0.070, Seconds: 12.56\n",
            "Epoch   4/100 Batch 10920/14303 - Loss:  0.070, Seconds: 12.77\n",
            "Epoch   4/100 Batch 10950/14303 - Loss:  0.070, Seconds: 12.82\n",
            "Epoch   4/100 Batch 10980/14303 - Loss:  0.068, Seconds: 12.73\n",
            "Epoch   4/100 Batch 11010/14303 - Loss:  0.068, Seconds: 12.72\n",
            "Epoch   4/100 Batch 11040/14303 - Loss:  0.068, Seconds: 12.97\n",
            "Epoch   4/100 Batch 11070/14303 - Loss:  0.069, Seconds: 12.86\n",
            "Epoch   4/100 Batch 11100/14303 - Loss:  0.073, Seconds: 12.92\n",
            "Epoch   4/100 Batch 11130/14303 - Loss:  0.070, Seconds: 12.86\n",
            "Epoch   4/100 Batch 11160/14303 - Loss:  0.069, Seconds: 12.97\n",
            "Epoch   4/100 Batch 11190/14303 - Loss:  0.070, Seconds: 12.89\n",
            "Epoch   4/100 Batch 11220/14303 - Loss:  0.069, Seconds: 13.03\n",
            "Epoch   4/100 Batch 11250/14303 - Loss:  0.068, Seconds: 13.06\n",
            "Epoch   4/100 Batch 11280/14303 - Loss:  0.065, Seconds: 13.13\n",
            "Epoch   4/100 Batch 11310/14303 - Loss:  0.068, Seconds: 13.10\n",
            "Epoch   4/100 Batch 11340/14303 - Loss:  0.066, Seconds: 13.16\n",
            "Epoch   4/100 Batch 11370/14303 - Loss:  0.068, Seconds: 13.22\n",
            "Epoch   4/100 Batch 11400/14303 - Loss:  0.070, Seconds: 13.26\n",
            "Epoch   4/100 Batch 11430/14303 - Loss:  0.164, Seconds: 13.26\n",
            "Epoch   4/100 Batch 11460/14303 - Loss:  0.220, Seconds: 13.33\n",
            "Epoch   4/100 Batch 11490/14303 - Loss:  0.119, Seconds: 13.38\n",
            "Epoch   4/100 Batch 11520/14303 - Loss:  0.107, Seconds: 13.49\n",
            "Epoch   4/100 Batch 11550/14303 - Loss:  0.096, Seconds: 13.45\n",
            "Epoch   4/100 Batch 11580/14303 - Loss:  0.091, Seconds: 13.42\n",
            "Epoch   4/100 Batch 11610/14303 - Loss:  0.091, Seconds: 13.44\n",
            "Epoch   4/100 Batch 11640/14303 - Loss:  0.088, Seconds: 13.67\n",
            "Epoch   4/100 Batch 11670/14303 - Loss:  0.083, Seconds: 13.54\n",
            "Epoch   4/100 Batch 11700/14303 - Loss:  0.084, Seconds: 13.75\n",
            "Epoch   4/100 Batch 11730/14303 - Loss:  0.085, Seconds: 13.59\n",
            "Epoch   4/100 Batch 11760/14303 - Loss:  0.081, Seconds: 13.78\n",
            "Epoch   4/100 Batch 11790/14303 - Loss:  0.084, Seconds: 13.87\n",
            "Epoch   4/100 Batch 11820/14303 - Loss:  0.078, Seconds: 13.89\n",
            "Epoch   4/100 Batch 11850/14303 - Loss:  0.081, Seconds: 13.71\n",
            "Epoch   4/100 Batch 11880/14303 - Loss:  0.080, Seconds: 13.96\n",
            "Epoch   4/100 Batch 11910/14303 - Loss:  0.076, Seconds: 13.88\n",
            "Epoch   4/100 Batch 11940/14303 - Loss:  0.078, Seconds: 13.97\n",
            "Epoch   4/100 Batch 11970/14303 - Loss:  0.076, Seconds: 14.04\n",
            "Epoch   4/100 Batch 12000/14303 - Loss:  0.074, Seconds: 14.19\n",
            "Epoch   4/100 Batch 12030/14303 - Loss:  0.077, Seconds: 14.13\n",
            "Epoch   4/100 Batch 12060/14303 - Loss:  0.076, Seconds: 14.20\n",
            "Epoch   4/100 Batch 12090/14303 - Loss:  0.077, Seconds: 14.17\n",
            "Epoch   4/100 Batch 12120/14303 - Loss:  0.075, Seconds: 14.34\n",
            "Epoch   4/100 Batch 12150/14303 - Loss:  0.077, Seconds: 14.36\n",
            "Epoch   4/100 Batch 12180/14303 - Loss:  0.073, Seconds: 14.29\n",
            "Epoch   4/100 Batch 12210/14303 - Loss:  0.074, Seconds: 14.42\n",
            "Epoch   4/100 Batch 12240/14303 - Loss:  0.074, Seconds: 14.48\n",
            "Epoch   4/100 Batch 12270/14303 - Loss:  0.075, Seconds: 14.45\n",
            "Epoch   4/100 Batch 12300/14303 - Loss:  0.075, Seconds: 14.56\n",
            "Epoch   4/100 Batch 12330/14303 - Loss:  0.074, Seconds: 14.80\n",
            "Epoch   4/100 Batch 12360/14303 - Loss:  0.074, Seconds: 14.67\n",
            "Epoch   4/100 Batch 12390/14303 - Loss:  0.073, Seconds: 14.68\n",
            "Epoch   4/100 Batch 12420/14303 - Loss:  0.071, Seconds: 15.03\n",
            "Epoch   4/100 Batch 12450/14303 - Loss:  0.068, Seconds: 14.81\n",
            "Epoch   4/100 Batch 12480/14303 - Loss:  0.072, Seconds: 14.91\n",
            "Epoch   4/100 Batch 12510/14303 - Loss:  0.072, Seconds: 15.05\n",
            "Epoch   4/100 Batch 12540/14303 - Loss:  0.073, Seconds: 15.05\n",
            "Epoch   4/100 Batch 12570/14303 - Loss:  0.070, Seconds: 15.18\n",
            "Epoch   4/100 Batch 12600/14303 - Loss:  0.071, Seconds: 15.27\n",
            "Epoch   4/100 Batch 12630/14303 - Loss:  0.072, Seconds: 15.18\n",
            "Epoch   4/100 Batch 12660/14303 - Loss:  0.072, Seconds: 15.33\n",
            "Epoch   4/100 Batch 12690/14303 - Loss:  0.070, Seconds: 15.44\n",
            "Epoch   4/100 Batch 12720/14303 - Loss:  0.073, Seconds: 15.48\n",
            "Epoch   4/100 Batch 12750/14303 - Loss:  0.069, Seconds: 15.59\n",
            "Epoch   4/100 Batch 12780/14303 - Loss:  0.071, Seconds: 15.63\n",
            "Epoch   4/100 Batch 12810/14303 - Loss:  0.070, Seconds: 15.71\n",
            "Epoch   4/100 Batch 12840/14303 - Loss:  0.071, Seconds: 15.81\n",
            "Epoch   4/100 Batch 12870/14303 - Loss:  0.068, Seconds: 15.90\n",
            "Epoch   4/100 Batch 12900/14303 - Loss:  0.071, Seconds: 16.11\n",
            "Epoch   4/100 Batch 12930/14303 - Loss:  0.069, Seconds: 15.97\n",
            "Epoch   4/100 Batch 12960/14303 - Loss:  0.070, Seconds: 16.16\n",
            "Epoch   4/100 Batch 12990/14303 - Loss:  0.070, Seconds: 16.27\n",
            "Epoch   4/100 Batch 13020/14303 - Loss:  0.070, Seconds: 16.39\n",
            "Epoch   4/100 Batch 13050/14303 - Loss:  0.067, Seconds: 16.44\n",
            "Epoch   4/100 Batch 13080/14303 - Loss:  0.068, Seconds: 16.39\n",
            "Epoch   4/100 Batch 13110/14303 - Loss:  0.069, Seconds: 16.59\n",
            "Epoch   4/100 Batch 13140/14303 - Loss:  0.070, Seconds: 16.67\n",
            "Epoch   4/100 Batch 13170/14303 - Loss:  0.070, Seconds: 16.85\n",
            "Epoch   4/100 Batch 13200/14303 - Loss:  0.066, Seconds: 16.77\n",
            "Epoch   4/100 Batch 13230/14303 - Loss:  0.070, Seconds: 16.99\n",
            "Epoch   4/100 Batch 13260/14303 - Loss:  0.070, Seconds: 17.12\n",
            "Epoch   4/100 Batch 13290/14303 - Loss:  0.068, Seconds: 17.29\n",
            "Epoch   4/100 Batch 13320/14303 - Loss:  0.069, Seconds: 17.27\n",
            "Epoch   4/100 Batch 13350/14303 - Loss:  0.068, Seconds: 17.39\n",
            "Epoch   4/100 Batch 13380/14303 - Loss:  0.071, Seconds: 17.49\n",
            "Epoch   4/100 Batch 13410/14303 - Loss:  0.067, Seconds: 17.77\n",
            "Epoch   4/100 Batch 13440/14303 - Loss:  0.066, Seconds: 17.83\n",
            "Epoch   4/100 Batch 13470/14303 - Loss:  0.067, Seconds: 18.05\n",
            "Epoch   4/100 Batch 13500/14303 - Loss:  0.068, Seconds: 18.05\n",
            "Epoch   4/100 Batch 13530/14303 - Loss:  0.066, Seconds: 18.23\n",
            "Epoch   4/100 Batch 13560/14303 - Loss:  0.068, Seconds: 18.36\n",
            "Epoch   4/100 Batch 13590/14303 - Loss:  0.068, Seconds: 18.52\n",
            "Epoch   4/100 Batch 13620/14303 - Loss:  0.066, Seconds: 18.70\n",
            "Epoch   4/100 Batch 13650/14303 - Loss:  0.067, Seconds: 18.87\n",
            "Epoch   4/100 Batch 13680/14303 - Loss:  0.069, Seconds: 19.10\n",
            "Epoch   4/100 Batch 13710/14303 - Loss:  0.068, Seconds: 19.23\n",
            "Epoch   4/100 Batch 13740/14303 - Loss:  0.068, Seconds: 19.44\n",
            "Epoch   4/100 Batch 13770/14303 - Loss:  0.069, Seconds: 19.64\n",
            "Epoch   4/100 Batch 13800/14303 - Loss:  0.070, Seconds: 19.94\n",
            "Epoch   4/100 Batch 13830/14303 - Loss:  0.099, Seconds: 20.06\n",
            "Epoch   4/100 Batch 13860/14303 - Loss:  0.094, Seconds: 20.31\n",
            "Epoch   4/100 Batch 13890/14303 - Loss:  0.077, Seconds: 20.77\n",
            "Epoch   4/100 Batch 13920/14303 - Loss:  0.075, Seconds: 20.97\n",
            "Epoch   4/100 Batch 13950/14303 - Loss:  0.072, Seconds: 21.21\n",
            "Epoch   4/100 Batch 13980/14303 - Loss:  0.074, Seconds: 21.50\n",
            "Epoch   4/100 Batch 14010/14303 - Loss:  0.070, Seconds: 21.89\n",
            "Epoch   4/100 Batch 14040/14303 - Loss:  0.071, Seconds: 22.32\n",
            "Epoch   4/100 Batch 14070/14303 - Loss:  0.072, Seconds: 22.70\n",
            "Epoch   4/100 Batch 14100/14303 - Loss:  0.072, Seconds: 23.27\n",
            "Epoch   4/100 Batch 14130/14303 - Loss:  0.070, Seconds: 23.76\n",
            "Epoch   4/100 Batch 14160/14303 - Loss:  0.070, Seconds: 24.42\n",
            "Epoch   4/100 Batch 14190/14303 - Loss:  0.069, Seconds: 25.23\n",
            "Epoch   4/100 Batch 14220/14303 - Loss:  0.071, Seconds: 26.20\n",
            "Epoch   4/100 Batch 14250/14303 - Loss:  0.070, Seconds: 28.02\n",
            "Epoch   4/100 Batch 14280/14303 - Loss:  0.072, Seconds: 30.32\n",
            "  Validation Input: nto as bad as on esandal thik\n",
            "  Validation Output: not as bad as one sandal thinks\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: t is also called grant sahib\n",
            "  Validation Output: it is also called grant shaib\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is itnendced for kadlut onyl\n",
            "  Validation Output: it is intended for kadult only\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reason things out for oyruself\n",
            "  Validation Output: reason things out for yourself\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: it khappened to work quite wiell\n",
            "  Validation Output: it happened to work quite well\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: chef and i are doing very well\n",
            "  Validation Output: chef and i are doing very well\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: centerfior the performing arts\n",
            "  Validation Output: center for the performing arts\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: sopon into a hotb bowl toerve\n",
            "  Validation Output: soppon into a hot bowl to serve\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: bpeople meet her nd they swoon\n",
            "  Validation Output: people meet her and they swoon\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: my poor little heand is obiling\n",
            "  Validation Output: my poor little head and is boiling\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this had become a cmmon scene\n",
            "  Validation Output: this had become a common scene\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: evien wen through th especias\n",
            "  Validation Output: evine when went through the specia\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: prehqet a steamer ogn hgi heat\n",
            "  Validation Output: preheqt a steamer on high heat\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lol well someoen had tos y it\n",
            "  Validation Output: lol well someone had to say it\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: he was weaving mt sont he lst\n",
            "  Validation Output: he was weaving met son the last\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: tahis ifs wrong for many resns\n",
            "  Validation Output: this is wrong for many resong for\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: oday we heaed gain t cubao\n",
            "  Validation Output: today we heared again to cubat\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep and membeer status requiredq\n",
            "  Validation Output: rep and member and member status requ\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it sqowed midldingyhard to day\n",
            "  Validation Output: it squowed middling your day\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: they boht werl eather sandals\n",
            "  Validation Output: they both were leather sandals\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  0.107, Seconds: 0.00\n",
            "New Accuracy Record!\n",
            "\n",
            "Training Model: 5\n",
            "Epoch   5/100 Batch   30/14303 - Loss:  0.097, Seconds: 6.05\n",
            "Epoch   5/100 Batch   60/14303 - Loss:  0.075, Seconds: 6.09\n",
            "Epoch   5/100 Batch   90/14303 - Loss:  0.072, Seconds: 5.79\n",
            "Epoch   5/100 Batch  120/14303 - Loss:  0.079, Seconds: 5.81\n",
            "Epoch   5/100 Batch  150/14303 - Loss:  0.075, Seconds: 5.74\n",
            "Epoch   5/100 Batch  180/14303 - Loss:  0.069, Seconds: 5.80\n",
            "Epoch   5/100 Batch  210/14303 - Loss:  0.076, Seconds: 5.80\n",
            "Epoch   5/100 Batch  240/14303 - Loss:  0.074, Seconds: 5.76\n",
            "Epoch   5/100 Batch  270/14303 - Loss:  0.071, Seconds: 5.79\n",
            "Epoch   5/100 Batch  300/14303 - Loss:  0.095, Seconds: 5.80\n",
            "Epoch   5/100 Batch  330/14303 - Loss:  0.071, Seconds: 5.96\n",
            "Epoch   5/100 Batch  360/14303 - Loss:  0.072, Seconds: 5.98\n",
            "Epoch   5/100 Batch  390/14303 - Loss:  0.076, Seconds: 6.00\n",
            "Epoch   5/100 Batch  420/14303 - Loss:  0.072, Seconds: 5.92\n",
            "Epoch   5/100 Batch  450/14303 - Loss:  0.072, Seconds: 5.92\n",
            "Epoch   5/100 Batch  480/14303 - Loss:  0.070, Seconds: 5.96\n",
            "Epoch   5/100 Batch  510/14303 - Loss:  0.074, Seconds: 5.96\n",
            "Epoch   5/100 Batch  540/14303 - Loss:  0.074, Seconds: 5.93\n",
            "Epoch   5/100 Batch  570/14303 - Loss:  0.073, Seconds: 5.92\n",
            "Epoch   5/100 Batch  600/14303 - Loss:  0.079, Seconds: 5.94\n",
            "Epoch   5/100 Batch  630/14303 - Loss:  0.069, Seconds: 6.11\n",
            "Epoch   5/100 Batch  660/14303 - Loss:  0.071, Seconds: 6.10\n",
            "Epoch   5/100 Batch  690/14303 - Loss:  0.070, Seconds: 6.10\n",
            "Epoch   5/100 Batch  720/14303 - Loss:  0.070, Seconds: 6.09\n",
            "Epoch   5/100 Batch  750/14303 - Loss:  0.070, Seconds: 6.09\n",
            "Epoch   5/100 Batch  780/14303 - Loss:  0.072, Seconds: 6.08\n",
            "Epoch   5/100 Batch  810/14303 - Loss:  0.072, Seconds: 6.12\n",
            "Epoch   5/100 Batch  840/14303 - Loss:  0.070, Seconds: 6.08\n",
            "Epoch   5/100 Batch  870/14303 - Loss:  0.072, Seconds: 6.14\n",
            "Epoch   5/100 Batch  900/14303 - Loss:  0.068, Seconds: 6.04\n",
            "Epoch   5/100 Batch  930/14303 - Loss:  0.078, Seconds: 6.21\n",
            "Epoch   5/100 Batch  960/14303 - Loss:  0.070, Seconds: 6.23\n",
            "Epoch   5/100 Batch  990/14303 - Loss:  0.073, Seconds: 6.28\n",
            "Epoch   5/100 Batch 1020/14303 - Loss:  0.072, Seconds: 6.23\n",
            "Epoch   5/100 Batch 1050/14303 - Loss:  0.073, Seconds: 6.26\n",
            "Epoch   5/100 Batch 1080/14303 - Loss:  0.069, Seconds: 6.29\n",
            "Epoch   5/100 Batch 1110/14303 - Loss:  0.067, Seconds: 6.24\n",
            "Epoch   5/100 Batch 1140/14303 - Loss:  0.070, Seconds: 6.23\n",
            "Epoch   5/100 Batch 1170/14303 - Loss:  0.068, Seconds: 6.28\n",
            "Epoch   5/100 Batch 1200/14303 - Loss:  0.069, Seconds: 6.31\n",
            "Epoch   5/100 Batch 1230/14303 - Loss:  0.070, Seconds: 6.32\n",
            "Epoch   5/100 Batch 1260/14303 - Loss:  0.073, Seconds: 6.46\n",
            "Epoch   5/100 Batch 1290/14303 - Loss:  0.069, Seconds: 6.44\n",
            "Epoch   5/100 Batch 1320/14303 - Loss:  0.071, Seconds: 6.40\n",
            "Epoch   5/100 Batch 1350/14303 - Loss:  0.070, Seconds: 6.53\n",
            "Epoch   5/100 Batch 1380/14303 - Loss:  0.066, Seconds: 6.54\n",
            "Epoch   5/100 Batch 1410/14303 - Loss:  0.073, Seconds: 6.40\n",
            "Epoch   5/100 Batch 1440/14303 - Loss:  0.070, Seconds: 6.49\n",
            "Epoch   5/100 Batch 1470/14303 - Loss:  0.074, Seconds: 6.48\n",
            "Epoch   5/100 Batch 1500/14303 - Loss:  0.068, Seconds: 6.42\n",
            "Epoch   5/100 Batch 1530/14303 - Loss:  0.072, Seconds: 6.42\n",
            "Epoch   5/100 Batch 1560/14303 - Loss:  0.080, Seconds: 6.49\n",
            "Epoch   5/100 Batch 1590/14303 - Loss:  0.063, Seconds: 6.59\n",
            "Epoch   5/100 Batch 1620/14303 - Loss:  0.067, Seconds: 6.63\n",
            "Epoch   5/100 Batch 1650/14303 - Loss:  0.066, Seconds: 6.59\n",
            "Epoch   5/100 Batch 1680/14303 - Loss:  0.068, Seconds: 6.59\n",
            "Epoch   5/100 Batch 1710/14303 - Loss:  0.070, Seconds: 6.61\n",
            "Epoch   5/100 Batch 1740/14303 - Loss:  0.066, Seconds: 6.61\n",
            "Epoch   5/100 Batch 1770/14303 - Loss:  0.070, Seconds: 6.59\n",
            "Epoch   5/100 Batch 1800/14303 - Loss:  0.068, Seconds: 6.56\n",
            "Epoch   5/100 Batch 1830/14303 - Loss:  0.067, Seconds: 6.61\n",
            "Epoch   5/100 Batch 1860/14303 - Loss:  0.069, Seconds: 6.62\n",
            "Epoch   5/100 Batch 1890/14303 - Loss:  0.075, Seconds: 6.78\n",
            "Epoch   5/100 Batch 1920/14303 - Loss:  0.069, Seconds: 6.82\n",
            "Epoch   5/100 Batch 1950/14303 - Loss:  0.068, Seconds: 6.80\n",
            "Epoch   5/100 Batch 1980/14303 - Loss:  0.069, Seconds: 6.70\n",
            "Epoch   5/100 Batch 2010/14303 - Loss:  0.070, Seconds: 6.77\n",
            "Epoch   5/100 Batch 2040/14303 - Loss:  0.067, Seconds: 6.77\n",
            "Epoch   5/100 Batch 2070/14303 - Loss:  0.068, Seconds: 6.77\n",
            "Epoch   5/100 Batch 2100/14303 - Loss:  0.065, Seconds: 6.74\n",
            "Epoch   5/100 Batch 2130/14303 - Loss:  0.066, Seconds: 6.82\n",
            "Epoch   5/100 Batch 2160/14303 - Loss:  0.069, Seconds: 6.71\n",
            "Epoch   5/100 Batch 2190/14303 - Loss:  0.071, Seconds: 6.81\n",
            "Epoch   5/100 Batch 2220/14303 - Loss:  0.069, Seconds: 6.91\n",
            "Epoch   5/100 Batch 2250/14303 - Loss:  0.071, Seconds: 6.93\n",
            "Epoch   5/100 Batch 2280/14303 - Loss:  0.068, Seconds: 6.92\n",
            "Epoch   5/100 Batch 2310/14303 - Loss:  0.070, Seconds: 6.96\n",
            "Epoch   5/100 Batch 2340/14303 - Loss:  0.071, Seconds: 7.01\n",
            "Epoch   5/100 Batch 2370/14303 - Loss:  0.068, Seconds: 6.95\n",
            "Epoch   5/100 Batch 2400/14303 - Loss:  0.066, Seconds: 6.96\n",
            "Epoch   5/100 Batch 2430/14303 - Loss:  0.066, Seconds: 6.96\n",
            "Epoch   5/100 Batch 2460/14303 - Loss:  0.072, Seconds: 6.95\n",
            "Epoch   5/100 Batch 2490/14303 - Loss:  0.069, Seconds: 6.90\n",
            "Epoch   5/100 Batch 2520/14303 - Loss:  0.069, Seconds: 7.03\n",
            "Epoch   5/100 Batch 2550/14303 - Loss:  0.072, Seconds: 7.14\n",
            "Epoch   5/100 Batch 2580/14303 - Loss:  0.068, Seconds: 7.17\n",
            "Epoch   5/100 Batch 2610/14303 - Loss:  0.068, Seconds: 7.16\n",
            "Epoch   5/100 Batch 2640/14303 - Loss:  0.067, Seconds: 7.25\n",
            "Epoch   5/100 Batch 2670/14303 - Loss:  0.066, Seconds: 7.20\n",
            "Epoch   5/100 Batch 2700/14303 - Loss:  0.069, Seconds: 7.22\n",
            "Epoch   5/100 Batch 2730/14303 - Loss:  0.070, Seconds: 7.14\n",
            "Epoch   5/100 Batch 2760/14303 - Loss:  0.067, Seconds: 7.15\n",
            "Epoch   5/100 Batch 2790/14303 - Loss:  0.067, Seconds: 7.09\n",
            "Epoch   5/100 Batch 2820/14303 - Loss:  0.068, Seconds: 7.12\n",
            "Epoch   5/100 Batch 2850/14303 - Loss:  0.074, Seconds: 7.29\n",
            "Epoch   5/100 Batch 2880/14303 - Loss:  0.068, Seconds: 7.34\n",
            "Epoch   5/100 Batch 2910/14303 - Loss:  0.068, Seconds: 7.31\n",
            "Epoch   5/100 Batch 2940/14303 - Loss:  0.062, Seconds: 7.31\n",
            "Epoch   5/100 Batch 2970/14303 - Loss:  0.069, Seconds: 7.31\n",
            "Epoch   5/100 Batch 3000/14303 - Loss:  0.065, Seconds: 7.26\n",
            "Epoch   5/100 Batch 3030/14303 - Loss:  0.068, Seconds: 7.22\n",
            "Epoch   5/100 Batch 3060/14303 - Loss:  0.065, Seconds: 7.28\n",
            "Epoch   5/100 Batch 3090/14303 - Loss:  0.068, Seconds: 7.27\n",
            "Epoch   5/100 Batch 3120/14303 - Loss:  0.068, Seconds: 7.24\n",
            "Epoch   5/100 Batch 3150/14303 - Loss:  0.070, Seconds: 7.30\n",
            "Epoch   5/100 Batch 3180/14303 - Loss:  0.074, Seconds: 7.45\n",
            "Epoch   5/100 Batch 3210/14303 - Loss:  0.069, Seconds: 7.52\n",
            "Epoch   5/100 Batch 3240/14303 - Loss:  0.068, Seconds: 7.49\n",
            "Epoch   5/100 Batch 3270/14303 - Loss:  0.062, Seconds: 7.47\n",
            "Epoch   5/100 Batch 3300/14303 - Loss:  0.068, Seconds: 7.47\n",
            "Epoch   5/100 Batch 3330/14303 - Loss:  0.067, Seconds: 7.46\n",
            "Epoch   5/100 Batch 3360/14303 - Loss:  0.070, Seconds: 7.45\n",
            "Epoch   5/100 Batch 3390/14303 - Loss:  0.069, Seconds: 7.43\n",
            "Epoch   5/100 Batch 3420/14303 - Loss:  0.065, Seconds: 7.41\n",
            "Epoch   5/100 Batch 3450/14303 - Loss:  0.067, Seconds: 7.41\n",
            "Epoch   5/100 Batch 3480/14303 - Loss:  0.070, Seconds: 7.50\n",
            "Epoch   5/100 Batch 3510/14303 - Loss:  0.067, Seconds: 7.71\n",
            "Epoch   5/100 Batch 3540/14303 - Loss:  0.068, Seconds: 7.62\n",
            "Epoch   5/100 Batch 3570/14303 - Loss:  0.067, Seconds: 7.60\n",
            "Epoch   5/100 Batch 3600/14303 - Loss:  0.065, Seconds: 7.60\n",
            "Epoch   5/100 Batch 3630/14303 - Loss:  0.069, Seconds: 7.64\n",
            "Epoch   5/100 Batch 3660/14303 - Loss:  0.068, Seconds: 7.63\n",
            "Epoch   5/100 Batch 3690/14303 - Loss:  0.070, Seconds: 7.56\n",
            "Epoch   5/100 Batch 3720/14303 - Loss:  0.066, Seconds: 7.63\n",
            "Epoch   5/100 Batch 3750/14303 - Loss:  0.067, Seconds: 7.59\n",
            "Epoch   5/100 Batch 3780/14303 - Loss:  0.066, Seconds: 7.68\n",
            "Epoch   5/100 Batch 3810/14303 - Loss:  0.070, Seconds: 7.82\n",
            "Epoch   5/100 Batch 3840/14303 - Loss:  0.063, Seconds: 7.91\n",
            "Epoch   5/100 Batch 3870/14303 - Loss:  0.067, Seconds: 7.88\n",
            "Epoch   5/100 Batch 3900/14303 - Loss:  0.067, Seconds: 7.77\n",
            "Epoch   5/100 Batch 3930/14303 - Loss:  0.067, Seconds: 7.75\n",
            "Epoch   5/100 Batch 3960/14303 - Loss:  0.071, Seconds: 7.77\n",
            "Epoch   5/100 Batch 3990/14303 - Loss:  0.066, Seconds: 7.80\n",
            "Epoch   5/100 Batch 4020/14303 - Loss:  0.067, Seconds: 7.80\n",
            "Epoch   5/100 Batch 4050/14303 - Loss:  0.067, Seconds: 7.75\n",
            "Epoch   5/100 Batch 4080/14303 - Loss:  0.066, Seconds: 7.82\n",
            "Epoch   5/100 Batch 4110/14303 - Loss:  0.064, Seconds: 7.77\n",
            "Epoch   5/100 Batch 4140/14303 - Loss:  0.074, Seconds: 7.92\n",
            "Epoch   5/100 Batch 4170/14303 - Loss:  0.070, Seconds: 7.95\n",
            "Epoch   5/100 Batch 4200/14303 - Loss:  0.067, Seconds: 7.97\n",
            "Epoch   5/100 Batch 4230/14303 - Loss:  0.065, Seconds: 7.96\n",
            "Epoch   5/100 Batch 4260/14303 - Loss:  0.066, Seconds: 7.96\n",
            "Epoch   5/100 Batch 4290/14303 - Loss:  0.063, Seconds: 7.95\n",
            "Epoch   5/100 Batch 4320/14303 - Loss:  0.066, Seconds: 7.96\n",
            "Epoch   5/100 Batch 4350/14303 - Loss:  0.066, Seconds: 7.97\n",
            "Epoch   5/100 Batch 4380/14303 - Loss:  0.070, Seconds: 8.05\n",
            "Epoch   5/100 Batch 4410/14303 - Loss:  0.067, Seconds: 7.98\n",
            "Epoch   5/100 Batch 4440/14303 - Loss:  0.073, Seconds: 8.00\n",
            "Epoch   5/100 Batch 4470/14303 - Loss:  0.066, Seconds: 8.10\n",
            "Epoch   5/100 Batch 4500/14303 - Loss:  0.066, Seconds: 8.11\n",
            "Epoch   5/100 Batch 4530/14303 - Loss:  0.066, Seconds: 8.10\n",
            "Epoch   5/100 Batch 4560/14303 - Loss:  0.070, Seconds: 8.18\n",
            "Epoch   5/100 Batch 4590/14303 - Loss:  0.065, Seconds: 8.18\n",
            "Epoch   5/100 Batch 4620/14303 - Loss:  0.068, Seconds: 8.13\n",
            "Epoch   5/100 Batch 4650/14303 - Loss:  0.066, Seconds: 8.19\n",
            "Epoch   5/100 Batch 4680/14303 - Loss:  0.067, Seconds: 8.12\n",
            "Epoch   5/100 Batch 4710/14303 - Loss:  0.069, Seconds: 8.11\n",
            "Epoch   5/100 Batch 4740/14303 - Loss:  0.068, Seconds: 8.17\n",
            "Epoch   5/100 Batch 4770/14303 - Loss:  0.068, Seconds: 8.30\n",
            "Epoch   5/100 Batch 4800/14303 - Loss:  0.066, Seconds: 8.29\n",
            "Epoch   5/100 Batch 4830/14303 - Loss:  0.066, Seconds: 8.37\n",
            "Epoch   5/100 Batch 4860/14303 - Loss:  0.067, Seconds: 8.28\n",
            "Epoch   5/100 Batch 4890/14303 - Loss:  0.067, Seconds: 8.32\n",
            "Epoch   5/100 Batch 4920/14303 - Loss:  0.067, Seconds: 8.50\n",
            "Epoch   5/100 Batch 4950/14303 - Loss:  0.063, Seconds: 8.34\n",
            "Epoch   5/100 Batch 4980/14303 - Loss:  0.068, Seconds: 8.27\n",
            "Epoch   5/100 Batch 5010/14303 - Loss:  0.065, Seconds: 8.33\n",
            "Epoch   5/100 Batch 5040/14303 - Loss:  0.065, Seconds: 8.29\n",
            "Epoch   5/100 Batch 5070/14303 - Loss:  0.069, Seconds: 8.50\n",
            "Epoch   5/100 Batch 5100/14303 - Loss:  0.068, Seconds: 8.50\n",
            "Epoch   5/100 Batch 5130/14303 - Loss:  0.068, Seconds: 8.47\n",
            "Epoch   5/100 Batch 5160/14303 - Loss:  0.067, Seconds: 8.48\n",
            "Epoch   5/100 Batch 5190/14303 - Loss:  0.068, Seconds: 8.51\n",
            "Epoch   5/100 Batch 5220/14303 - Loss:  0.065, Seconds: 8.59\n",
            "Epoch   5/100 Batch 5250/14303 - Loss:  0.066, Seconds: 8.47\n",
            "Epoch   5/100 Batch 5280/14303 - Loss:  0.069, Seconds: 8.50\n",
            "Epoch   5/100 Batch 5310/14303 - Loss:  0.066, Seconds: 8.49\n",
            "Epoch   5/100 Batch 5340/14303 - Loss:  0.069, Seconds: 8.47\n",
            "Epoch   5/100 Batch 5370/14303 - Loss:  0.072, Seconds: 8.72\n",
            "Epoch   5/100 Batch 5400/14303 - Loss:  0.066, Seconds: 8.69\n",
            "Epoch   5/100 Batch 5430/14303 - Loss:  0.065, Seconds: 8.72\n",
            "Epoch   5/100 Batch 5460/14303 - Loss:  0.068, Seconds: 8.73\n",
            "Epoch   5/100 Batch 5490/14303 - Loss:  0.072, Seconds: 8.67\n",
            "Epoch   5/100 Batch 5520/14303 - Loss:  0.070, Seconds: 8.66\n",
            "Epoch   5/100 Batch 5550/14303 - Loss:  0.064, Seconds: 8.69\n",
            "Epoch   5/100 Batch 5580/14303 - Loss:  0.065, Seconds: 8.64\n",
            "Epoch   5/100 Batch 5610/14303 - Loss:  0.065, Seconds: 8.61\n",
            "Epoch   5/100 Batch 5640/14303 - Loss:  0.068, Seconds: 8.62\n",
            "Epoch   5/100 Batch 5670/14303 - Loss:  0.069, Seconds: 8.79\n",
            "Epoch   5/100 Batch 5700/14303 - Loss:  0.067, Seconds: 8.88\n",
            "Epoch   5/100 Batch 5730/14303 - Loss:  0.071, Seconds: 8.90\n",
            "Epoch   5/100 Batch 5760/14303 - Loss:  0.064, Seconds: 8.85\n",
            "Epoch   5/100 Batch 5790/14303 - Loss:  0.069, Seconds: 8.81\n",
            "Epoch   5/100 Batch 5820/14303 - Loss:  0.067, Seconds: 8.93\n",
            "Epoch   5/100 Batch 5850/14303 - Loss:  0.067, Seconds: 8.82\n",
            "Epoch   5/100 Batch 5880/14303 - Loss:  0.069, Seconds: 8.87\n",
            "Epoch   5/100 Batch 5910/14303 - Loss:  0.067, Seconds: 8.97\n",
            "Epoch   5/100 Batch 5940/14303 - Loss:  0.065, Seconds: 8.99\n",
            "Epoch   5/100 Batch 5970/14303 - Loss:  0.066, Seconds: 9.07\n",
            "Epoch   5/100 Batch 6000/14303 - Loss:  0.067, Seconds: 8.99\n",
            "Epoch   5/100 Batch 6030/14303 - Loss:  0.071, Seconds: 9.04\n",
            "Epoch   5/100 Batch 6060/14303 - Loss:  0.067, Seconds: 8.99\n",
            "Epoch   5/100 Batch 6090/14303 - Loss:  0.065, Seconds: 9.01\n",
            "Epoch   5/100 Batch 6120/14303 - Loss:  0.068, Seconds: 8.95\n",
            "Epoch   5/100 Batch 6150/14303 - Loss:  0.065, Seconds: 8.97\n",
            "Epoch   5/100 Batch 6180/14303 - Loss:  0.069, Seconds: 9.04\n",
            "Epoch   5/100 Batch 6210/14303 - Loss:  0.066, Seconds: 9.00\n",
            "Epoch   5/100 Batch 6240/14303 - Loss:  0.065, Seconds: 9.13\n",
            "Epoch   5/100 Batch 6270/14303 - Loss:  0.066, Seconds: 9.18\n",
            "Epoch   5/100 Batch 6300/14303 - Loss:  0.067, Seconds: 9.28\n",
            "Epoch   5/100 Batch 6330/14303 - Loss:  0.070, Seconds: 9.22\n",
            "Epoch   5/100 Batch 6360/14303 - Loss:  0.067, Seconds: 9.14\n",
            "Epoch   5/100 Batch 6390/14303 - Loss:  0.067, Seconds: 9.18\n",
            "Epoch   5/100 Batch 6420/14303 - Loss:  0.068, Seconds: 9.16\n",
            "Epoch   5/100 Batch 6450/14303 - Loss:  0.063, Seconds: 9.21\n",
            "Epoch   5/100 Batch 6480/14303 - Loss:  0.067, Seconds: 9.24\n",
            "Epoch   5/100 Batch 6510/14303 - Loss:  0.066, Seconds: 9.15\n",
            "Epoch   5/100 Batch 6540/14303 - Loss:  0.063, Seconds: 9.33\n",
            "Epoch   5/100 Batch 6570/14303 - Loss:  0.064, Seconds: 9.50\n",
            "Epoch   5/100 Batch 6600/14303 - Loss:  0.066, Seconds: 9.36\n",
            "Epoch   5/100 Batch 6630/14303 - Loss:  0.064, Seconds: 9.39\n",
            "Epoch   5/100 Batch 6660/14303 - Loss:  0.068, Seconds: 9.32\n",
            "Epoch   5/100 Batch 6690/14303 - Loss:  0.067, Seconds: 9.38\n",
            "Epoch   5/100 Batch 6720/14303 - Loss:  0.068, Seconds: 9.38\n",
            "Epoch   5/100 Batch 6750/14303 - Loss:  0.070, Seconds: 9.42\n",
            "Epoch   5/100 Batch 6780/14303 - Loss:  0.066, Seconds: 9.35\n",
            "Epoch   5/100 Batch 6810/14303 - Loss:  0.066, Seconds: 9.43\n",
            "Epoch   5/100 Batch 6840/14303 - Loss:  0.066, Seconds: 9.58\n",
            "Epoch   5/100 Batch 6870/14303 - Loss:  0.066, Seconds: 9.75\n",
            "Epoch   5/100 Batch 6900/14303 - Loss:  0.070, Seconds: 9.60\n",
            "Epoch   5/100 Batch 6930/14303 - Loss:  0.067, Seconds: 9.59\n",
            "Epoch   5/100 Batch 6960/14303 - Loss:  0.066, Seconds: 9.64\n",
            "Epoch   5/100 Batch 6990/14303 - Loss:  0.068, Seconds: 9.51\n",
            "Epoch   5/100 Batch 7020/14303 - Loss:  0.066, Seconds: 9.51\n",
            "Epoch   5/100 Batch 7050/14303 - Loss:  0.066, Seconds: 9.52\n",
            "Epoch   5/100 Batch 7080/14303 - Loss:  0.066, Seconds: 9.57\n",
            "Epoch   5/100 Batch 7110/14303 - Loss:  0.068, Seconds: 9.71\n",
            "Epoch   5/100 Batch 7140/14303 - Loss:  0.066, Seconds: 9.65\n",
            "Epoch   5/100 Batch 7170/14303 - Loss:  0.065, Seconds: 9.74\n",
            "Epoch   5/100 Batch 7200/14303 - Loss:  0.065, Seconds: 9.72\n",
            "Epoch   5/100 Batch 7230/14303 - Loss:  0.066, Seconds: 9.69\n",
            "Epoch   5/100 Batch 7260/14303 - Loss:  0.063, Seconds: 9.68\n",
            "Epoch   5/100 Batch 7290/14303 - Loss:  0.063, Seconds: 9.72\n",
            "Epoch   5/100 Batch 7320/14303 - Loss:  0.064, Seconds: 9.64\n",
            "Epoch   5/100 Batch 7350/14303 - Loss:  0.069, Seconds: 9.74\n",
            "Epoch   5/100 Batch 7380/14303 - Loss:  0.065, Seconds: 9.90\n",
            "Epoch   5/100 Batch 7410/14303 - Loss:  0.065, Seconds: 9.93\n",
            "Epoch   5/100 Batch 7440/14303 - Loss:  0.068, Seconds: 9.86\n",
            "Epoch   5/100 Batch 7470/14303 - Loss:  0.069, Seconds: 9.92\n",
            "Epoch   5/100 Batch 7500/14303 - Loss:  0.066, Seconds: 9.81\n",
            "Epoch   5/100 Batch 7530/14303 - Loss:  0.067, Seconds: 9.87\n",
            "Epoch   5/100 Batch 7560/14303 - Loss:  0.065, Seconds: 9.84\n",
            "Epoch   5/100 Batch 7590/14303 - Loss:  0.066, Seconds: 9.81\n",
            "Epoch   5/100 Batch 7620/14303 - Loss:  0.065, Seconds: 10.02\n",
            "Epoch   5/100 Batch 7650/14303 - Loss:  0.067, Seconds: 10.16\n",
            "Epoch   5/100 Batch 7680/14303 - Loss:  0.070, Seconds: 10.03\n",
            "Epoch   5/100 Batch 7710/14303 - Loss:  0.065, Seconds: 10.01\n",
            "Epoch   5/100 Batch 7740/14303 - Loss:  0.067, Seconds: 10.17\n",
            "Epoch   5/100 Batch 7770/14303 - Loss:  0.064, Seconds: 10.14\n",
            "Epoch   5/100 Batch 7800/14303 - Loss:  0.068, Seconds: 10.00\n",
            "Epoch   5/100 Batch 7830/14303 - Loss:  0.065, Seconds: 10.09\n",
            "Epoch   5/100 Batch 7860/14303 - Loss:  0.069, Seconds: 10.08\n",
            "Epoch   5/100 Batch 7890/14303 - Loss:  0.067, Seconds: 10.29\n",
            "Epoch   5/100 Batch 7920/14303 - Loss:  0.065, Seconds: 10.19\n",
            "Epoch   5/100 Batch 7950/14303 - Loss:  0.068, Seconds: 10.22\n",
            "Epoch   5/100 Batch 7980/14303 - Loss:  0.067, Seconds: 10.28\n",
            "Epoch   5/100 Batch 8010/14303 - Loss:  0.063, Seconds: 10.17\n",
            "Epoch   5/100 Batch 8040/14303 - Loss:  0.069, Seconds: 10.22\n",
            "Epoch   5/100 Batch 8070/14303 - Loss:  0.064, Seconds: 10.21\n",
            "Epoch   5/100 Batch 8100/14303 - Loss:  0.064, Seconds: 10.27\n",
            "Epoch   5/100 Batch 8130/14303 - Loss:  0.066, Seconds: 10.44\n",
            "Epoch   5/100 Batch 8160/14303 - Loss:  0.062, Seconds: 10.49\n",
            "Epoch   5/100 Batch 8190/14303 - Loss:  0.066, Seconds: 10.37\n",
            "Epoch   5/100 Batch 8220/14303 - Loss:  0.066, Seconds: 10.39\n",
            "Epoch   5/100 Batch 8250/14303 - Loss:  0.063, Seconds: 10.35\n",
            "Epoch   5/100 Batch 8280/14303 - Loss:  0.069, Seconds: 10.39\n",
            "Epoch   5/100 Batch 8310/14303 - Loss:  0.065, Seconds: 10.53\n",
            "Epoch   5/100 Batch 8340/14303 - Loss:  0.067, Seconds: 10.37\n",
            "Epoch   5/100 Batch 8370/14303 - Loss:  0.069, Seconds: 10.56\n",
            "Epoch   5/100 Batch 8400/14303 - Loss:  0.066, Seconds: 10.54\n",
            "Epoch   5/100 Batch 8430/14303 - Loss:  0.067, Seconds: 10.59\n",
            "Epoch   5/100 Batch 8460/14303 - Loss:  0.064, Seconds: 10.61\n",
            "Epoch   5/100 Batch 8490/14303 - Loss:  0.066, Seconds: 10.65\n",
            "Epoch   5/100 Batch 8520/14303 - Loss:  0.065, Seconds: 10.66\n",
            "Epoch   5/100 Batch 8550/14303 - Loss:  0.061, Seconds: 10.64\n",
            "Epoch   5/100 Batch 8580/14303 - Loss:  0.066, Seconds: 10.59\n",
            "Epoch   5/100 Batch 8610/14303 - Loss:  0.065, Seconds: 10.97\n",
            "Epoch   5/100 Batch 8640/14303 - Loss:  0.065, Seconds: 10.77\n",
            "Epoch   5/100 Batch 8670/14303 - Loss:  0.065, Seconds: 10.71\n",
            "Epoch   5/100 Batch 8700/14303 - Loss:  0.065, Seconds: 10.72\n",
            "Epoch   5/100 Batch 8730/14303 - Loss:  0.066, Seconds: 10.76\n",
            "Epoch   5/100 Batch 8760/14303 - Loss:  0.064, Seconds: 10.80\n",
            "Epoch   5/100 Batch 8790/14303 - Loss:  0.063, Seconds: 10.77\n",
            "Epoch   5/100 Batch 8820/14303 - Loss:  0.064, Seconds: 10.80\n",
            "Epoch   5/100 Batch 8850/14303 - Loss:  0.063, Seconds: 10.92\n",
            "Epoch   5/100 Batch 8880/14303 - Loss:  0.066, Seconds: 10.89\n",
            "Epoch   5/100 Batch 8910/14303 - Loss:  0.066, Seconds: 10.82\n",
            "Epoch   5/100 Batch 8940/14303 - Loss:  0.065, Seconds: 10.98\n",
            "Epoch   5/100 Batch 8970/14303 - Loss:  0.066, Seconds: 10.91\n",
            "Epoch   5/100 Batch 9000/14303 - Loss:  0.065, Seconds: 10.90\n",
            "Epoch   5/100 Batch 9030/14303 - Loss:  0.066, Seconds: 10.92\n",
            "Epoch   5/100 Batch 9060/14303 - Loss:  0.066, Seconds: 11.07\n",
            "Epoch   5/100 Batch 9090/14303 - Loss:  0.063, Seconds: 11.04\n",
            "Epoch   5/100 Batch 9120/14303 - Loss:  0.068, Seconds: 11.14\n",
            "Epoch   5/100 Batch 9150/14303 - Loss:  0.067, Seconds: 11.17\n",
            "Epoch   5/100 Batch 9180/14303 - Loss:  0.065, Seconds: 11.03\n",
            "Epoch   5/100 Batch 9210/14303 - Loss:  0.065, Seconds: 11.08\n",
            "Epoch   5/100 Batch 9240/14303 - Loss:  0.067, Seconds: 11.10\n",
            "Epoch   5/100 Batch 9270/14303 - Loss:  0.065, Seconds: 11.25\n",
            "Epoch   5/100 Batch 9300/14303 - Loss:  0.066, Seconds: 11.26\n",
            "Epoch   5/100 Batch 9330/14303 - Loss:  0.066, Seconds: 11.29\n",
            "Epoch   5/100 Batch 9360/14303 - Loss:  0.067, Seconds: 11.39\n",
            "Epoch   5/100 Batch 9390/14303 - Loss:  0.068, Seconds: 11.36\n",
            "Epoch   5/100 Batch 9420/14303 - Loss:  0.066, Seconds: 11.41\n",
            "Epoch   5/100 Batch 9450/14303 - Loss:  0.066, Seconds: 11.22\n",
            "Epoch   5/100 Batch 9480/14303 - Loss:  0.065, Seconds: 11.41\n",
            "Epoch   5/100 Batch 9510/14303 - Loss:  0.066, Seconds: 11.39\n",
            "Epoch   5/100 Batch 9540/14303 - Loss:  0.064, Seconds: 11.43\n",
            "Epoch   5/100 Batch 9570/14303 - Loss:  0.066, Seconds: 11.54\n",
            "Epoch   5/100 Batch 9600/14303 - Loss:  0.066, Seconds: 11.41\n",
            "Epoch   5/100 Batch 9630/14303 - Loss:  0.067, Seconds: 11.46\n",
            "Epoch   5/100 Batch 9660/14303 - Loss:  0.067, Seconds: 11.36\n",
            "Epoch   5/100 Batch 9690/14303 - Loss:  0.066, Seconds: 11.63\n",
            "Epoch   5/100 Batch 9720/14303 - Loss:  0.064, Seconds: 11.62\n",
            "Epoch   5/100 Batch 9750/14303 - Loss:  0.064, Seconds: 11.72\n",
            "Epoch   5/100 Batch 9780/14303 - Loss:  0.064, Seconds: 11.64\n",
            "Epoch   5/100 Batch 9810/14303 - Loss:  0.065, Seconds: 11.60\n",
            "Epoch   5/100 Batch 9840/14303 - Loss:  0.067, Seconds: 11.61\n",
            "Epoch   5/100 Batch 9870/14303 - Loss:  0.067, Seconds: 11.78\n",
            "Epoch   5/100 Batch 9900/14303 - Loss:  0.066, Seconds: 11.75\n",
            "Epoch   5/100 Batch 9930/14303 - Loss:  0.066, Seconds: 11.87\n",
            "Epoch   5/100 Batch 9960/14303 - Loss:  0.064, Seconds: 11.83\n",
            "Epoch   5/100 Batch 9990/14303 - Loss:  0.064, Seconds: 11.85\n",
            "Epoch   5/100 Batch 10020/14303 - Loss:  0.065, Seconds: 11.84\n",
            "Epoch   5/100 Batch 10050/14303 - Loss:  0.064, Seconds: 11.95\n",
            "Epoch   5/100 Batch 10080/14303 - Loss:  0.065, Seconds: 12.02\n",
            "Epoch   5/100 Batch 10110/14303 - Loss:  0.063, Seconds: 12.45\n",
            "Epoch   5/100 Batch 10140/14303 - Loss:  0.067, Seconds: 12.26\n",
            "Epoch   5/100 Batch 10170/14303 - Loss:  0.063, Seconds: 12.15\n",
            "Epoch   5/100 Batch 10200/14303 - Loss:  0.063, Seconds: 12.08\n",
            "Epoch   5/100 Batch 10230/14303 - Loss:  0.065, Seconds: 12.10\n",
            "Epoch   5/100 Batch 10260/14303 - Loss:  0.063, Seconds: 12.20\n",
            "Epoch   5/100 Batch 10290/14303 - Loss:  0.065, Seconds: 12.19\n",
            "Epoch   5/100 Batch 10320/14303 - Loss:  0.063, Seconds: 12.25\n",
            "Epoch   5/100 Batch 10350/14303 - Loss:  0.062, Seconds: 12.21\n",
            "Epoch   5/100 Batch 10380/14303 - Loss:  0.061, Seconds: 12.16\n",
            "Epoch   5/100 Batch 10410/14303 - Loss:  0.066, Seconds: 12.19\n",
            "Epoch   5/100 Batch 10440/14303 - Loss:  0.066, Seconds: 12.39\n",
            "Epoch   5/100 Batch 10470/14303 - Loss:  0.067, Seconds: 12.36\n",
            "Epoch   5/100 Batch 10500/14303 - Loss:  0.062, Seconds: 12.37\n",
            "Epoch   5/100 Batch 10530/14303 - Loss:  0.066, Seconds: 12.42\n",
            "Epoch   5/100 Batch 10560/14303 - Loss:  0.064, Seconds: 12.38\n",
            "Epoch   5/100 Batch 10590/14303 - Loss:  0.067, Seconds: 12.48\n",
            "Epoch   5/100 Batch 10620/14303 - Loss:  0.065, Seconds: 12.53\n",
            "Epoch   5/100 Batch 10650/14303 - Loss:  0.068, Seconds: 12.53\n",
            "Epoch   5/100 Batch 10680/14303 - Loss:  0.065, Seconds: 12.60\n",
            "Epoch   5/100 Batch 10710/14303 - Loss:  0.065, Seconds: 12.63\n",
            "Epoch   5/100 Batch 10740/14303 - Loss:  0.062, Seconds: 12.67\n",
            "Epoch   5/100 Batch 10770/14303 - Loss:  0.064, Seconds: 12.77\n",
            "Epoch   5/100 Batch 10800/14303 - Loss:  0.065, Seconds: 12.78\n",
            "Epoch   5/100 Batch 10830/14303 - Loss:  0.065, Seconds: 12.76\n",
            "Epoch   5/100 Batch 10860/14303 - Loss:  0.064, Seconds: 12.96\n",
            "Epoch   5/100 Batch 10890/14303 - Loss:  0.065, Seconds: 12.77\n",
            "Epoch   5/100 Batch 10920/14303 - Loss:  0.068, Seconds: 12.89\n",
            "Epoch   5/100 Batch 10950/14303 - Loss:  0.068, Seconds: 12.90\n",
            "Epoch   5/100 Batch 10980/14303 - Loss:  0.064, Seconds: 12.93\n",
            "Epoch   5/100 Batch 11010/14303 - Loss:  0.064, Seconds: 12.92\n",
            "Epoch   5/100 Batch 11040/14303 - Loss:  0.064, Seconds: 12.98\n",
            "Epoch   5/100 Batch 11070/14303 - Loss:  0.063, Seconds: 13.08\n",
            "Epoch   5/100 Batch 11100/14303 - Loss:  0.068, Seconds: 13.15\n",
            "Epoch   5/100 Batch 11130/14303 - Loss:  0.067, Seconds: 13.04\n",
            "Epoch   5/100 Batch 11160/14303 - Loss:  0.063, Seconds: 13.07\n",
            "Epoch   5/100 Batch 11190/14303 - Loss:  0.062, Seconds: 13.12\n",
            "Epoch   5/100 Batch 11220/14303 - Loss:  0.066, Seconds: 13.37\n",
            "Epoch   5/100 Batch 11250/14303 - Loss:  0.065, Seconds: 13.32\n",
            "Epoch   5/100 Batch 11280/14303 - Loss:  0.064, Seconds: 13.29\n",
            "Epoch   5/100 Batch 11310/14303 - Loss:  0.064, Seconds: 13.30\n",
            "Epoch   5/100 Batch 11340/14303 - Loss:  0.065, Seconds: 13.41\n",
            "Epoch   5/100 Batch 11370/14303 - Loss:  0.066, Seconds: 13.58\n",
            "Epoch   5/100 Batch 11400/14303 - Loss:  0.065, Seconds: 13.54\n",
            "Epoch   5/100 Batch 11430/14303 - Loss:  0.064, Seconds: 13.55\n",
            "Epoch   5/100 Batch 11460/14303 - Loss:  0.064, Seconds: 13.50\n",
            "Epoch   5/100 Batch 11490/14303 - Loss:  0.094, Seconds: 13.64\n",
            "Epoch   5/100 Batch 11520/14303 - Loss:  0.317, Seconds: 13.94\n",
            "Epoch   5/100 Batch 11550/14303 - Loss:  0.182, Seconds: 13.83\n",
            "Epoch   5/100 Batch 11580/14303 - Loss:  0.131, Seconds: 13.79\n",
            "Epoch   5/100 Batch 11610/14303 - Loss:  0.119, Seconds: 13.79\n",
            "Epoch   5/100 Batch 11640/14303 - Loss:  0.113, Seconds: 13.93\n",
            "Epoch   5/100 Batch 11670/14303 - Loss:  0.105, Seconds: 13.95\n",
            "Epoch   5/100 Batch 11700/14303 - Loss:  0.104, Seconds: 13.91\n",
            "Epoch   5/100 Batch 11730/14303 - Loss:  0.099, Seconds: 14.01\n",
            "Epoch   5/100 Batch 11760/14303 - Loss:  0.097, Seconds: 14.07\n",
            "Epoch   5/100 Batch 11790/14303 - Loss:  0.098, Seconds: 14.08\n",
            "Epoch   5/100 Batch 11820/14303 - Loss:  0.092, Seconds: 14.14\n",
            "Epoch   5/100 Batch 11850/14303 - Loss:  0.094, Seconds: 14.12\n",
            "Epoch   5/100 Batch 11880/14303 - Loss:  0.091, Seconds: 14.32\n",
            "Epoch   5/100 Batch 11910/14303 - Loss:  0.087, Seconds: 14.24\n",
            "Epoch   5/100 Batch 11940/14303 - Loss:  0.086, Seconds: 14.25\n",
            "Epoch   5/100 Batch 11970/14303 - Loss:  0.086, Seconds: 14.31\n",
            "Epoch   5/100 Batch 12000/14303 - Loss:  0.085, Seconds: 14.46\n",
            "Epoch   5/100 Batch 12030/14303 - Loss:  0.083, Seconds: 14.57\n",
            "Epoch   5/100 Batch 12060/14303 - Loss:  0.085, Seconds: 14.52\n",
            "Epoch   5/100 Batch 12090/14303 - Loss:  0.083, Seconds: 14.58\n",
            "Epoch   5/100 Batch 12120/14303 - Loss:  0.084, Seconds: 14.73\n",
            "Epoch   5/100 Batch 12150/14303 - Loss:  0.083, Seconds: 14.70\n",
            "Epoch   5/100 Batch 12180/14303 - Loss:  0.083, Seconds: 14.76\n",
            "Epoch   5/100 Batch 12210/14303 - Loss:  0.081, Seconds: 14.89\n",
            "Epoch   5/100 Batch 12240/14303 - Loss:  0.082, Seconds: 14.79\n",
            "Epoch   5/100 Batch 12270/14303 - Loss:  0.081, Seconds: 14.86\n",
            "Epoch   5/100 Batch 12300/14303 - Loss:  0.081, Seconds: 14.99\n",
            "Epoch   5/100 Batch 12330/14303 - Loss:  0.080, Seconds: 15.12\n",
            "Epoch   5/100 Batch 12360/14303 - Loss:  0.079, Seconds: 15.07\n",
            "Epoch   5/100 Batch 12390/14303 - Loss:  0.077, Seconds: 15.15\n",
            "Epoch   5/100 Batch 12420/14303 - Loss:  0.080, Seconds: 15.24\n",
            "Epoch   5/100 Batch 12450/14303 - Loss:  0.075, Seconds: 15.33\n",
            "Epoch   5/100 Batch 12480/14303 - Loss:  0.076, Seconds: 15.31\n",
            "Epoch   5/100 Batch 12510/14303 - Loss:  0.077, Seconds: 15.49\n",
            "Epoch   5/100 Batch 12540/14303 - Loss:  0.078, Seconds: 15.46\n",
            "Epoch   5/100 Batch 12570/14303 - Loss:  0.074, Seconds: 15.52\n",
            "Epoch   5/100 Batch 12600/14303 - Loss:  0.075, Seconds: 15.63\n",
            "Epoch   5/100 Batch 12630/14303 - Loss:  0.075, Seconds: 15.84\n",
            "Epoch   5/100 Batch 12660/14303 - Loss:  0.074, Seconds: 15.87\n",
            "Epoch   5/100 Batch 12690/14303 - Loss:  0.075, Seconds: 15.95\n",
            "Epoch   5/100 Batch 12720/14303 - Loss:  0.075, Seconds: 16.03\n",
            "Epoch   5/100 Batch 12750/14303 - Loss:  0.075, Seconds: 16.05\n",
            "Epoch   5/100 Batch 12780/14303 - Loss:  0.073, Seconds: 16.25\n",
            "Epoch   5/100 Batch 12810/14303 - Loss:  0.073, Seconds: 16.21\n",
            "Epoch   5/100 Batch 12840/14303 - Loss:  0.077, Seconds: 16.34\n",
            "Epoch   5/100 Batch 12870/14303 - Loss:  0.073, Seconds: 16.35\n",
            "Epoch   5/100 Batch 12900/14303 - Loss:  0.073, Seconds: 16.50\n",
            "Epoch   5/100 Batch 12930/14303 - Loss:  0.072, Seconds: 16.68\n",
            "Epoch   5/100 Batch 12960/14303 - Loss:  0.074, Seconds: 16.60\n",
            "Epoch   5/100 Batch 12990/14303 - Loss:  0.071, Seconds: 16.90\n",
            "Epoch   5/100 Batch 13020/14303 - Loss:  0.070, Seconds: 16.85\n",
            "Epoch   5/100 Batch 13050/14303 - Loss:  0.070, Seconds: 17.04\n",
            "Epoch   5/100 Batch 13080/14303 - Loss:  0.072, Seconds: 17.02\n",
            "Epoch   5/100 Batch 13110/14303 - Loss:  0.073, Seconds: 17.22\n",
            "Epoch   5/100 Batch 13140/14303 - Loss:  0.073, Seconds: 17.36\n",
            "Epoch   5/100 Batch 13170/14303 - Loss:  0.071, Seconds: 17.50\n",
            "Epoch   5/100 Batch 13200/14303 - Loss:  0.070, Seconds: 17.53\n",
            "Epoch   5/100 Batch 13230/14303 - Loss:  0.072, Seconds: 17.89\n",
            "Epoch   5/100 Batch 13260/14303 - Loss:  0.072, Seconds: 17.77\n",
            "Epoch   5/100 Batch 13290/14303 - Loss:  0.069, Seconds: 17.95\n",
            "Epoch   5/100 Batch 13320/14303 - Loss:  0.071, Seconds: 18.02\n",
            "Epoch   5/100 Batch 13350/14303 - Loss:  0.071, Seconds: 18.16\n",
            "Epoch   5/100 Batch 13380/14303 - Loss:  0.072, Seconds: 18.43\n",
            "Epoch   5/100 Batch 13410/14303 - Loss:  0.068, Seconds: 18.39\n",
            "Epoch   5/100 Batch 13440/14303 - Loss:  0.071, Seconds: 18.53\n",
            "Epoch   5/100 Batch 13470/14303 - Loss:  0.070, Seconds: 18.80\n",
            "Epoch   5/100 Batch 13500/14303 - Loss:  0.072, Seconds: 18.99\n",
            "Epoch   5/100 Batch 13530/14303 - Loss:  0.069, Seconds: 19.10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}