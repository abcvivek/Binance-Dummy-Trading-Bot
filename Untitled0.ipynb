{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xbLD0alDNNYFFkupqXz2AXswtJIRwWRx",
      "authorship_tag": "ABX9TyO+NnumTf/FFdHv/GaV3kq6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abcvivek/Binance-Dummy-Trading-Bot/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrb2welaDidG",
        "colab_type": "code",
        "outputId": "426de9a2-9bec-41f3-eb35-b76629c14af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 1.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oM4KcA1HpaC",
        "colab_type": "code",
        "outputId": "a4fdcb5c-408f-4f17-9059-4d83c6ebf036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\"\"\" Importing Libraries\n",
        " numpy are used for maths calculations\n",
        " tenserflow is used for building Neural Networks\n",
        " os will help in managing all the os related stuff \n",
        " re is used as regular expression\n",
        "train_test_split will help in dividing dataset into training set and testing set \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import time\n",
        "from math import ceil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from collections import namedtuple\n",
        "\n",
        "################ LOADING OF DATA ##################\n",
        "\n",
        "path = '/content/drive/My Drive/clean.txt'\n",
        "\n",
        "# Function to load the contents of the dataset\n",
        "     \n",
        "def load_file(path): \n",
        "  input_file = os.path.join(path) \n",
        "  with open(input_file) as file:\n",
        "    File = file.read()\n",
        "  return File\n",
        "       \n",
        "file_content = load_file(path)\n",
        "\n",
        "\n",
        "#################  Preprocessing of Data  #################\n",
        "\n",
        "# Create a dictionary to convert the vocabulary (characters) to integers\n",
        "\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "\n",
        "for character in file_content:\n",
        "  if character not in vocab_to_int:\n",
        "    vocab_to_int[character] = count\n",
        "    count += 1\n",
        "\n",
        "\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "  vocab_to_int[code] = count\n",
        "  count += 1\n",
        "\n",
        "\n",
        "# Check the size of vocabulary and all of the values\n",
        "\n",
        "print(\"The vocabulary contains {} characters.\".format(len(vocab_to_int)))\n",
        "print(sorted(vocab_to_int))\n",
        "\n",
        "\n",
        "#Create another dictionary to convert integers to their respective characters\n",
        "\n",
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character\n",
        "\n",
        "print(int_to_vocab.items())\n",
        "\n",
        "# Split the text from the file into sentences.\n",
        "\n",
        "sentences = []\n",
        "for sentence in file_content.splitlines():\n",
        "  sentences.append(sentence)\n",
        "print(\" Dataset contains {} sentences.\".format(len(sentences)))\n",
        "\n",
        "\n",
        "# Convert character sentences to integer sentence\n",
        "\n",
        "int_sentences = []\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)\n",
        "\n",
        "\n",
        "# Limit the data we will use to train our model\n",
        "\n",
        "max_length = 250\n",
        "min_length = 30\n",
        "\n",
        "\n",
        "good_sentences = []\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))\n",
        "\n",
        "\n",
        "# Split the data into training, testing and validation sentences\n",
        "\n",
        "training, testing = train_test_split(good_sentences, test_size = 0.10, random_state = 2)\n",
        "testing, validation = train_test_split(testing, test_size = 0.70, random_state = 2)\n",
        "print(\"Number of Training sentences:\", len(training))\n",
        "print(\"Number of Validiation sentences:\", len(validation))\n",
        "print(\"Number of Testing sentences:\", len(testing))\n",
        "\n",
        "\n",
        "# Sort the sentences by length to reduce padding, which will allow the model to train faster\n",
        "\n",
        "training_sorted = []\n",
        "validation_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "\n",
        "    for sentence in validation:\n",
        "        if len(sentence) == i:\n",
        "            validation_sorted.append(sentence)\n",
        "\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)\n",
        "\n",
        "\n",
        "# Generate Artificial noise into Correct sentence\n",
        "\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "\n",
        "    '''Relocate, remove, or add characters to create spelling mistakes''' \n",
        "\n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence\n",
        "\n",
        "\n",
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()\n",
        "\n",
        "\n",
        "###########################  Building the Model  ###############################\n",
        "\n",
        "# Model input pipes for data feed to the model\n",
        "\n",
        "'''\n",
        "Tensorflow placeholders acts as pipes into the model.\n",
        "name_scope makes sure so given values are from the same graph.\n",
        "'''\n",
        "\n",
        "def model_inputs():\n",
        "\n",
        "    with tf.name_scope('inputs'):\n",
        "        # ARGS: dtype, shape of the tensor to be fed, name for operation\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "\n",
        "    # ARGS: input tensor, name for operation\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length\n",
        "\n",
        "\n",
        "# Remove last word from each batch, add <GO> token to the start of each batch\n",
        "\n",
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "\n",
        "    with tf.name_scope(\"processing_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "    return dec_input\n",
        "\n",
        "\n",
        "#Encoding layer takes the input sequence and creates a encoded representation\n",
        "\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "\n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "            return enc_output, enc_state\n",
        "\n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "\n",
        "            # Concat outputs\n",
        "            enc_output = tf.concat(enc_output, 2)\n",
        "\n",
        "            # Use only forwarded state\n",
        "            return enc_output, enc_state[0]\n",
        "\n",
        "\n",
        "# Create training logits\n",
        "\n",
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length):\n",
        "\n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, sequence_length=targets_length, time_major=False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
        "        training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return training_logits\n",
        "\n",
        "# Create inference logits\n",
        "\n",
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, max_target_length, batch_size):\n",
        "     \n",
        "     print(max_target_length)\n",
        "\n",
        "     with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
        "        inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits\n",
        "\n",
        "#Create the decoding cell and attention.\n",
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "\n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "\n",
        "    output_layer = Dense(vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, inputs_length, normalize=False, name='BahdanauAttention')\n",
        "\n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
        "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
        "    initial_state = initial_state.clone(cell_state=enc_state)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], dec_cell, initial_state, output_layer, max_target_length, batch_size)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "\n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, enc_embed_input, keep_prob, direction)\n",
        "\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "\n",
        "###############   Hyper-Parameters   ##################\n",
        "\n",
        "# The default parameters\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "num_layers = 4\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.90\n",
        "keep_probability = 0.65 #0.75\n",
        "\n",
        "\n",
        "def build_accuracy(predictions, targets):\n",
        "    correct_prediction = tf.equal(tf.cast(tf.round(predictions), tf.int32), targets)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# Build graph\n",
        "\n",
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Load model inputs\n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, inputs_length, targets_length, max_target_length, len(vocab_to_int)+1, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    accuracy = build_accuracy(predictions, targets)\n",
        "\n",
        "    # Merge summaries\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    # Export the nodes\n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length', 'predictions', 'merged', 'train_op','accuracy', 'optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "\n",
        "def get_batches(sentences, batch_size, threshold):\n",
        "\n",
        "    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n",
        "       With each epoch, sentences will receive new mistakes\"\"\"\n",
        "\n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "\n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "\n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "\n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "\n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths\n",
        "\n",
        "\n",
        "################## Training the Model   ######################\n",
        "\n",
        "def MakeSentenceReadable(correct):\n",
        "  correct_sentence = \"\"\n",
        "  for i in correct:\n",
        "    if i < 28:\n",
        "      correct_sentence += int_to_vocab[i]\n",
        "  return correct_sentence.strip()\n",
        "      \n",
        "\n",
        "\n",
        "def train(model, epochs):\n",
        "\n",
        "    '''Train the RNN'''    \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 3 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
        "        per_epoch = 1 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
        "\n",
        "       \n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "\n",
        "            checkpoint = \"./Model-{}.ckpt\".format(epoch_i)\n",
        "\n",
        "            print()\n",
        "            print(\"Training Model: {}\".format(epoch_i))\n",
        "\n",
        "            train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(epoch_i), sess.graph)\n",
        "            test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(epoch_i))\n",
        "\n",
        "\n",
        "             # Per batch\n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(training_sorted,batch_size,threshold)):\n",
        "                start_time = time.time()\n",
        "                summary, loss, _ = sess.run([model.merged, model.cost, model.train_op],\n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                # Print info\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    # Reset\n",
        "\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "\n",
        "                #### Run Validation Testing ####\n",
        "\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(validation_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged, model.cost],\n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Record the progress of testing\n",
        "\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "\n",
        "                    # Print Result\n",
        "\n",
        "                    for i in range(100, 120):\n",
        "\n",
        "                        correct = validation_sorted[i]\n",
        "                        text = noise_maker(validation_sorted[i],threshold)\n",
        "                        answer_logits = sess.run(model.predictions, {model.inputs: [text]* batch_size,\n",
        "                                                                 model.inputs_length: [len(text)]* batch_size,\n",
        "                                                                 model.targets_length: [len(text)+1],\n",
        "                                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "                        correct_sentence = MakeSentenceReadable(correct)\n",
        "                        text_sentence = MakeSentenceReadable(text)\n",
        "                        answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "\n",
        "                       \n",
        "                        # # Remove <PAD> from output\n",
        "                        # pad = vocab_to_int[\"<PAD>\"]\n",
        "                        # eos = vocab_to_int[\"<EOS>\"]\n",
        "\n",
        "                        # answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != eos])\n",
        "                        # answer_logits.strip()\n",
        "                        # #answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "                        # correct = \"\".join([int_to_vocab[i] for i in correct if i != eos])\n",
        "                        # correct.strip()\n",
        "\n",
        "                        print('  Validation Input: {}'.format(text_sentence))\n",
        "                        print('  Validation Output: {}'.format(answer_logits_sentence))\n",
        "                        print('  Correct: {}'.format(correct_sentence))\n",
        "                        print('  Is Correct: {}'.format(answer_logits_sentence == correct_sentence))\n",
        "                        print()\n",
        "                    \n",
        "\n",
        "                    batch_time_testing = 0\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'.format(batch_loss_testing / n_batches_testing, batch_time_testing))\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "                    if batch_loss_testing <= min(testing_loss_summary):\n",
        "                        print('New Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"./Model-{}.ckpt\".format(epoch_i)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "                    else:\n",
        "                        print(\"No Improvement.\")\n",
        "                        stop_early += 1\n",
        "                        if stop_early == stop:\n",
        "                            break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "# Train the model with the desired tuning parameters\n",
        "\n",
        "for keep_probability in [0.65]:\n",
        "    for num_layers in [4]:\n",
        "        for threshold in [0.90]:\n",
        "            \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
        "            train(model, epochs)\n",
        "\n",
        "\n",
        "#################   Testing the Model   ################\n",
        "\n",
        "def test(model,testing_set,log_string):\n",
        "    # Start session\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "        print()\n",
        "        print(\"Testing LSTM Model\")\n",
        "\n",
        "        testing_check = (len(testing_set)//batch_size//1)-1\n",
        "        tested = 0\n",
        "        is_correct = 0\n",
        "        checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "        saver.restore(sess, checkpoint)\n",
        "\n",
        "        # Per batch\n",
        "        for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(testing_set,batch_size,threshold)):\n",
        "            if batch_i % testing_check == 0 and batch_i > 0:  \n",
        "                \n",
        "                print_tested_each = 100\n",
        "\n",
        "                for i in range(0, len(testing_set)):\n",
        "\n",
        "                    if (tested > print_tested_each):\n",
        "                        print_tested_each  += 100\n",
        "                        print(\"Tested {}% of test set\".format((ceil(i / len(testing_set) * 100) * 100) / 100.0))\n",
        "\n",
        "                    text = noise_maker(testing_set[i],threshold)\n",
        "                    correct = testing_set[i]\n",
        "                    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size,\n",
        "                                                             model.inputs_length: [len(text)]*batch_size,\n",
        "                                                             model.targets_length: [len(text)+1],\n",
        "                                                             model.keep_prob: [1.0]})[0]\n",
        "\n",
        "                    # # Remove <PAD> from output\n",
        "                    # pad = vocab_to_int[\"<PAD>\"]\n",
        "                    # eos = vocab_to_int[\"<EOS>\"]\n",
        "\n",
        "                    # answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != eos])\n",
        "                    # #answer_logits = \"\".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "                    # correct = \"\".join([int_to_vocab[i] for i in correct if i != eos])\n",
        "\n",
        "                    correct_sentence = MakeSentenceReadable(correct)\n",
        "                    text_sentence = MakeSentenceReadable(text)\n",
        "                    answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "\n",
        "                    tested += 1\n",
        "                    if (answer_logits_sentence == correct_sentence):\n",
        "                        is_correct += 1\n",
        "\n",
        "                # Reset\n",
        "                print(\"Accuracy %: {}%\".format((ceil((is_correct / tested) * 100) * 100) / 100.0))\n",
        "                print(\"Exact Accuracy: {}\".format(is_correct / tested))\n",
        "                \n",
        "                return is_correct / tested\n",
        "\n",
        "for keep_probability in [0.65]:\n",
        "    for num_layers in [4]:\n",
        "        for threshold in [0.90]:\n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
        "            total = 0\n",
        "            total = test(model, testing_sorted,log_string)\n",
        "\n",
        "            print(\"Total Accuracy %: {}\".format((ceil((total / 3) * 100) * 100) / 100.0))\n",
        "            print(\"Total Accuracy Exact: {}\".format(total / 3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Fixing Custom Sentences\n",
        "\n",
        "def text_to_ints(text):\n",
        "\n",
        "    '''Prepare the text for the model'''\n",
        "\n",
        "    return [vocab_to_int[word] for word in text]\n",
        "\n",
        "\n",
        "\n",
        "# Create your own sentence or use one from the dataset\n",
        "\n",
        "text = \"spellin is difficult whch is wyh you need to study everyday\"\n",
        "\n",
        "text = text_to_ints(text)\n",
        "\n",
        "checkpoint = checkpoint = checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Load saved model\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "\n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "# Remove the padding from the generated sentence\n",
        "\n",
        "eos = vocab_to_int[\"<EOS>\"] \n",
        "print('\\nText'),\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != eos]))\n",
        "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != eos])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary contains 31 characters.\n",
            "['\\n', ' ', '<EOS>', '<GO>', '<PAD>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "dict_items([(0, 't'), (1, 'h'), (2, 'i'), (3, 's'), (4, ' '), (5, 'w'), (6, 'a'), (7, 'y'), (8, 'o'), (9, 'u'), (10, 'c'), (11, 'v'), (12, 'e'), (13, 'r'), (14, 'b'), (15, 'p'), (16, 'f'), (17, 'l'), (18, 'd'), (19, 'n'), (20, '\\n'), (21, 'j'), (22, 'm'), (23, 'k'), (24, 'g'), (25, 'x'), (26, 'z'), (27, 'q'), (28, '<PAD>'), (29, '<EOS>'), (30, '<GO>')])\n",
            " Dataset contains 1232681 sentences.\n",
            "We will use 1017165 to train and test our model.\n",
            "Number of Training sentences: 915448\n",
            "Number of Validiation sentences: 71202\n",
            "Number of Testing sentences: 30515\n",
            "[6, 4, 5, 6, 13, 19, 2, 19, 24, 4, 5, 6, 3, 4, 24, 2, 11, 12, 19, 4, 14, 7, 4, 2, 3, 6, 6, 10, 4, 22]\n",
            "[6, 4, 5, 6, 13, 19, 19, 2, 24, 4, 5, 6, 3, 4, 24, 2, 11, 12, 19, 4, 14, 7, 4, 2, 3, 6, 6, 10, 4, 22]\n",
            "\n",
            "[6, 4, 16, 12, 5, 4, 15, 12, 0, 13, 8, 17, 4, 14, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 1, 13, 8, 5, 19]\n",
            "[6, 4, 16, 12, 5, 4, 15, 12, 0, 8, 13, 17, 4, 14, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 1, 13, 8, 5, 19]\n",
            "\n",
            "[0, 1, 12, 4, 13, 12, 3, 0, 4, 8, 16, 4, 9, 3, 4, 19, 12, 12, 18, 4, 12, 6, 10, 1, 4, 8, 0, 1, 12, 13]\n",
            "[0, 1, 12, 4, 13, 12, 3, 0, 8, 4, 16, 4, 9, 15, 3, 4, 19, 12, 12, 18, 4, 12, 6, 10, 1, 4, 8, 0, 1]\n",
            "\n",
            "[3, 8, 22, 12, 4, 3, 0, 9, 22, 14, 17, 12, 4, 8, 11, 12, 13, 4, 6, 4, 16, 6, 17, 3, 12, 4, 1, 8, 15, 12]\n",
            "[3, 8, 22, 12, 4, 3, 0, 9, 22, 14, 12, 17, 4, 11, 8, 12, 13, 4, 6, 4, 16, 6, 17, 12, 4, 1, 8, 15, 12]\n",
            "\n",
            "[3, 2, 19, 10, 12, 4, 0, 1, 12, 4, 22, 6, 13, 23, 12, 0, 4, 5, 2, 17, 17, 4, 14, 12, 4, 24, 2, 11, 12, 19]\n",
            "[3, 2, 19, 10, 12, 4, 1, 12, 4, 22, 6, 13, 23, 12, 0, 4, 5, 2, 17, 17, 4, 14, 12, 4, 2, 24, 11, 12, 19]\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-d6c309ecfe49>:234: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-d6c309ecfe49>:240: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Tensor(\"max_target_len:0\", shape=(), dtype=int32)\n",
            "\n",
            "Training Model: 1\n",
            "Epoch   1/100 Batch   30/14303 - Loss:  2.715, Seconds: 8.10\n",
            "Epoch   1/100 Batch   60/14303 - Loss:  1.982, Seconds: 5.13\n",
            "Epoch   1/100 Batch   90/14303 - Loss:  1.425, Seconds: 5.21\n",
            "Epoch   1/100 Batch  120/14303 - Loss:  0.941, Seconds: 5.18\n",
            "Epoch   1/100 Batch  150/14303 - Loss:  0.652, Seconds: 5.24\n",
            "Epoch   1/100 Batch  180/14303 - Loss:  0.504, Seconds: 5.20\n",
            "Epoch   1/100 Batch  210/14303 - Loss:  0.435, Seconds: 5.18\n",
            "Epoch   1/100 Batch  240/14303 - Loss:  0.394, Seconds: 5.19\n",
            "Epoch   1/100 Batch  270/14303 - Loss:  0.374, Seconds: 5.21\n",
            "Epoch   1/100 Batch  300/14303 - Loss:  0.385, Seconds: 5.22\n",
            "Epoch   1/100 Batch  330/14303 - Loss:  0.358, Seconds: 5.35\n",
            "Epoch   1/100 Batch  360/14303 - Loss:  0.340, Seconds: 5.51\n",
            "Epoch   1/100 Batch  390/14303 - Loss:  0.329, Seconds: 5.44\n",
            "Epoch   1/100 Batch  420/14303 - Loss:  0.331, Seconds: 5.45\n",
            "Epoch   1/100 Batch  450/14303 - Loss:  0.321, Seconds: 5.36\n",
            "Epoch   1/100 Batch  480/14303 - Loss:  0.342, Seconds: 5.38\n",
            "Epoch   1/100 Batch  510/14303 - Loss:  0.315, Seconds: 5.35\n",
            "Epoch   1/100 Batch  540/14303 - Loss:  0.308, Seconds: 5.40\n",
            "Epoch   1/100 Batch  570/14303 - Loss:  0.301, Seconds: 5.33\n",
            "Epoch   1/100 Batch  600/14303 - Loss:  0.293, Seconds: 5.34\n",
            "Epoch   1/100 Batch  630/14303 - Loss:  0.296, Seconds: 5.48\n",
            "Epoch   1/100 Batch  660/14303 - Loss:  0.297, Seconds: 5.55\n",
            "Epoch   1/100 Batch  690/14303 - Loss:  0.281, Seconds: 5.47\n",
            "Epoch   1/100 Batch  720/14303 - Loss:  0.278, Seconds: 5.47\n",
            "Epoch   1/100 Batch  750/14303 - Loss:  0.267, Seconds: 5.58\n",
            "Epoch   1/100 Batch  780/14303 - Loss:  0.264, Seconds: 5.56\n",
            "Epoch   1/100 Batch  810/14303 - Loss:  0.280, Seconds: 5.58\n",
            "Epoch   1/100 Batch  840/14303 - Loss:  0.266, Seconds: 5.67\n",
            "Epoch   1/100 Batch  870/14303 - Loss:  0.265, Seconds: 5.60\n",
            "Epoch   1/100 Batch  900/14303 - Loss:  0.257, Seconds: 5.51\n",
            "Epoch   1/100 Batch  930/14303 - Loss:  0.258, Seconds: 5.65\n",
            "Epoch   1/100 Batch  960/14303 - Loss:  0.248, Seconds: 5.65\n",
            "Epoch   1/100 Batch  990/14303 - Loss:  0.266, Seconds: 5.69\n",
            "Epoch   1/100 Batch 1020/14303 - Loss:  0.260, Seconds: 5.68\n",
            "Epoch   1/100 Batch 1050/14303 - Loss:  0.254, Seconds: 5.70\n",
            "Epoch   1/100 Batch 1080/14303 - Loss:  0.243, Seconds: 5.73\n",
            "Epoch   1/100 Batch 1110/14303 - Loss:  0.245, Seconds: 5.64\n",
            "Epoch   1/100 Batch 1140/14303 - Loss:  0.234, Seconds: 5.64\n",
            "Epoch   1/100 Batch 1170/14303 - Loss:  0.239, Seconds: 5.68\n",
            "Epoch   1/100 Batch 1200/14303 - Loss:  0.238, Seconds: 5.77\n",
            "Epoch   1/100 Batch 1230/14303 - Loss:  0.237, Seconds: 5.72\n",
            "Epoch   1/100 Batch 1260/14303 - Loss:  0.235, Seconds: 5.82\n",
            "Epoch   1/100 Batch 1290/14303 - Loss:  0.245, Seconds: 5.83\n",
            "Epoch   1/100 Batch 1320/14303 - Loss:  0.233, Seconds: 5.83\n",
            "Epoch   1/100 Batch 1350/14303 - Loss:  0.231, Seconds: 5.89\n",
            "Epoch   1/100 Batch 1380/14303 - Loss:  0.228, Seconds: 5.84\n",
            "Epoch   1/100 Batch 1410/14303 - Loss:  0.228, Seconds: 5.85\n",
            "Epoch   1/100 Batch 1440/14303 - Loss:  0.222, Seconds: 5.86\n",
            "Epoch   1/100 Batch 1470/14303 - Loss:  0.226, Seconds: 5.87\n",
            "Epoch   1/100 Batch 1500/14303 - Loss:  0.220, Seconds: 5.84\n",
            "Epoch   1/100 Batch 1530/14303 - Loss:  0.224, Seconds: 5.90\n",
            "Epoch   1/100 Batch 1560/14303 - Loss:  0.243, Seconds: 5.97\n",
            "Epoch   1/100 Batch 1590/14303 - Loss:  0.241, Seconds: 6.12\n",
            "Epoch   1/100 Batch 1620/14303 - Loss:  0.221, Seconds: 6.10\n",
            "Epoch   1/100 Batch 1650/14303 - Loss:  0.209, Seconds: 6.09\n",
            "Epoch   1/100 Batch 1680/14303 - Loss:  0.222, Seconds: 6.12\n",
            "Epoch   1/100 Batch 1710/14303 - Loss:  0.216, Seconds: 6.09\n",
            "Epoch   1/100 Batch 1740/14303 - Loss:  0.218, Seconds: 6.05\n",
            "Epoch   1/100 Batch 1770/14303 - Loss:  0.205, Seconds: 6.12\n",
            "Epoch   1/100 Batch 1800/14303 - Loss:  0.209, Seconds: 6.10\n",
            "Epoch   1/100 Batch 1830/14303 - Loss:  0.208, Seconds: 6.00\n",
            "Epoch   1/100 Batch 1860/14303 - Loss:  0.202, Seconds: 6.21\n",
            "Epoch   1/100 Batch 1890/14303 - Loss:  0.203, Seconds: 6.26\n",
            "Epoch   1/100 Batch 1920/14303 - Loss:  0.204, Seconds: 6.36\n",
            "Epoch   1/100 Batch 1950/14303 - Loss:  0.208, Seconds: 6.32\n",
            "Epoch   1/100 Batch 1980/14303 - Loss:  0.206, Seconds: 6.36\n",
            "Epoch   1/100 Batch 2010/14303 - Loss:  0.200, Seconds: 6.29\n",
            "Epoch   1/100 Batch 2040/14303 - Loss:  0.200, Seconds: 6.29\n",
            "Epoch   1/100 Batch 2070/14303 - Loss:  0.196, Seconds: 6.22\n",
            "Epoch   1/100 Batch 2100/14303 - Loss:  0.198, Seconds: 6.36\n",
            "Epoch   1/100 Batch 2130/14303 - Loss:  0.195, Seconds: 6.29\n",
            "Epoch   1/100 Batch 2160/14303 - Loss:  0.200, Seconds: 6.35\n",
            "Epoch   1/100 Batch 2190/14303 - Loss:  0.202, Seconds: 6.38\n",
            "Epoch   1/100 Batch 2220/14303 - Loss:  0.244, Seconds: 6.47\n",
            "Epoch   1/100 Batch 2250/14303 - Loss:  0.204, Seconds: 6.49\n",
            "Epoch   1/100 Batch 2280/14303 - Loss:  0.196, Seconds: 6.58\n",
            "Epoch   1/100 Batch 2310/14303 - Loss:  0.194, Seconds: 6.63\n",
            "Epoch   1/100 Batch 2340/14303 - Loss:  0.196, Seconds: 6.49\n",
            "Epoch   1/100 Batch 2370/14303 - Loss:  0.191, Seconds: 6.51\n",
            "Epoch   1/100 Batch 2400/14303 - Loss:  0.193, Seconds: 6.43\n",
            "Epoch   1/100 Batch 2430/14303 - Loss:  0.188, Seconds: 6.37\n",
            "Epoch   1/100 Batch 2460/14303 - Loss:  0.190, Seconds: 6.46\n",
            "Epoch   1/100 Batch 2490/14303 - Loss:  0.190, Seconds: 6.38\n",
            "Epoch   1/100 Batch 2520/14303 - Loss:  0.184, Seconds: 6.53\n",
            "Epoch   1/100 Batch 2550/14303 - Loss:  0.195, Seconds: 6.56\n",
            "Epoch   1/100 Batch 2580/14303 - Loss:  0.185, Seconds: 6.60\n",
            "Epoch   1/100 Batch 2610/14303 - Loss:  0.186, Seconds: 6.63\n",
            "Epoch   1/100 Batch 2640/14303 - Loss:  0.177, Seconds: 6.63\n",
            "Epoch   1/100 Batch 2670/14303 - Loss:  0.183, Seconds: 6.60\n",
            "Epoch   1/100 Batch 2700/14303 - Loss:  0.186, Seconds: 6.60\n",
            "Epoch   1/100 Batch 2730/14303 - Loss:  0.184, Seconds: 6.58\n",
            "Epoch   1/100 Batch 2760/14303 - Loss:  0.184, Seconds: 6.53\n",
            "Epoch   1/100 Batch 2790/14303 - Loss:  0.181, Seconds: 6.59\n",
            "Epoch   1/100 Batch 2820/14303 - Loss:  0.182, Seconds: 6.60\n",
            "Epoch   1/100 Batch 2850/14303 - Loss:  0.180, Seconds: 6.75\n",
            "Epoch   1/100 Batch 2880/14303 - Loss:  0.188, Seconds: 6.80\n",
            "Epoch   1/100 Batch 2910/14303 - Loss:  0.174, Seconds: 6.69\n",
            "Epoch   1/100 Batch 2940/14303 - Loss:  0.172, Seconds: 6.73\n",
            "Epoch   1/100 Batch 2970/14303 - Loss:  0.187, Seconds: 6.81\n",
            "Epoch   1/100 Batch 3000/14303 - Loss:  0.179, Seconds: 6.82\n",
            "Epoch   1/100 Batch 3030/14303 - Loss:  0.179, Seconds: 6.76\n",
            "Epoch   1/100 Batch 3060/14303 - Loss:  0.177, Seconds: 6.74\n",
            "Epoch   1/100 Batch 3090/14303 - Loss:  0.173, Seconds: 6.75\n",
            "Epoch   1/100 Batch 3120/14303 - Loss:  0.172, Seconds: 6.73\n",
            "Epoch   1/100 Batch 3150/14303 - Loss:  0.169, Seconds: 6.79\n",
            "Epoch   1/100 Batch 3180/14303 - Loss:  0.180, Seconds: 6.97\n",
            "Epoch   1/100 Batch 3210/14303 - Loss:  0.178, Seconds: 6.95\n",
            "Epoch   1/100 Batch 3240/14303 - Loss:  0.171, Seconds: 6.86\n",
            "Epoch   1/100 Batch 3270/14303 - Loss:  0.170, Seconds: 6.89\n",
            "Epoch   1/100 Batch 3300/14303 - Loss:  0.174, Seconds: 6.92\n",
            "Epoch   1/100 Batch 3330/14303 - Loss:  0.167, Seconds: 6.98\n",
            "Epoch   1/100 Batch 3360/14303 - Loss:  0.181, Seconds: 6.92\n",
            "Epoch   1/100 Batch 3390/14303 - Loss:  0.173, Seconds: 6.91\n",
            "Epoch   1/100 Batch 3420/14303 - Loss:  0.168, Seconds: 6.86\n",
            "Epoch   1/100 Batch 3450/14303 - Loss:  0.170, Seconds: 6.89\n",
            "Epoch   1/100 Batch 3480/14303 - Loss:  0.168, Seconds: 6.97\n",
            "Epoch   1/100 Batch 3510/14303 - Loss:  0.167, Seconds: 7.05\n",
            "Epoch   1/100 Batch 3540/14303 - Loss:  0.165, Seconds: 7.06\n",
            "Epoch   1/100 Batch 3570/14303 - Loss:  0.171, Seconds: 7.15\n",
            "Epoch   1/100 Batch 3600/14303 - Loss:  0.167, Seconds: 7.29\n",
            "Epoch   1/100 Batch 3630/14303 - Loss:  0.172, Seconds: 7.08\n",
            "Epoch   1/100 Batch 3660/14303 - Loss:  0.164, Seconds: 7.10\n",
            "Epoch   1/100 Batch 3690/14303 - Loss:  0.168, Seconds: 7.06\n",
            "Epoch   1/100 Batch 3720/14303 - Loss:  0.167, Seconds: 7.02\n",
            "Epoch   1/100 Batch 3750/14303 - Loss:  0.161, Seconds: 7.01\n",
            "Epoch   1/100 Batch 3780/14303 - Loss:  0.163, Seconds: 7.14\n",
            "Epoch   1/100 Batch 3810/14303 - Loss:  0.168, Seconds: 7.12\n",
            "Epoch   1/100 Batch 3840/14303 - Loss:  0.161, Seconds: 7.25\n",
            "Epoch   1/100 Batch 3870/14303 - Loss:  0.162, Seconds: 7.18\n",
            "Epoch   1/100 Batch 3900/14303 - Loss:  0.161, Seconds: 7.17\n",
            "Epoch   1/100 Batch 3930/14303 - Loss:  0.167, Seconds: 7.19\n",
            "Epoch   1/100 Batch 3960/14303 - Loss:  0.164, Seconds: 7.26\n",
            "Epoch   1/100 Batch 3990/14303 - Loss:  0.160, Seconds: 7.17\n",
            "Epoch   1/100 Batch 4020/14303 - Loss:  0.164, Seconds: 7.22\n",
            "Epoch   1/100 Batch 4050/14303 - Loss:  0.162, Seconds: 7.22\n",
            "Epoch   1/100 Batch 4080/14303 - Loss:  0.164, Seconds: 7.21\n",
            "Epoch   1/100 Batch 4110/14303 - Loss:  0.159, Seconds: 7.14\n",
            "Epoch   1/100 Batch 4140/14303 - Loss:  0.166, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4170/14303 - Loss:  0.155, Seconds: 7.34\n",
            "Epoch   1/100 Batch 4200/14303 - Loss:  0.154, Seconds: 7.40\n",
            "Epoch   1/100 Batch 4230/14303 - Loss:  0.157, Seconds: 7.30\n",
            "Epoch   1/100 Batch 4260/14303 - Loss:  0.166, Seconds: 7.41\n",
            "Epoch   1/100 Batch 4290/14303 - Loss:  0.157, Seconds: 7.43\n",
            "Epoch   1/100 Batch 4320/14303 - Loss:  0.158, Seconds: 7.30\n",
            "Epoch   1/100 Batch 4350/14303 - Loss:  0.160, Seconds: 7.38\n",
            "Epoch   1/100 Batch 4380/14303 - Loss:  0.160, Seconds: 7.41\n",
            "Epoch   1/100 Batch 4410/14303 - Loss:  0.161, Seconds: 7.36\n",
            "Epoch   1/100 Batch 4440/14303 - Loss:  0.155, Seconds: 7.48\n",
            "Epoch   1/100 Batch 4470/14303 - Loss:  0.158, Seconds: 7.50\n",
            "Epoch   1/100 Batch 4500/14303 - Loss:  0.162, Seconds: 7.58\n",
            "Epoch   1/100 Batch 4530/14303 - Loss:  0.158, Seconds: 7.50\n",
            "Epoch   1/100 Batch 4560/14303 - Loss:  0.161, Seconds: 7.52\n",
            "Epoch   1/100 Batch 4590/14303 - Loss:  0.159, Seconds: 7.61\n",
            "Epoch   1/100 Batch 4620/14303 - Loss:  0.153, Seconds: 7.51\n",
            "Epoch   1/100 Batch 4650/14303 - Loss:  0.157, Seconds: 7.52\n",
            "Epoch   1/100 Batch 4680/14303 - Loss:  0.151, Seconds: 7.54\n",
            "Epoch   1/100 Batch 4710/14303 - Loss:  0.151, Seconds: 7.47\n",
            "Epoch   1/100 Batch 4740/14303 - Loss:  0.158, Seconds: 7.57\n",
            "Epoch   1/100 Batch 4770/14303 - Loss:  0.159, Seconds: 7.79\n",
            "Epoch   1/100 Batch 4800/14303 - Loss:  0.151, Seconds: 7.87\n",
            "Epoch   1/100 Batch 4830/14303 - Loss:  0.146, Seconds: 7.68\n",
            "Epoch   1/100 Batch 4860/14303 - Loss:  0.156, Seconds: 7.73\n",
            "Epoch   1/100 Batch 4890/14303 - Loss:  0.147, Seconds: 7.79\n",
            "Epoch   1/100 Batch 4920/14303 - Loss:  0.146, Seconds: 7.69\n",
            "Epoch   1/100 Batch 4950/14303 - Loss:  0.149, Seconds: 7.71\n",
            "Epoch   1/100 Batch 4980/14303 - Loss:  0.145, Seconds: 7.76\n",
            "Epoch   1/100 Batch 5010/14303 - Loss:  0.145, Seconds: 7.74\n",
            "Epoch   1/100 Batch 5040/14303 - Loss:  0.153, Seconds: 7.78\n",
            "Epoch   1/100 Batch 5070/14303 - Loss:  0.162, Seconds: 8.01\n",
            "Epoch   1/100 Batch 5100/14303 - Loss:  0.504, Seconds: 7.88\n",
            "Epoch   1/100 Batch 5130/14303 - Loss:  0.222, Seconds: 7.90\n",
            "Epoch   1/100 Batch 5160/14303 - Loss:  0.183, Seconds: 7.93\n",
            "Epoch   1/100 Batch 5190/14303 - Loss:  0.173, Seconds: 7.91\n",
            "Epoch   1/100 Batch 5220/14303 - Loss:  0.160, Seconds: 7.83\n",
            "Epoch   1/100 Batch 5250/14303 - Loss:  0.160, Seconds: 7.82\n",
            "Epoch   1/100 Batch 5280/14303 - Loss:  0.166, Seconds: 7.89\n",
            "Epoch   1/100 Batch 5310/14303 - Loss:  0.157, Seconds: 7.85\n",
            "Epoch   1/100 Batch 5340/14303 - Loss:  0.158, Seconds: 7.82\n",
            "Epoch   1/100 Batch 5370/14303 - Loss:  0.160, Seconds: 8.01\n",
            "Epoch   1/100 Batch 5400/14303 - Loss:  0.157, Seconds: 7.99\n",
            "Epoch   1/100 Batch 5430/14303 - Loss:  0.159, Seconds: 8.02\n",
            "Epoch   1/100 Batch 5460/14303 - Loss:  0.152, Seconds: 8.15\n",
            "Epoch   1/100 Batch 5490/14303 - Loss:  0.151, Seconds: 8.08\n",
            "Epoch   1/100 Batch 5520/14303 - Loss:  0.151, Seconds: 8.03\n",
            "Epoch   1/100 Batch 5550/14303 - Loss:  0.149, Seconds: 8.05\n",
            "Epoch   1/100 Batch 5580/14303 - Loss:  0.151, Seconds: 7.97\n",
            "Epoch   1/100 Batch 5610/14303 - Loss:  0.146, Seconds: 8.04\n",
            "Epoch   1/100 Batch 5640/14303 - Loss:  0.146, Seconds: 8.04\n",
            "Epoch   1/100 Batch 5670/14303 - Loss:  0.147, Seconds: 8.15\n",
            "Epoch   1/100 Batch 5700/14303 - Loss:  0.148, Seconds: 8.26\n",
            "Epoch   1/100 Batch 5730/14303 - Loss:  0.144, Seconds: 8.21\n",
            "Epoch   1/100 Batch 5760/14303 - Loss:  0.142, Seconds: 8.31\n",
            "Epoch   1/100 Batch 5790/14303 - Loss:  0.140, Seconds: 8.16\n",
            "Epoch   1/100 Batch 5820/14303 - Loss:  0.147, Seconds: 8.25\n",
            "Epoch   1/100 Batch 5850/14303 - Loss:  0.141, Seconds: 8.19\n",
            "Epoch   1/100 Batch 5880/14303 - Loss:  0.141, Seconds: 8.44\n",
            "Epoch   1/100 Batch 5910/14303 - Loss:  0.148, Seconds: 8.27\n",
            "Epoch   1/100 Batch 5940/14303 - Loss:  0.140, Seconds: 8.19\n",
            "Epoch   1/100 Batch 5970/14303 - Loss:  0.142, Seconds: 8.36\n",
            "Epoch   1/100 Batch 6000/14303 - Loss:  0.146, Seconds: 8.41\n",
            "Epoch   1/100 Batch 6030/14303 - Loss:  0.146, Seconds: 8.43\n",
            "Epoch   1/100 Batch 6060/14303 - Loss:  0.139, Seconds: 8.36\n",
            "Epoch   1/100 Batch 6090/14303 - Loss:  0.141, Seconds: 8.38\n",
            "Epoch   1/100 Batch 6120/14303 - Loss:  0.139, Seconds: 8.42\n",
            "Epoch   1/100 Batch 6150/14303 - Loss:  0.138, Seconds: 8.39\n",
            "Epoch   1/100 Batch 6180/14303 - Loss:  0.141, Seconds: 8.37\n",
            "Epoch   1/100 Batch 6210/14303 - Loss:  0.140, Seconds: 8.41\n",
            "Epoch   1/100 Batch 6240/14303 - Loss:  0.141, Seconds: 8.51\n",
            "Epoch   1/100 Batch 6270/14303 - Loss:  0.142, Seconds: 8.55\n",
            "Epoch   1/100 Batch 6300/14303 - Loss:  0.141, Seconds: 8.60\n",
            "Epoch   1/100 Batch 6330/14303 - Loss:  0.136, Seconds: 8.57\n",
            "Epoch   1/100 Batch 6360/14303 - Loss:  0.136, Seconds: 8.58\n",
            "Epoch   1/100 Batch 6390/14303 - Loss:  0.137, Seconds: 8.48\n",
            "Epoch   1/100 Batch 6420/14303 - Loss:  0.140, Seconds: 8.46\n",
            "Epoch   1/100 Batch 6450/14303 - Loss:  0.140, Seconds: 8.45\n",
            "Epoch   1/100 Batch 6480/14303 - Loss:  0.141, Seconds: 8.54\n",
            "Epoch   1/100 Batch 6510/14303 - Loss:  0.137, Seconds: 8.64\n",
            "Epoch   1/100 Batch 6540/14303 - Loss:  0.131, Seconds: 8.79\n",
            "Epoch   1/100 Batch 6570/14303 - Loss:  0.134, Seconds: 8.73\n",
            "Epoch   1/100 Batch 6600/14303 - Loss:  0.136, Seconds: 8.70\n",
            "Epoch   1/100 Batch 6630/14303 - Loss:  0.140, Seconds: 8.76\n",
            "Epoch   1/100 Batch 6660/14303 - Loss:  0.135, Seconds: 8.69\n",
            "Epoch   1/100 Batch 6690/14303 - Loss:  0.133, Seconds: 8.71\n",
            "Epoch   1/100 Batch 6720/14303 - Loss:  0.136, Seconds: 8.72\n",
            "Epoch   1/100 Batch 6750/14303 - Loss:  0.135, Seconds: 8.82\n",
            "Epoch   1/100 Batch 6780/14303 - Loss:  0.134, Seconds: 8.81\n",
            "Epoch   1/100 Batch 6810/14303 - Loss:  0.130, Seconds: 8.95\n",
            "Epoch   1/100 Batch 6840/14303 - Loss:  0.132, Seconds: 9.01\n",
            "Epoch   1/100 Batch 6870/14303 - Loss:  0.135, Seconds: 8.91\n",
            "Epoch   1/100 Batch 6900/14303 - Loss:  0.133, Seconds: 9.17\n",
            "Epoch   1/100 Batch 6930/14303 - Loss:  0.132, Seconds: 8.83\n",
            "Epoch   1/100 Batch 6960/14303 - Loss:  0.136, Seconds: 8.85\n",
            "Epoch   1/100 Batch 6990/14303 - Loss:  0.133, Seconds: 8.92\n",
            "Epoch   1/100 Batch 7020/14303 - Loss:  0.135, Seconds: 8.89\n",
            "Epoch   1/100 Batch 7050/14303 - Loss:  0.134, Seconds: 8.89\n",
            "Epoch   1/100 Batch 7080/14303 - Loss:  0.132, Seconds: 8.99\n",
            "Epoch   1/100 Batch 7110/14303 - Loss:  0.132, Seconds: 9.08\n",
            "Epoch   1/100 Batch 7140/14303 - Loss:  0.136, Seconds: 9.05\n",
            "Epoch   1/100 Batch 7170/14303 - Loss:  0.132, Seconds: 9.04\n",
            "Epoch   1/100 Batch 7200/14303 - Loss:  0.129, Seconds: 9.13\n",
            "Epoch   1/100 Batch 7230/14303 - Loss:  0.135, Seconds: 9.06\n",
            "Epoch   1/100 Batch 7260/14303 - Loss:  0.126, Seconds: 9.00\n",
            "Epoch   1/100 Batch 7290/14303 - Loss:  0.126, Seconds: 9.03\n",
            "Epoch   1/100 Batch 7320/14303 - Loss:  0.132, Seconds: 9.10\n",
            "Epoch   1/100 Batch 7350/14303 - Loss:  0.132, Seconds: 9.19\n",
            "Epoch   1/100 Batch 7380/14303 - Loss:  0.127, Seconds: 9.22\n",
            "Epoch   1/100 Batch 7410/14303 - Loss:  0.136, Seconds: 9.25\n",
            "Epoch   1/100 Batch 7440/14303 - Loss:  0.132, Seconds: 9.20\n",
            "Epoch   1/100 Batch 7470/14303 - Loss:  0.129, Seconds: 9.30\n",
            "Epoch   1/100 Batch 7500/14303 - Loss:  0.128, Seconds: 9.17\n",
            "Epoch   1/100 Batch 7530/14303 - Loss:  0.124, Seconds: 9.21\n",
            "Epoch   1/100 Batch 7560/14303 - Loss:  0.130, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7590/14303 - Loss:  0.128, Seconds: 9.23\n",
            "Epoch   1/100 Batch 7620/14303 - Loss:  0.130, Seconds: 9.39\n",
            "Epoch   1/100 Batch 7650/14303 - Loss:  0.132, Seconds: 9.47\n",
            "Epoch   1/100 Batch 7680/14303 - Loss:  0.133, Seconds: 9.41\n",
            "Epoch   1/100 Batch 7710/14303 - Loss:  0.126, Seconds: 9.44\n",
            "Epoch   1/100 Batch 7740/14303 - Loss:  0.129, Seconds: 9.46\n",
            "Epoch   1/100 Batch 7770/14303 - Loss:  0.129, Seconds: 9.37\n",
            "Epoch   1/100 Batch 7800/14303 - Loss:  0.127, Seconds: 9.41\n",
            "Epoch   1/100 Batch 7830/14303 - Loss:  0.125, Seconds: 9.58\n",
            "Epoch   1/100 Batch 7860/14303 - Loss:  0.131, Seconds: 9.58\n",
            "Epoch   1/100 Batch 7890/14303 - Loss:  0.130, Seconds: 9.50\n",
            "Epoch   1/100 Batch 7920/14303 - Loss:  0.123, Seconds: 9.55\n",
            "Epoch   1/100 Batch 7950/14303 - Loss:  0.139, Seconds: 9.56\n",
            "Epoch   1/100 Batch 7980/14303 - Loss:  0.133, Seconds: 9.53\n",
            "Epoch   1/100 Batch 8010/14303 - Loss:  0.127, Seconds: 9.53\n",
            "Epoch   1/100 Batch 8040/14303 - Loss:  0.128, Seconds: 9.62\n",
            "Epoch   1/100 Batch 8070/14303 - Loss:  0.120, Seconds: 9.53\n",
            "Epoch   1/100 Batch 8100/14303 - Loss:  0.126, Seconds: 9.53\n",
            "Epoch   1/100 Batch 8130/14303 - Loss:  0.129, Seconds: 9.72\n",
            "Epoch   1/100 Batch 8160/14303 - Loss:  0.123, Seconds: 9.76\n",
            "Epoch   1/100 Batch 8190/14303 - Loss:  0.122, Seconds: 9.74\n",
            "Epoch   1/100 Batch 8220/14303 - Loss:  0.125, Seconds: 9.74\n",
            "Epoch   1/100 Batch 8250/14303 - Loss:  0.125, Seconds: 9.74\n",
            "Epoch   1/100 Batch 8280/14303 - Loss:  0.127, Seconds: 9.80\n",
            "Epoch   1/100 Batch 8310/14303 - Loss:  0.123, Seconds: 9.66\n",
            "Epoch   1/100 Batch 8340/14303 - Loss:  0.123, Seconds: 9.71\n",
            "Epoch   1/100 Batch 8370/14303 - Loss:  0.129, Seconds: 9.94\n",
            "Epoch   1/100 Batch 8400/14303 - Loss:  0.128, Seconds: 9.90\n",
            "Epoch   1/100 Batch 8430/14303 - Loss:  0.130, Seconds: 9.83\n",
            "Epoch   1/100 Batch 8460/14303 - Loss:  0.125, Seconds: 9.91\n",
            "Epoch   1/100 Batch 8490/14303 - Loss:  0.125, Seconds: 10.01\n",
            "Epoch   1/100 Batch 8520/14303 - Loss:  0.124, Seconds: 10.00\n",
            "Epoch   1/100 Batch 8550/14303 - Loss:  0.122, Seconds: 9.99\n",
            "Epoch   1/100 Batch 8580/14303 - Loss:  0.124, Seconds: 9.90\n",
            "Epoch   1/100 Batch 8610/14303 - Loss:  0.120, Seconds: 10.14\n",
            "Epoch   1/100 Batch 8640/14303 - Loss:  0.125, Seconds: 10.06\n",
            "Epoch   1/100 Batch 8670/14303 - Loss:  0.123, Seconds: 10.11\n",
            "Epoch   1/100 Batch 8700/14303 - Loss:  0.125, Seconds: 10.01\n",
            "Epoch   1/100 Batch 8730/14303 - Loss:  0.124, Seconds: 10.29\n",
            "Epoch   1/100 Batch 8760/14303 - Loss:  0.122, Seconds: 10.20\n",
            "Epoch   1/100 Batch 8790/14303 - Loss:  0.120, Seconds: 10.04\n",
            "Epoch   1/100 Batch 8820/14303 - Loss:  0.127, Seconds: 10.13\n",
            "Epoch   1/100 Batch 8850/14303 - Loss:  0.123, Seconds: 10.26\n",
            "Epoch   1/100 Batch 8880/14303 - Loss:  0.124, Seconds: 10.20\n",
            "Epoch   1/100 Batch 8910/14303 - Loss:  0.119, Seconds: 10.24\n",
            "Epoch   1/100 Batch 8940/14303 - Loss:  0.120, Seconds: 10.34\n",
            "Epoch   1/100 Batch 8970/14303 - Loss:  0.118, Seconds: 10.24\n",
            "Epoch   1/100 Batch 9000/14303 - Loss:  0.123, Seconds: 10.22\n",
            "Epoch   1/100 Batch 9030/14303 - Loss:  0.123, Seconds: 10.26\n",
            "Epoch   1/100 Batch 9060/14303 - Loss:  0.123, Seconds: 10.51\n",
            "Epoch   1/100 Batch 9090/14303 - Loss:  0.126, Seconds: 10.45\n",
            "Epoch   1/100 Batch 9120/14303 - Loss:  0.122, Seconds: 10.49\n",
            "Epoch   1/100 Batch 9150/14303 - Loss:  0.122, Seconds: 10.41\n",
            "Epoch   1/100 Batch 9180/14303 - Loss:  0.119, Seconds: 10.50\n",
            "Epoch   1/100 Batch 9210/14303 - Loss:  0.118, Seconds: 10.52\n",
            "Epoch   1/100 Batch 9240/14303 - Loss:  0.122, Seconds: 10.47\n",
            "Epoch   1/100 Batch 9270/14303 - Loss:  0.122, Seconds: 10.57\n",
            "Epoch   1/100 Batch 9300/14303 - Loss:  0.118, Seconds: 10.59\n",
            "Epoch   1/100 Batch 9330/14303 - Loss:  0.117, Seconds: 10.63\n",
            "Epoch   1/100 Batch 9360/14303 - Loss:  0.119, Seconds: 10.59\n",
            "Epoch   1/100 Batch 9390/14303 - Loss:  0.123, Seconds: 10.73\n",
            "Epoch   1/100 Batch 9420/14303 - Loss:  0.120, Seconds: 10.71\n",
            "Epoch   1/100 Batch 9450/14303 - Loss:  0.119, Seconds: 10.65\n",
            "Epoch   1/100 Batch 9480/14303 - Loss:  0.120, Seconds: 10.73\n",
            "Epoch   1/100 Batch 9510/14303 - Loss:  0.121, Seconds: 10.71\n",
            "Epoch   1/100 Batch 9540/14303 - Loss:  0.116, Seconds: 10.74\n",
            "Epoch   1/100 Batch 9570/14303 - Loss:  0.117, Seconds: 10.90\n",
            "Epoch   1/100 Batch 9600/14303 - Loss:  0.120, Seconds: 11.06\n",
            "Epoch   1/100 Batch 9630/14303 - Loss:  0.121, Seconds: 10.86\n",
            "Epoch   1/100 Batch 9660/14303 - Loss:  0.118, Seconds: 10.84\n",
            "Epoch   1/100 Batch 9690/14303 - Loss:  0.122, Seconds: 10.91\n",
            "Epoch   1/100 Batch 9720/14303 - Loss:  0.118, Seconds: 10.93\n",
            "Epoch   1/100 Batch 9750/14303 - Loss:  0.119, Seconds: 11.02\n",
            "Epoch   1/100 Batch 9780/14303 - Loss:  0.116, Seconds: 10.86\n",
            "Epoch   1/100 Batch 9810/14303 - Loss:  0.120, Seconds: 11.03\n",
            "Epoch   1/100 Batch 9840/14303 - Loss:  0.117, Seconds: 10.92\n",
            "Epoch   1/100 Batch 9870/14303 - Loss:  0.121, Seconds: 11.01\n",
            "Epoch   1/100 Batch 9900/14303 - Loss:  0.117, Seconds: 11.14\n",
            "Epoch   1/100 Batch 9930/14303 - Loss:  0.124, Seconds: 11.11\n",
            "Epoch   1/100 Batch 9960/14303 - Loss:  0.116, Seconds: 11.08\n",
            "Epoch   1/100 Batch 9990/14303 - Loss:  0.116, Seconds: 11.12\n",
            "Epoch   1/100 Batch 10020/14303 - Loss:  0.121, Seconds: 11.25\n",
            "Epoch   1/100 Batch 10050/14303 - Loss:  0.116, Seconds: 11.12\n",
            "Epoch   1/100 Batch 10080/14303 - Loss:  0.115, Seconds: 11.30\n",
            "Epoch   1/100 Batch 10110/14303 - Loss:  0.119, Seconds: 11.30\n",
            "Epoch   1/100 Batch 10140/14303 - Loss:  0.117, Seconds: 11.28\n",
            "Epoch   1/100 Batch 10170/14303 - Loss:  0.114, Seconds: 11.30\n",
            "Epoch   1/100 Batch 10200/14303 - Loss:  0.115, Seconds: 11.32\n",
            "Epoch   1/100 Batch 10230/14303 - Loss:  0.118, Seconds: 11.28\n",
            "Epoch   1/100 Batch 10260/14303 - Loss:  0.116, Seconds: 11.45\n",
            "Epoch   1/100 Batch 10290/14303 - Loss:  0.119, Seconds: 11.43\n",
            "Epoch   1/100 Batch 10320/14303 - Loss:  0.120, Seconds: 11.45\n",
            "Epoch   1/100 Batch 10350/14303 - Loss:  0.116, Seconds: 11.62\n",
            "Epoch   1/100 Batch 10380/14303 - Loss:  0.112, Seconds: 11.59\n",
            "Epoch   1/100 Batch 10410/14303 - Loss:  0.117, Seconds: 11.57\n",
            "Epoch   1/100 Batch 10440/14303 - Loss:  0.115, Seconds: 11.71\n",
            "Epoch   1/100 Batch 10470/14303 - Loss:  0.113, Seconds: 11.61\n",
            "Epoch   1/100 Batch 10500/14303 - Loss:  0.114, Seconds: 11.66\n",
            "Epoch   1/100 Batch 10530/14303 - Loss:  0.114, Seconds: 11.67\n",
            "Epoch   1/100 Batch 10560/14303 - Loss:  0.114, Seconds: 11.58\n",
            "Epoch   1/100 Batch 10590/14303 - Loss:  0.116, Seconds: 11.79\n",
            "Epoch   1/100 Batch 10620/14303 - Loss:  0.111, Seconds: 11.83\n",
            "Epoch   1/100 Batch 10650/14303 - Loss:  0.117, Seconds: 11.91\n",
            "Epoch   1/100 Batch 10680/14303 - Loss:  0.111, Seconds: 11.83\n",
            "Epoch   1/100 Batch 10710/14303 - Loss:  0.114, Seconds: 11.91\n",
            "Epoch   1/100 Batch 10740/14303 - Loss:  0.111, Seconds: 11.85\n",
            "Epoch   1/100 Batch 10770/14303 - Loss:  0.114, Seconds: 12.08\n",
            "Epoch   1/100 Batch 10800/14303 - Loss:  0.115, Seconds: 12.19\n",
            "Epoch   1/100 Batch 10830/14303 - Loss:  0.109, Seconds: 12.03\n",
            "Epoch   1/100 Batch 10860/14303 - Loss:  0.110, Seconds: 11.97\n",
            "Epoch   1/100 Batch 10890/14303 - Loss:  0.111, Seconds: 12.00\n",
            "Epoch   1/100 Batch 10920/14303 - Loss:  0.113, Seconds: 12.16\n",
            "Epoch   1/100 Batch 10950/14303 - Loss:  0.111, Seconds: 12.16\n",
            "Epoch   1/100 Batch 10980/14303 - Loss:  0.113, Seconds: 12.21\n",
            "Epoch   1/100 Batch 11010/14303 - Loss:  0.114, Seconds: 12.17\n",
            "Epoch   1/100 Batch 11040/14303 - Loss:  0.110, Seconds: 12.08\n",
            "Epoch   1/100 Batch 11070/14303 - Loss:  0.112, Seconds: 12.26\n",
            "Epoch   1/100 Batch 11100/14303 - Loss:  0.115, Seconds: 12.55\n",
            "Epoch   1/100 Batch 11130/14303 - Loss:  0.111, Seconds: 12.30\n",
            "Epoch   1/100 Batch 11160/14303 - Loss:  0.117, Seconds: 12.39\n",
            "Epoch   1/100 Batch 11190/14303 - Loss:  0.110, Seconds: 12.41\n",
            "Epoch   1/100 Batch 11220/14303 - Loss:  0.109, Seconds: 12.55\n",
            "Epoch   1/100 Batch 11250/14303 - Loss:  0.113, Seconds: 12.41\n",
            "Epoch   1/100 Batch 11280/14303 - Loss:  0.108, Seconds: 12.41\n",
            "Epoch   1/100 Batch 11310/14303 - Loss:  0.114, Seconds: 12.43\n",
            "Epoch   1/100 Batch 11340/14303 - Loss:  0.110, Seconds: 12.57\n",
            "Epoch   1/100 Batch 11370/14303 - Loss:  0.109, Seconds: 12.65\n",
            "Epoch   1/100 Batch 11400/14303 - Loss:  0.110, Seconds: 12.60\n",
            "Epoch   1/100 Batch 11430/14303 - Loss:  0.110, Seconds: 12.60\n",
            "Epoch   1/100 Batch 11460/14303 - Loss:  0.106, Seconds: 12.70\n",
            "Epoch   1/100 Batch 11490/14303 - Loss:  0.113, Seconds: 12.78\n",
            "Epoch   1/100 Batch 11520/14303 - Loss:  0.110, Seconds: 12.86\n",
            "Epoch   1/100 Batch 11550/14303 - Loss:  0.113, Seconds: 12.87\n",
            "Epoch   1/100 Batch 11580/14303 - Loss:  0.110, Seconds: 12.91\n",
            "Epoch   1/100 Batch 11610/14303 - Loss:  0.109, Seconds: 13.00\n",
            "Epoch   1/100 Batch 11640/14303 - Loss:  0.113, Seconds: 13.10\n",
            "Epoch   1/100 Batch 11670/14303 - Loss:  0.111, Seconds: 12.97\n",
            "Epoch   1/100 Batch 11700/14303 - Loss:  0.111, Seconds: 13.09\n",
            "Epoch   1/100 Batch 11730/14303 - Loss:  0.107, Seconds: 13.05\n",
            "Epoch   1/100 Batch 11760/14303 - Loss:  0.109, Seconds: 13.23\n",
            "Epoch   1/100 Batch 11790/14303 - Loss:  0.110, Seconds: 13.40\n",
            "Epoch   1/100 Batch 11820/14303 - Loss:  0.107, Seconds: 13.17\n",
            "Epoch   1/100 Batch 11850/14303 - Loss:  0.109, Seconds: 13.22\n",
            "Epoch   1/100 Batch 11880/14303 - Loss:  0.107, Seconds: 13.43\n",
            "Epoch   1/100 Batch 11910/14303 - Loss:  0.105, Seconds: 13.33\n",
            "Epoch   1/100 Batch 11970/14303 - Loss:  0.105, Seconds: 13.37\n",
            "Epoch   1/100 Batch 12000/14303 - Loss:  0.106, Seconds: 13.49\n",
            "Epoch   1/100 Batch 12030/14303 - Loss:  0.109, Seconds: 13.58\n",
            "Epoch   1/100 Batch 12060/14303 - Loss:  0.106, Seconds: 13.54\n",
            "Epoch   1/100 Batch 12090/14303 - Loss:  0.105, Seconds: 13.63\n",
            "Epoch   1/100 Batch 12120/14303 - Loss:  0.104, Seconds: 13.80\n",
            "Epoch   1/100 Batch 12150/14303 - Loss:  0.109, Seconds: 13.75\n",
            "Epoch   1/100 Batch 12180/14303 - Loss:  0.106, Seconds: 13.66\n",
            "Epoch   1/100 Batch 12210/14303 - Loss:  0.105, Seconds: 13.95\n",
            "Epoch   1/100 Batch 12240/14303 - Loss:  0.104, Seconds: 13.81\n",
            "Epoch   1/100 Batch 12270/14303 - Loss:  0.105, Seconds: 13.84\n",
            "Epoch   1/100 Batch 12300/14303 - Loss:  0.109, Seconds: 13.87\n",
            "Epoch   1/100 Batch 12330/14303 - Loss:  0.106, Seconds: 13.97\n",
            "Epoch   1/100 Batch 12360/14303 - Loss:  0.108, Seconds: 14.04\n",
            "Epoch   1/100 Batch 12390/14303 - Loss:  0.108, Seconds: 14.17\n",
            "Epoch   1/100 Batch 12420/14303 - Loss:  0.209, Seconds: 14.45\n",
            "Epoch   1/100 Batch 12450/14303 - Loss:  0.257, Seconds: 14.43\n",
            "Epoch   1/100 Batch 12480/14303 - Loss:  0.154, Seconds: 14.36\n",
            "Epoch   1/100 Batch 12510/14303 - Loss:  0.136, Seconds: 14.53\n",
            "Epoch   1/100 Batch 12540/14303 - Loss:  0.136, Seconds: 14.62\n",
            "Epoch   1/100 Batch 12570/14303 - Loss:  0.130, Seconds: 14.60\n",
            "Epoch   1/100 Batch 12600/14303 - Loss:  0.126, Seconds: 14.79\n",
            "Epoch   1/100 Batch 12630/14303 - Loss:  0.121, Seconds: 14.77\n",
            "Epoch   1/100 Batch 12660/14303 - Loss:  0.119, Seconds: 14.76\n",
            "Epoch   1/100 Batch 12690/14303 - Loss:  0.117, Seconds: 14.94\n",
            "Epoch   1/100 Batch 12720/14303 - Loss:  0.116, Seconds: 15.06\n",
            "Epoch   1/100 Batch 12750/14303 - Loss:  0.113, Seconds: 15.11\n",
            "Epoch   1/100 Batch 12780/14303 - Loss:  0.112, Seconds: 15.16\n",
            "Epoch   1/100 Batch 12810/14303 - Loss:  0.112, Seconds: 15.27\n",
            "Epoch   1/100 Batch 12840/14303 - Loss:  0.116, Seconds: 15.55\n",
            "Epoch   1/100 Batch 12870/14303 - Loss:  0.110, Seconds: 15.38\n",
            "Epoch   1/100 Batch 12900/14303 - Loss:  0.110, Seconds: 15.55\n",
            "Epoch   1/100 Batch 12930/14303 - Loss:  0.111, Seconds: 15.61\n",
            "Epoch   1/100 Batch 12960/14303 - Loss:  0.111, Seconds: 15.53\n",
            "Epoch   1/100 Batch 12990/14303 - Loss:  0.110, Seconds: 15.76\n",
            "Epoch   1/100 Batch 13020/14303 - Loss:  0.109, Seconds: 15.87\n",
            "Epoch   1/100 Batch 13050/14303 - Loss:  0.107, Seconds: 15.90\n",
            "Epoch   1/100 Batch 13080/14303 - Loss:  0.108, Seconds: 15.96\n",
            "Epoch   1/100 Batch 13110/14303 - Loss:  0.107, Seconds: 16.21\n",
            "Epoch   1/100 Batch 13140/14303 - Loss:  0.109, Seconds: 16.17\n",
            "Epoch   1/100 Batch 13170/14303 - Loss:  0.108, Seconds: 16.31\n",
            "Epoch   1/100 Batch 13200/14303 - Loss:  0.105, Seconds: 16.42\n",
            "Epoch   1/100 Batch 13230/14303 - Loss:  0.107, Seconds: 16.51\n",
            "Epoch   1/100 Batch 13260/14303 - Loss:  0.104, Seconds: 16.72\n",
            "Epoch   1/100 Batch 13290/14303 - Loss:  0.106, Seconds: 16.79\n",
            "Epoch   1/100 Batch 13320/14303 - Loss:  0.102, Seconds: 16.78\n",
            "Epoch   1/100 Batch 13350/14303 - Loss:  0.103, Seconds: 17.03\n",
            "Epoch   1/100 Batch 13380/14303 - Loss:  0.106, Seconds: 17.07\n",
            "Epoch   1/100 Batch 13410/14303 - Loss:  0.105, Seconds: 17.21\n",
            "Epoch   1/100 Batch 13440/14303 - Loss:  0.104, Seconds: 17.27\n",
            "Epoch   1/100 Batch 13470/14303 - Loss:  0.104, Seconds: 17.20\n",
            "Epoch   1/100 Batch 13500/14303 - Loss:  0.103, Seconds: 17.60\n",
            "Epoch   1/100 Batch 13530/14303 - Loss:  0.104, Seconds: 18.03\n",
            "Epoch   1/100 Batch 13560/14303 - Loss:  0.101, Seconds: 17.95\n",
            "Epoch   1/100 Batch 13590/14303 - Loss:  0.102, Seconds: 17.99\n",
            "Epoch   1/100 Batch 13620/14303 - Loss:  0.102, Seconds: 18.10\n",
            "Epoch   1/100 Batch 13650/14303 - Loss:  0.102, Seconds: 18.30\n",
            "Epoch   1/100 Batch 13680/14303 - Loss:  0.103, Seconds: 18.47\n",
            "Epoch   1/100 Batch 13710/14303 - Loss:  0.101, Seconds: 18.58\n",
            "Epoch   1/100 Batch 13740/14303 - Loss:  0.103, Seconds: 18.93\n",
            "Epoch   1/100 Batch 13770/14303 - Loss:  0.102, Seconds: 19.08\n",
            "Epoch   1/100 Batch 13800/14303 - Loss:  0.099, Seconds: 19.20\n",
            "Epoch   1/100 Batch 13830/14303 - Loss:  0.104, Seconds: 19.44\n",
            "Epoch   1/100 Batch 13860/14303 - Loss:  0.100, Seconds: 19.75\n",
            "Epoch   1/100 Batch 13890/14303 - Loss:  0.098, Seconds: 20.00\n",
            "Epoch   1/100 Batch 13920/14303 - Loss:  0.102, Seconds: 20.17\n",
            "Epoch   1/100 Batch 13950/14303 - Loss:  0.100, Seconds: 20.53\n",
            "Epoch   1/100 Batch 13980/14303 - Loss:  0.101, Seconds: 21.00\n",
            "Epoch   1/100 Batch 14010/14303 - Loss:  0.096, Seconds: 21.58\n",
            "Epoch   1/100 Batch 14040/14303 - Loss:  0.102, Seconds: 21.71\n",
            "Epoch   1/100 Batch 14070/14303 - Loss:  0.101, Seconds: 22.22\n",
            "Epoch   1/100 Batch 14100/14303 - Loss:  0.102, Seconds: 22.65\n",
            "Epoch   1/100 Batch 14130/14303 - Loss:  0.098, Seconds: 23.16\n",
            "Epoch   1/100 Batch 14160/14303 - Loss:  0.097, Seconds: 23.99\n",
            "Epoch   1/100 Batch 14190/14303 - Loss:  0.100, Seconds: 24.86\n",
            "Epoch   1/100 Batch 14220/14303 - Loss:  0.101, Seconds: 25.66\n",
            "Epoch   1/100 Batch 14250/14303 - Loss:  0.099, Seconds: 27.00\n",
            "Epoch   1/100 Batch 14280/14303 - Loss:  0.099, Seconds: 29.66\n",
            "  Validation Input: not as bad a sone sandal trhink\n",
            "  Validation Output: knowking knownkingkonkhokingkonkh\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is also clled granth shaib\n",
            "  Validation Output: iblebiblizbombic biblicabit\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is natendedf or adults only\n",
            "  Validation Output: lypholy joy loyely joy loyely\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: eason thingout for ofurself\n",
            "  Validation Output: flowed\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: i happened to rok quite ewll\n",
            "  Validation Output: lew\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: chef and i are doing very well\n",
            "  Validation Output: lemblowslowely lowel leghole\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: ecnter for tvhe peorfoorimng arts\n",
            "  Validation Output: storms to grasherstory storts more\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: spomoninto a hot bowl to sewrve\n",
            "  Validation Output: very very very very very very ver\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: poplei meet her and they swoon\n",
            "  Validation Output: poople meet her and they swoon\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: ym poor little headi s boiling\n",
            "  Validation Output: my poor little head is boiling\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: tchis had becom a common scene\n",
            "  Validation Output: encernez once menoge new menoger\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: veen went through the spcenials\n",
            "  Validation Output: else jost most list most list mos\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: preheat a steamer onf high heat\n",
            "  Validation Output: hated\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lol well someofneh ad o say it\n",
            "  Validation Output: it more to git my thing lol well\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he was weaving mats on th eslt\n",
            "  Validation Output: them to let mother told the late\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this is wrong for many reasonis\n",
            "  Validation Output: sings insonishions is some signs\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: todbay we hneade agani to cubao\n",
            "  Validation Output: of job job job job job job job jo\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep and member saus requirejd\n",
            "  Validation Output: djjodjedjodjedjodjedjodjedjodje\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: itw snowd middling hard to day\n",
            "  Validation Output: yathey dyjomay dogay dyjomay doy\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: theybouth wear leather sadnals\n",
            "  Validation Output: some slows last most heybouth we\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  0.846, Seconds: 0.00\n",
            "New Record!\n",
            "\n",
            "Training Model: 2\n",
            "Epoch   2/100 Batch   30/14303 - Loss:  0.169, Seconds: 5.65\n",
            "Epoch   2/100 Batch   60/14303 - Loss:  0.125, Seconds: 5.48\n",
            "Epoch   2/100 Batch   90/14303 - Loss:  0.122, Seconds: 5.47\n",
            "Epoch   2/100 Batch  120/14303 - Loss:  0.121, Seconds: 5.44\n",
            "Epoch   2/100 Batch  150/14303 - Loss:  0.120, Seconds: 5.44\n",
            "Epoch   2/100 Batch  180/14303 - Loss:  0.115, Seconds: 5.45\n",
            "Epoch   2/100 Batch  210/14303 - Loss:  0.113, Seconds: 5.48\n",
            "Epoch   2/100 Batch  240/14303 - Loss:  0.113, Seconds: 5.45\n",
            "Epoch   2/100 Batch  270/14303 - Loss:  0.116, Seconds: 5.42\n",
            "Epoch   2/100 Batch  300/14303 - Loss:  0.141, Seconds: 5.48\n",
            "Epoch   2/100 Batch  330/14303 - Loss:  0.117, Seconds: 5.58\n",
            "Epoch   2/100 Batch  360/14303 - Loss:  0.116, Seconds: 5.56\n",
            "Epoch   2/100 Batch  390/14303 - Loss:  0.114, Seconds: 5.60\n",
            "Epoch   2/100 Batch  420/14303 - Loss:  0.116, Seconds: 5.64\n",
            "Epoch   2/100 Batch  450/14303 - Loss:  0.111, Seconds: 5.79\n",
            "Epoch   2/100 Batch  480/14303 - Loss:  0.111, Seconds: 5.75\n",
            "Epoch   2/100 Batch  510/14303 - Loss:  0.112, Seconds: 5.59\n",
            "Epoch   2/100 Batch  540/14303 - Loss:  0.110, Seconds: 5.66\n",
            "Epoch   2/100 Batch  570/14303 - Loss:  0.113, Seconds: 5.65\n",
            "Epoch   2/100 Batch  600/14303 - Loss:  0.113, Seconds: 5.73\n",
            "Epoch   2/100 Batch  630/14303 - Loss:  0.112, Seconds: 5.83\n",
            "Epoch   2/100 Batch  660/14303 - Loss:  0.108, Seconds: 5.76\n",
            "Epoch   2/100 Batch  690/14303 - Loss:  0.102, Seconds: 5.80\n",
            "Epoch   2/100 Batch  720/14303 - Loss:  0.110, Seconds: 5.78\n",
            "Epoch   2/100 Batch  750/14303 - Loss:  0.110, Seconds: 5.78\n",
            "Epoch   2/100 Batch  780/14303 - Loss:  0.102, Seconds: 5.73\n",
            "Epoch   2/100 Batch  810/14303 - Loss:  0.106, Seconds: 5.84\n",
            "Epoch   2/100 Batch  840/14303 - Loss:  0.104, Seconds: 5.76\n",
            "Epoch   2/100 Batch  870/14303 - Loss:  0.114, Seconds: 5.79\n",
            "Epoch   2/100 Batch  900/14303 - Loss:  0.099, Seconds: 5.85\n",
            "Epoch   2/100 Batch  930/14303 - Loss:  0.114, Seconds: 5.88\n",
            "Epoch   2/100 Batch  960/14303 - Loss:  0.108, Seconds: 5.90\n",
            "Epoch   2/100 Batch  990/14303 - Loss:  0.108, Seconds: 5.99\n",
            "Epoch   2/100 Batch 1020/14303 - Loss:  0.106, Seconds: 6.00\n",
            "Epoch   2/100 Batch 1050/14303 - Loss:  0.111, Seconds: 5.96\n",
            "Epoch   2/100 Batch 1080/14303 - Loss:  0.104, Seconds: 6.00\n",
            "Epoch   2/100 Batch 1110/14303 - Loss:  0.106, Seconds: 5.92\n",
            "Epoch   2/100 Batch 1140/14303 - Loss:  0.105, Seconds: 6.02\n",
            "Epoch   2/100 Batch 1170/14303 - Loss:  0.103, Seconds: 5.95\n",
            "Epoch   2/100 Batch 1200/14303 - Loss:  0.105, Seconds: 5.94\n",
            "Epoch   2/100 Batch 1230/14303 - Loss:  0.106, Seconds: 5.89\n",
            "Epoch   2/100 Batch 1260/14303 - Loss:  0.102, Seconds: 6.12\n",
            "Epoch   2/100 Batch 1290/14303 - Loss:  0.103, Seconds: 6.08\n",
            "Epoch   2/100 Batch 1320/14303 - Loss:  0.102, Seconds: 6.12\n",
            "Epoch   2/100 Batch 1350/14303 - Loss:  0.107, Seconds: 6.18\n",
            "Epoch   2/100 Batch 1380/14303 - Loss:  0.101, Seconds: 6.19\n",
            "Epoch   2/100 Batch 1410/14303 - Loss:  0.103, Seconds: 6.12\n",
            "Epoch   2/100 Batch 1440/14303 - Loss:  0.104, Seconds: 6.21\n",
            "Epoch   2/100 Batch 1470/14303 - Loss:  0.104, Seconds: 6.11\n",
            "Epoch   2/100 Batch 1500/14303 - Loss:  0.107, Seconds: 6.13\n",
            "Epoch   2/100 Batch 1530/14303 - Loss:  0.100, Seconds: 6.17\n",
            "Epoch   2/100 Batch 1560/14303 - Loss:  0.107, Seconds: 6.21\n",
            "Epoch   2/100 Batch 1590/14303 - Loss:  0.103, Seconds: 6.32\n",
            "Epoch   2/100 Batch 1620/14303 - Loss:  0.096, Seconds: 6.33\n",
            "Epoch   2/100 Batch 1650/14303 - Loss:  0.103, Seconds: 6.40\n",
            "Epoch   2/100 Batch 1680/14303 - Loss:  0.106, Seconds: 6.33\n",
            "Epoch   2/100 Batch 1710/14303 - Loss:  0.103, Seconds: 6.35\n",
            "Epoch   2/100 Batch 1740/14303 - Loss:  0.098, Seconds: 6.31\n",
            "Epoch   2/100 Batch 1770/14303 - Loss:  0.099, Seconds: 6.36\n",
            "Epoch   2/100 Batch 1800/14303 - Loss:  0.099, Seconds: 6.28\n",
            "Epoch   2/100 Batch 1830/14303 - Loss:  0.099, Seconds: 6.29\n",
            "Epoch   2/100 Batch 1860/14303 - Loss:  0.104, Seconds: 6.30\n",
            "Epoch   2/100 Batch 1890/14303 - Loss:  0.103, Seconds: 6.63\n",
            "Epoch   2/100 Batch 1920/14303 - Loss:  0.096, Seconds: 6.64\n",
            "Epoch   2/100 Batch 1950/14303 - Loss:  0.105, Seconds: 6.52\n",
            "Epoch   2/100 Batch 1980/14303 - Loss:  0.100, Seconds: 6.42\n",
            "Epoch   2/100 Batch 2010/14303 - Loss:  0.099, Seconds: 6.52\n",
            "Epoch   2/100 Batch 2040/14303 - Loss:  0.102, Seconds: 6.47\n",
            "Epoch   2/100 Batch 2070/14303 - Loss:  0.097, Seconds: 6.51\n",
            "Epoch   2/100 Batch 2100/14303 - Loss:  0.101, Seconds: 6.50\n",
            "Epoch   2/100 Batch 2130/14303 - Loss:  0.099, Seconds: 6.44\n",
            "Epoch   2/100 Batch 2160/14303 - Loss:  0.097, Seconds: 6.41\n",
            "Epoch   2/100 Batch 2190/14303 - Loss:  0.104, Seconds: 6.59\n",
            "Epoch   2/100 Batch 2220/14303 - Loss:  0.107, Seconds: 6.66\n",
            "Epoch   2/100 Batch 2250/14303 - Loss:  0.102, Seconds: 6.62\n",
            "Epoch   2/100 Batch 2280/14303 - Loss:  0.100, Seconds: 6.64\n",
            "Epoch   2/100 Batch 2310/14303 - Loss:  0.101, Seconds: 6.59\n",
            "Epoch   2/100 Batch 2340/14303 - Loss:  0.104, Seconds: 6.64\n",
            "Epoch   2/100 Batch 2370/14303 - Loss:  0.102, Seconds: 6.67\n",
            "Epoch   2/100 Batch 2400/14303 - Loss:  0.100, Seconds: 6.77\n",
            "Epoch   2/100 Batch 2430/14303 - Loss:  0.101, Seconds: 6.78\n",
            "Epoch   2/100 Batch 2460/14303 - Loss:  0.102, Seconds: 6.64\n",
            "Epoch   2/100 Batch 2490/14303 - Loss:  0.101, Seconds: 6.66\n",
            "Epoch   2/100 Batch 2520/14303 - Loss:  0.104, Seconds: 6.78\n",
            "Epoch   2/100 Batch 2550/14303 - Loss:  0.101, Seconds: 6.79\n",
            "Epoch   2/100 Batch 2580/14303 - Loss:  0.103, Seconds: 6.75\n",
            "Epoch   2/100 Batch 2610/14303 - Loss:  0.102, Seconds: 6.85\n",
            "Epoch   2/100 Batch 2640/14303 - Loss:  0.096, Seconds: 6.79\n",
            "Epoch   2/100 Batch 2670/14303 - Loss:  0.102, Seconds: 6.81\n",
            "Epoch   2/100 Batch 2700/14303 - Loss:  0.105, Seconds: 6.81\n",
            "Epoch   2/100 Batch 2730/14303 - Loss:  0.099, Seconds: 6.78\n",
            "Epoch   2/100 Batch 2760/14303 - Loss:  0.101, Seconds: 6.82\n",
            "Epoch   2/100 Batch 2790/14303 - Loss:  0.095, Seconds: 6.83\n",
            "Epoch   2/100 Batch 2820/14303 - Loss:  0.096, Seconds: 6.85\n",
            "Epoch   2/100 Batch 2850/14303 - Loss:  0.102, Seconds: 6.91\n",
            "Epoch   2/100 Batch 2880/14303 - Loss:  0.102, Seconds: 6.99\n",
            "Epoch   2/100 Batch 2910/14303 - Loss:  0.102, Seconds: 7.01\n",
            "Epoch   2/100 Batch 2940/14303 - Loss:  0.095, Seconds: 6.98\n",
            "Epoch   2/100 Batch 2970/14303 - Loss:  0.102, Seconds: 7.01\n",
            "Epoch   2/100 Batch 3000/14303 - Loss:  0.097, Seconds: 6.99\n",
            "Epoch   2/100 Batch 3030/14303 - Loss:  0.096, Seconds: 7.02\n",
            "Epoch   2/100 Batch 3060/14303 - Loss:  0.101, Seconds: 6.97\n",
            "Epoch   2/100 Batch 3090/14303 - Loss:  0.100, Seconds: 7.07\n",
            "Epoch   2/100 Batch 3120/14303 - Loss:  0.098, Seconds: 7.04\n",
            "Epoch   2/100 Batch 3150/14303 - Loss:  0.101, Seconds: 6.93\n",
            "Epoch   2/100 Batch 3180/14303 - Loss:  0.095, Seconds: 7.28\n",
            "Epoch   2/100 Batch 3210/14303 - Loss:  0.101, Seconds: 7.43\n",
            "Epoch   2/100 Batch 3240/14303 - Loss:  0.098, Seconds: 7.21\n",
            "Epoch   2/100 Batch 3270/14303 - Loss:  0.093, Seconds: 7.24\n",
            "Epoch   2/100 Batch 3300/14303 - Loss:  0.099, Seconds: 7.18\n",
            "Epoch   2/100 Batch 3330/14303 - Loss:  0.096, Seconds: 7.20\n",
            "Epoch   2/100 Batch 3360/14303 - Loss:  0.100, Seconds: 7.10\n",
            "Epoch   2/100 Batch 3390/14303 - Loss:  0.100, Seconds: 7.16\n",
            "Epoch   2/100 Batch 3420/14303 - Loss:  0.098, Seconds: 7.06\n",
            "Epoch   2/100 Batch 3450/14303 - Loss:  0.099, Seconds: 7.02\n",
            "Epoch   2/100 Batch 3480/14303 - Loss:  0.101, Seconds: 7.08\n",
            "Epoch   2/100 Batch 3510/14303 - Loss:  0.101, Seconds: 7.19\n",
            "Epoch   2/100 Batch 3540/14303 - Loss:  0.099, Seconds: 7.19\n",
            "Epoch   2/100 Batch 3570/14303 - Loss:  0.099, Seconds: 7.22\n",
            "Epoch   2/100 Batch 3600/14303 - Loss:  0.099, Seconds: 7.14\n",
            "Epoch   2/100 Batch 3630/14303 - Loss:  0.097, Seconds: 7.27\n",
            "Epoch   2/100 Batch 3660/14303 - Loss:  0.100, Seconds: 7.24\n",
            "Epoch   2/100 Batch 3690/14303 - Loss:  0.099, Seconds: 7.25\n",
            "Epoch   2/100 Batch 3720/14303 - Loss:  0.099, Seconds: 7.23\n",
            "Epoch   2/100 Batch 3750/14303 - Loss:  0.094, Seconds: 7.21\n",
            "Epoch   2/100 Batch 3780/14303 - Loss:  0.101, Seconds: 7.31\n",
            "Epoch   2/100 Batch 3810/14303 - Loss:  0.101, Seconds: 7.35\n",
            "Epoch   2/100 Batch 3840/14303 - Loss:  0.095, Seconds: 7.47\n",
            "Epoch   2/100 Batch 3870/14303 - Loss:  0.098, Seconds: 7.49\n",
            "Epoch   2/100 Batch 3900/14303 - Loss:  0.097, Seconds: 7.50\n",
            "Epoch   2/100 Batch 3930/14303 - Loss:  0.099, Seconds: 7.52\n",
            "Epoch   2/100 Batch 3960/14303 - Loss:  0.101, Seconds: 7.52\n",
            "Epoch   2/100 Batch 3990/14303 - Loss:  0.097, Seconds: 7.50\n",
            "Epoch   2/100 Batch 4020/14303 - Loss:  0.101, Seconds: 7.55\n",
            "Epoch   2/100 Batch 4050/14303 - Loss:  0.093, Seconds: 7.59\n",
            "Epoch   2/100 Batch 4080/14303 - Loss:  0.094, Seconds: 7.57\n",
            "Epoch   2/100 Batch 4110/14303 - Loss:  0.092, Seconds: 7.43\n",
            "Epoch   2/100 Batch 4140/14303 - Loss:  0.098, Seconds: 7.74\n",
            "Epoch   2/100 Batch 4170/14303 - Loss:  0.099, Seconds: 7.74\n",
            "Epoch   2/100 Batch 4200/14303 - Loss:  0.097, Seconds: 7.72\n",
            "Epoch   2/100 Batch 4230/14303 - Loss:  0.096, Seconds: 7.75\n",
            "Epoch   2/100 Batch 4260/14303 - Loss:  0.097, Seconds: 7.69\n",
            "Epoch   2/100 Batch 4290/14303 - Loss:  0.096, Seconds: 7.62\n",
            "Epoch   2/100 Batch 4320/14303 - Loss:  0.095, Seconds: 7.78\n",
            "Epoch   2/100 Batch 4350/14303 - Loss:  0.098, Seconds: 7.77\n",
            "Epoch   2/100 Batch 4380/14303 - Loss:  0.100, Seconds: 7.96\n",
            "Epoch   2/100 Batch 4410/14303 - Loss:  0.098, Seconds: 7.74\n",
            "Epoch   2/100 Batch 4440/14303 - Loss:  0.097, Seconds: 7.86\n",
            "Epoch   2/100 Batch 4470/14303 - Loss:  0.095, Seconds: 7.88\n",
            "Epoch   2/100 Batch 4500/14303 - Loss:  0.102, Seconds: 7.88\n",
            "Epoch   2/100 Batch 4530/14303 - Loss:  0.098, Seconds: 7.88\n",
            "Epoch   2/100 Batch 4560/14303 - Loss:  0.100, Seconds: 7.90\n",
            "Epoch   2/100 Batch 4590/14303 - Loss:  0.095, Seconds: 7.98\n",
            "Epoch   2/100 Batch 4620/14303 - Loss:  0.099, Seconds: 7.92\n",
            "Epoch   2/100 Batch 4650/14303 - Loss:  0.102, Seconds: 7.91\n",
            "Epoch   2/100 Batch 4680/14303 - Loss:  0.095, Seconds: 7.87\n",
            "Epoch   2/100 Batch 4710/14303 - Loss:  0.097, Seconds: 7.89\n",
            "Epoch   2/100 Batch 4740/14303 - Loss:  0.097, Seconds: 8.00\n",
            "Epoch   2/100 Batch 4770/14303 - Loss:  0.101, Seconds: 8.05\n",
            "Epoch   2/100 Batch 4800/14303 - Loss:  0.097, Seconds: 8.06\n",
            "Epoch   2/100 Batch 4830/14303 - Loss:  0.100, Seconds: 8.05\n",
            "Epoch   2/100 Batch 4860/14303 - Loss:  0.097, Seconds: 8.04\n",
            "Epoch   2/100 Batch 4890/14303 - Loss:  0.098, Seconds: 8.10\n",
            "Epoch   2/100 Batch 4920/14303 - Loss:  0.095, Seconds: 8.05\n",
            "Epoch   2/100 Batch 4950/14303 - Loss:  0.095, Seconds: 8.05\n",
            "Epoch   2/100 Batch 4980/14303 - Loss:  0.095, Seconds: 7.99\n",
            "Epoch   2/100 Batch 5010/14303 - Loss:  0.095, Seconds: 8.07\n",
            "Epoch   2/100 Batch 5040/14303 - Loss:  0.095, Seconds: 8.02\n",
            "Epoch   2/100 Batch 5070/14303 - Loss:  0.101, Seconds: 8.13\n",
            "Epoch   2/100 Batch 5100/14303 - Loss:  0.095, Seconds: 8.15\n",
            "Epoch   2/100 Batch 5130/14303 - Loss:  0.099, Seconds: 8.18\n",
            "Epoch   2/100 Batch 5160/14303 - Loss:  0.093, Seconds: 8.24\n",
            "Epoch   2/100 Batch 5190/14303 - Loss:  0.094, Seconds: 8.27\n",
            "Epoch   2/100 Batch 5220/14303 - Loss:  0.093, Seconds: 8.23\n",
            "Epoch   2/100 Batch 5250/14303 - Loss:  0.097, Seconds: 8.23\n",
            "Epoch   2/100 Batch 5280/14303 - Loss:  0.096, Seconds: 8.22\n",
            "Epoch   2/100 Batch 5310/14303 - Loss:  0.093, Seconds: 8.25\n",
            "Epoch   2/100 Batch 5340/14303 - Loss:  0.095, Seconds: 8.19\n",
            "Epoch   2/100 Batch 5370/14303 - Loss:  0.100, Seconds: 8.31\n",
            "Epoch   2/100 Batch 5400/14303 - Loss:  0.097, Seconds: 8.33\n",
            "Epoch   2/100 Batch 5430/14303 - Loss:  0.093, Seconds: 8.46\n",
            "Epoch   2/100 Batch 5460/14303 - Loss:  0.092, Seconds: 8.54\n",
            "Epoch   2/100 Batch 5490/14303 - Loss:  0.096, Seconds: 8.41\n",
            "Epoch   2/100 Batch 5520/14303 - Loss:  0.096, Seconds: 8.37\n",
            "Epoch   2/100 Batch 5550/14303 - Loss:  0.096, Seconds: 8.36\n",
            "Epoch   2/100 Batch 5580/14303 - Loss:  0.097, Seconds: 8.38\n",
            "Epoch   2/100 Batch 5610/14303 - Loss:  0.096, Seconds: 8.40\n",
            "Epoch   2/100 Batch 5640/14303 - Loss:  0.097, Seconds: 8.37\n",
            "Epoch   2/100 Batch 5670/14303 - Loss:  0.097, Seconds: 8.56\n",
            "Epoch   2/100 Batch 5700/14303 - Loss:  0.094, Seconds: 8.63\n",
            "Epoch   2/100 Batch 5730/14303 - Loss:  0.097, Seconds: 8.47\n",
            "Epoch   2/100 Batch 5760/14303 - Loss:  0.095, Seconds: 8.60\n",
            "Epoch   2/100 Batch 5790/14303 - Loss:  0.094, Seconds: 8.72\n",
            "Epoch   2/100 Batch 5820/14303 - Loss:  0.094, Seconds: 8.54\n",
            "Epoch   2/100 Batch 5850/14303 - Loss:  0.097, Seconds: 8.60\n",
            "Epoch   2/100 Batch 5880/14303 - Loss:  0.096, Seconds: 8.64\n",
            "Epoch   2/100 Batch 5910/14303 - Loss:  0.094, Seconds: 8.57\n",
            "Epoch   2/100 Batch 5940/14303 - Loss:  0.092, Seconds: 8.55\n",
            "Epoch   2/100 Batch 5970/14303 - Loss:  0.094, Seconds: 8.75\n",
            "Epoch   2/100 Batch 6000/14303 - Loss:  0.095, Seconds: 8.84\n",
            "Epoch   2/100 Batch 6030/14303 - Loss:  0.096, Seconds: 8.76\n",
            "Epoch   2/100 Batch 6060/14303 - Loss:  0.095, Seconds: 8.72\n",
            "Epoch   2/100 Batch 6090/14303 - Loss:  0.093, Seconds: 8.77\n",
            "Epoch   2/100 Batch 6120/14303 - Loss:  0.093, Seconds: 8.70\n",
            "Epoch   2/100 Batch 6150/14303 - Loss:  0.090, Seconds: 8.68\n",
            "Epoch   2/100 Batch 6180/14303 - Loss:  0.100, Seconds: 8.74\n",
            "Epoch   2/100 Batch 6210/14303 - Loss:  0.092, Seconds: 8.85\n",
            "Epoch   2/100 Batch 6240/14303 - Loss:  0.095, Seconds: 8.87\n",
            "Epoch   2/100 Batch 6270/14303 - Loss:  0.096, Seconds: 8.92\n",
            "Epoch   2/100 Batch 6300/14303 - Loss:  0.093, Seconds: 8.91\n",
            "Epoch   2/100 Batch 6330/14303 - Loss:  0.097, Seconds: 8.87\n",
            "Epoch   2/100 Batch 6360/14303 - Loss:  0.092, Seconds: 8.94\n",
            "Epoch   2/100 Batch 6390/14303 - Loss:  0.091, Seconds: 8.91\n",
            "Epoch   2/100 Batch 6420/14303 - Loss:  0.095, Seconds: 8.91\n",
            "Epoch   2/100 Batch 6450/14303 - Loss:  0.091, Seconds: 9.07\n",
            "Epoch   2/100 Batch 6480/14303 - Loss:  0.099, Seconds: 9.12\n",
            "Epoch   2/100 Batch 6510/14303 - Loss:  0.094, Seconds: 8.92\n",
            "Epoch   2/100 Batch 6540/14303 - Loss:  0.090, Seconds: 9.08\n",
            "Epoch   2/100 Batch 6570/14303 - Loss:  0.094, Seconds: 9.19\n",
            "Epoch   2/100 Batch 6600/14303 - Loss:  0.095, Seconds: 9.03\n",
            "Epoch   2/100 Batch 6630/14303 - Loss:  0.094, Seconds: 9.07\n",
            "Epoch   2/100 Batch 6660/14303 - Loss:  0.098, Seconds: 9.07\n",
            "Epoch   2/100 Batch 6690/14303 - Loss:  0.094, Seconds: 9.03\n",
            "Epoch   2/100 Batch 6720/14303 - Loss:  0.091, Seconds: 9.07\n",
            "Epoch   2/100 Batch 6750/14303 - Loss:  0.095, Seconds: 9.19\n",
            "Epoch   2/100 Batch 6780/14303 - Loss:  0.092, Seconds: 9.08\n",
            "Epoch   2/100 Batch 6810/14303 - Loss:  0.097, Seconds: 9.20\n",
            "Epoch   2/100 Batch 6840/14303 - Loss:  0.099, Seconds: 9.23\n",
            "Epoch   2/100 Batch 6870/14303 - Loss:  0.098, Seconds: 9.20\n",
            "Epoch   2/100 Batch 6900/14303 - Loss:  0.098, Seconds: 9.24\n",
            "Epoch   2/100 Batch 6930/14303 - Loss:  0.095, Seconds: 9.22\n",
            "Epoch   2/100 Batch 6960/14303 - Loss:  0.096, Seconds: 9.27\n",
            "Epoch   2/100 Batch 6990/14303 - Loss:  0.091, Seconds: 9.25\n",
            "Epoch   2/100 Batch 7020/14303 - Loss:  0.094, Seconds: 9.13\n",
            "Epoch   2/100 Batch 7050/14303 - Loss:  0.091, Seconds: 9.18\n",
            "Epoch   2/100 Batch 7080/14303 - Loss:  0.091, Seconds: 9.31\n",
            "Epoch   2/100 Batch 7110/14303 - Loss:  0.094, Seconds: 9.44\n",
            "Epoch   2/100 Batch 7140/14303 - Loss:  0.096, Seconds: 9.47\n",
            "Epoch   2/100 Batch 7170/14303 - Loss:  0.094, Seconds: 9.49\n",
            "Epoch   2/100 Batch 7200/14303 - Loss:  0.095, Seconds: 9.60\n",
            "Epoch   2/100 Batch 7230/14303 - Loss:  0.093, Seconds: 9.58\n",
            "Epoch   2/100 Batch 7260/14303 - Loss:  0.091, Seconds: 9.49\n",
            "Epoch   2/100 Batch 7290/14303 - Loss:  0.093, Seconds: 9.54\n",
            "Epoch   2/100 Batch 7320/14303 - Loss:  0.091, Seconds: 9.52\n",
            "Epoch   2/100 Batch 7350/14303 - Loss:  0.094, Seconds: 9.62\n",
            "Epoch   2/100 Batch 7380/14303 - Loss:  0.090, Seconds: 9.63\n",
            "Epoch   2/100 Batch 7410/14303 - Loss:  0.094, Seconds: 9.86\n",
            "Epoch   2/100 Batch 7440/14303 - Loss:  0.090, Seconds: 9.61\n",
            "Epoch   2/100 Batch 7470/14303 - Loss:  0.092, Seconds: 9.66\n",
            "Epoch   2/100 Batch 7500/14303 - Loss:  0.093, Seconds: 9.65\n",
            "Epoch   2/100 Batch 7530/14303 - Loss:  0.838, Seconds: 9.67\n",
            "Epoch   2/100 Batch 7560/14303 - Loss:  0.175, Seconds: 9.69\n",
            "Epoch   2/100 Batch 7590/14303 - Loss:  0.134, Seconds: 9.61\n",
            "Epoch   2/100 Batch 7620/14303 - Loss:  0.123, Seconds: 9.77\n",
            "Epoch   2/100 Batch 7650/14303 - Loss:  0.114, Seconds: 9.90\n",
            "Epoch   2/100 Batch 7680/14303 - Loss:  0.116, Seconds: 9.98\n",
            "Epoch   2/100 Batch 7710/14303 - Loss:  0.109, Seconds: 9.91\n",
            "Epoch   2/100 Batch 7740/14303 - Loss:  0.112, Seconds: 9.81\n",
            "Epoch   2/100 Batch 7770/14303 - Loss:  0.104, Seconds: 9.77\n",
            "Epoch   2/100 Batch 7800/14303 - Loss:  0.104, Seconds: 9.79\n",
            "Epoch   2/100 Batch 7830/14303 - Loss:  0.104, Seconds: 9.77\n",
            "Epoch   2/100 Batch 7860/14303 - Loss:  0.101, Seconds: 9.85\n",
            "Epoch   2/100 Batch 7890/14303 - Loss:  0.096, Seconds: 9.96\n",
            "Epoch   2/100 Batch 7920/14303 - Loss:  0.100, Seconds: 9.98\n",
            "Epoch   2/100 Batch 7950/14303 - Loss:  0.102, Seconds: 9.99\n",
            "Epoch   2/100 Batch 7980/14303 - Loss:  0.099, Seconds: 10.12\n",
            "Epoch   2/100 Batch 8010/14303 - Loss:  0.096, Seconds: 10.05\n",
            "Epoch   2/100 Batch 8040/14303 - Loss:  0.094, Seconds: 9.95\n",
            "Epoch   2/100 Batch 8070/14303 - Loss:  0.092, Seconds: 9.97\n",
            "Epoch   2/100 Batch 8100/14303 - Loss:  0.096, Seconds: 9.96\n",
            "Epoch   2/100 Batch 8130/14303 - Loss:  0.100, Seconds: 10.12\n",
            "Epoch   2/100 Batch 8160/14303 - Loss:  0.094, Seconds: 10.24\n",
            "Epoch   2/100 Batch 8190/14303 - Loss:  0.095, Seconds: 10.17\n",
            "Epoch   2/100 Batch 8220/14303 - Loss:  0.098, Seconds: 10.21\n",
            "Epoch   2/100 Batch 8250/14303 - Loss:  0.095, Seconds: 10.13\n",
            "Epoch   2/100 Batch 8280/14303 - Loss:  0.094, Seconds: 10.33\n",
            "Epoch   2/100 Batch 8310/14303 - Loss:  0.092, Seconds: 10.32\n",
            "Epoch   2/100 Batch 8340/14303 - Loss:  0.092, Seconds: 10.17\n",
            "Epoch   2/100 Batch 8370/14303 - Loss:  0.093, Seconds: 10.37\n",
            "Epoch   2/100 Batch 8400/14303 - Loss:  0.091, Seconds: 10.42\n",
            "Epoch   2/100 Batch 8430/14303 - Loss:  0.093, Seconds: 10.25\n",
            "Epoch   2/100 Batch 8460/14303 - Loss:  0.091, Seconds: 10.30\n",
            "Epoch   2/100 Batch 8490/14303 - Loss:  0.093, Seconds: 10.33\n",
            "Epoch   2/100 Batch 8520/14303 - Loss:  0.089, Seconds: 10.48\n",
            "Epoch   2/100 Batch 8550/14303 - Loss:  0.091, Seconds: 10.23\n",
            "Epoch   2/100 Batch 8580/14303 - Loss:  0.090, Seconds: 10.37\n",
            "Epoch   2/100 Batch 8610/14303 - Loss:  0.091, Seconds: 10.60\n",
            "Epoch   2/100 Batch 8640/14303 - Loss:  0.092, Seconds: 10.49\n",
            "Epoch   2/100 Batch 8670/14303 - Loss:  0.090, Seconds: 10.52\n",
            "Epoch   2/100 Batch 8700/14303 - Loss:  0.088, Seconds: 10.46\n",
            "Epoch   2/100 Batch 8730/14303 - Loss:  0.091, Seconds: 10.49\n",
            "Epoch   2/100 Batch 8760/14303 - Loss:  0.092, Seconds: 10.45\n",
            "Epoch   2/100 Batch 8790/14303 - Loss:  0.090, Seconds: 10.35\n",
            "Epoch   2/100 Batch 8820/14303 - Loss:  0.091, Seconds: 10.64\n",
            "Epoch   2/100 Batch 8850/14303 - Loss:  0.093, Seconds: 10.78\n",
            "Epoch   2/100 Batch 8880/14303 - Loss:  0.090, Seconds: 10.78\n",
            "Epoch   2/100 Batch 8910/14303 - Loss:  0.091, Seconds: 10.72\n",
            "Epoch   2/100 Batch 8940/14303 - Loss:  0.090, Seconds: 10.60\n",
            "Epoch   2/100 Batch 8970/14303 - Loss:  0.090, Seconds: 10.58\n",
            "Epoch   2/100 Batch 9000/14303 - Loss:  0.091, Seconds: 10.62\n",
            "Epoch   2/100 Batch 9030/14303 - Loss:  0.091, Seconds: 10.79\n",
            "Epoch   2/100 Batch 9060/14303 - Loss:  0.091, Seconds: 10.84\n",
            "Epoch   2/100 Batch 9090/14303 - Loss:  0.091, Seconds: 10.74\n",
            "Epoch   2/100 Batch 9120/14303 - Loss:  0.090, Seconds: 11.03\n",
            "Epoch   2/100 Batch 9150/14303 - Loss:  0.089, Seconds: 10.88\n",
            "Epoch   2/100 Batch 9180/14303 - Loss:  0.090, Seconds: 10.80\n",
            "Epoch   2/100 Batch 9210/14303 - Loss:  0.088, Seconds: 10.80\n",
            "Epoch   2/100 Batch 9240/14303 - Loss:  0.091, Seconds: 10.90\n",
            "Epoch   2/100 Batch 9270/14303 - Loss:  0.087, Seconds: 10.94\n",
            "Epoch   2/100 Batch 9300/14303 - Loss:  0.087, Seconds: 11.12\n",
            "Epoch   2/100 Batch 9330/14303 - Loss:  0.090, Seconds: 11.12\n",
            "Epoch   2/100 Batch 9360/14303 - Loss:  0.088, Seconds: 11.03\n",
            "Epoch   2/100 Batch 9390/14303 - Loss:  0.091, Seconds: 11.00\n",
            "Epoch   2/100 Batch 9420/14303 - Loss:  0.087, Seconds: 10.99\n",
            "Epoch   2/100 Batch 9450/14303 - Loss:  0.089, Seconds: 11.09\n",
            "Epoch   2/100 Batch 9480/14303 - Loss:  0.087, Seconds: 10.95\n",
            "Epoch   2/100 Batch 9510/14303 - Loss:  0.090, Seconds: 11.01\n",
            "Epoch   2/100 Batch 9540/14303 - Loss:  0.090, Seconds: 11.24\n",
            "Epoch   2/100 Batch 9570/14303 - Loss:  0.087, Seconds: 11.11\n",
            "Epoch   2/100 Batch 9600/14303 - Loss:  0.089, Seconds: 11.23\n",
            "Epoch   2/100 Batch 9630/14303 - Loss:  0.094, Seconds: 11.19\n",
            "Epoch   2/100 Batch 9660/14303 - Loss:  0.085, Seconds: 11.33\n",
            "Epoch   2/100 Batch 9690/14303 - Loss:  0.092, Seconds: 11.34\n",
            "Epoch   2/100 Batch 9720/14303 - Loss:  0.087, Seconds: 11.34\n",
            "Epoch   2/100 Batch 9750/14303 - Loss:  0.090, Seconds: 11.38\n",
            "Epoch   2/100 Batch 9780/14303 - Loss:  0.088, Seconds: 11.33\n",
            "Epoch   2/100 Batch 9810/14303 - Loss:  0.089, Seconds: 11.43\n",
            "Epoch   2/100 Batch 9840/14303 - Loss:  0.090, Seconds: 11.39\n",
            "Epoch   2/100 Batch 9870/14303 - Loss:  0.090, Seconds: 11.51\n",
            "Epoch   2/100 Batch 9900/14303 - Loss:  0.090, Seconds: 11.70\n",
            "Epoch   2/100 Batch 9930/14303 - Loss:  0.088, Seconds: 11.59\n",
            "Epoch   2/100 Batch 9960/14303 - Loss:  0.088, Seconds: 11.48\n",
            "Epoch   2/100 Batch 9990/14303 - Loss:  0.090, Seconds: 11.47\n",
            "Epoch   2/100 Batch 10020/14303 - Loss:  0.088, Seconds: 11.45\n",
            "Epoch   2/100 Batch 10050/14303 - Loss:  0.086, Seconds: 11.63\n",
            "Epoch   2/100 Batch 10080/14303 - Loss:  0.086, Seconds: 11.82\n",
            "Epoch   2/100 Batch 10110/14303 - Loss:  0.086, Seconds: 11.72\n",
            "Epoch   2/100 Batch 10140/14303 - Loss:  0.086, Seconds: 11.57\n",
            "Epoch   2/100 Batch 10170/14303 - Loss:  0.084, Seconds: 11.68\n",
            "Epoch   2/100 Batch 10200/14303 - Loss:  0.086, Seconds: 11.75\n",
            "Epoch   2/100 Batch 10230/14303 - Loss:  0.090, Seconds: 11.71\n",
            "Epoch   2/100 Batch 10260/14303 - Loss:  0.087, Seconds: 11.84\n",
            "Epoch   2/100 Batch 10290/14303 - Loss:  0.091, Seconds: 11.86\n",
            "Epoch   2/100 Batch 10320/14303 - Loss:  0.088, Seconds: 11.88\n",
            "Epoch   2/100 Batch 10350/14303 - Loss:  0.085, Seconds: 11.73\n",
            "Epoch   2/100 Batch 10380/14303 - Loss:  0.084, Seconds: 11.85\n",
            "Epoch   2/100 Batch 10410/14303 - Loss:  0.090, Seconds: 11.97\n",
            "Epoch   2/100 Batch 10440/14303 - Loss:  0.090, Seconds: 12.08\n",
            "Epoch   2/100 Batch 10470/14303 - Loss:  0.088, Seconds: 12.00\n",
            "Epoch   2/100 Batch 10500/14303 - Loss:  0.085, Seconds: 11.99\n",
            "Epoch   2/100 Batch 10530/14303 - Loss:  0.088, Seconds: 12.01\n",
            "Epoch   2/100 Batch 10560/14303 - Loss:  0.089, Seconds: 11.99\n",
            "Epoch   2/100 Batch 10590/14303 - Loss:  0.092, Seconds: 12.06\n",
            "Epoch   2/100 Batch 10620/14303 - Loss:  0.087, Seconds: 12.19\n",
            "Epoch   2/100 Batch 10650/14303 - Loss:  0.089, Seconds: 12.46\n",
            "Epoch   2/100 Batch 10680/14303 - Loss:  0.084, Seconds: 12.16\n",
            "Epoch   2/100 Batch 10710/14303 - Loss:  0.091, Seconds: 12.19\n",
            "Epoch   2/100 Batch 10740/14303 - Loss:  0.086, Seconds: 12.20\n",
            "Epoch   2/100 Batch 10770/14303 - Loss:  0.088, Seconds: 12.30\n",
            "Epoch   2/100 Batch 10800/14303 - Loss:  0.085, Seconds: 12.42\n",
            "Epoch   2/100 Batch 10830/14303 - Loss:  0.089, Seconds: 12.29\n",
            "Epoch   2/100 Batch 10860/14303 - Loss:  0.088, Seconds: 12.33\n",
            "Epoch   2/100 Batch 10890/14303 - Loss:  0.088, Seconds: 12.32\n",
            "Epoch   2/100 Batch 10920/14303 - Loss:  0.088, Seconds: 12.51\n",
            "Epoch   2/100 Batch 10950/14303 - Loss:  0.088, Seconds: 12.56\n",
            "Epoch   2/100 Batch 10980/14303 - Loss:  0.088, Seconds: 12.60\n",
            "Epoch   2/100 Batch 11010/14303 - Loss:  0.084, Seconds: 12.56\n",
            "Epoch   2/100 Batch 11040/14303 - Loss:  0.084, Seconds: 12.46\n",
            "Epoch   2/100 Batch 11070/14303 - Loss:  0.085, Seconds: 12.60\n",
            "Epoch   2/100 Batch 11100/14303 - Loss:  0.089, Seconds: 12.69\n",
            "Epoch   2/100 Batch 11130/14303 - Loss:  0.089, Seconds: 12.66\n",
            "Epoch   2/100 Batch 11160/14303 - Loss:  0.087, Seconds: 12.82\n",
            "Epoch   2/100 Batch 11190/14303 - Loss:  0.085, Seconds: 12.85\n",
            "Epoch   2/100 Batch 11220/14303 - Loss:  0.085, Seconds: 12.95\n",
            "Epoch   2/100 Batch 11250/14303 - Loss:  0.087, Seconds: 13.00\n",
            "Epoch   2/100 Batch 11280/14303 - Loss:  0.085, Seconds: 12.95\n",
            "Epoch   2/100 Batch 11310/14303 - Loss:  0.089, Seconds: 12.98\n",
            "Epoch   2/100 Batch 11340/14303 - Loss:  0.087, Seconds: 13.11\n",
            "Epoch   2/100 Batch 11370/14303 - Loss:  0.087, Seconds: 13.20\n",
            "Epoch   2/100 Batch 11400/14303 - Loss:  0.084, Seconds: 13.09\n",
            "Epoch   2/100 Batch 11430/14303 - Loss:  0.085, Seconds: 13.13\n",
            "Epoch   2/100 Batch 11460/14303 - Loss:  0.084, Seconds: 13.15\n",
            "Epoch   2/100 Batch 11490/14303 - Loss:  0.085, Seconds: 13.38\n",
            "Epoch   2/100 Batch 11520/14303 - Loss:  0.084, Seconds: 13.37\n",
            "Epoch   2/100 Batch 11550/14303 - Loss:  0.089, Seconds: 13.28\n",
            "Epoch   2/100 Batch 11580/14303 - Loss:  0.086, Seconds: 13.26\n",
            "Epoch   2/100 Batch 11610/14303 - Loss:  0.089, Seconds: 13.25\n",
            "Epoch   2/100 Batch 11640/14303 - Loss:  0.089, Seconds: 13.49\n",
            "Epoch   2/100 Batch 11670/14303 - Loss:  0.083, Seconds: 13.43\n",
            "Epoch   2/100 Batch 11700/14303 - Loss:  0.086, Seconds: 13.53\n",
            "Epoch   2/100 Batch 11730/14303 - Loss:  0.085, Seconds: 13.42\n",
            "Epoch   2/100 Batch 11760/14303 - Loss:  0.084, Seconds: 13.55\n",
            "Epoch   2/100 Batch 11790/14303 - Loss:  0.087, Seconds: 13.66\n",
            "Epoch   2/100 Batch 11820/14303 - Loss:  0.085, Seconds: 13.59\n",
            "Epoch   2/100 Batch 11850/14303 - Loss:  0.088, Seconds: 13.68\n",
            "Epoch   2/100 Batch 11880/14303 - Loss:  0.083, Seconds: 13.72\n",
            "Epoch   2/100 Batch 11910/14303 - Loss:  0.083, Seconds: 13.53\n",
            "Epoch   2/100 Batch 11940/14303 - Loss:  0.084, Seconds: 13.51\n",
            "Epoch   2/100 Batch 11970/14303 - Loss:  0.086, Seconds: 13.63\n",
            "Epoch   2/100 Batch 12000/14303 - Loss:  0.086, Seconds: 14.03\n",
            "Epoch   2/100 Batch 12030/14303 - Loss:  0.084, Seconds: 14.04\n",
            "Epoch   2/100 Batch 12060/14303 - Loss:  0.084, Seconds: 13.95\n",
            "Epoch   2/100 Batch 12090/14303 - Loss:  0.085, Seconds: 13.98\n",
            "Epoch   2/100 Batch 12120/14303 - Loss:  0.083, Seconds: 14.34\n",
            "Epoch   2/100 Batch 12150/14303 - Loss:  0.086, Seconds: 14.11\n",
            "Epoch   2/100 Batch 12180/14303 - Loss:  0.086, Seconds: 14.27\n",
            "Epoch   2/100 Batch 12210/14303 - Loss:  0.087, Seconds: 14.37\n",
            "Epoch   2/100 Batch 12240/14303 - Loss:  0.085, Seconds: 14.27\n",
            "Epoch   2/100 Batch 12270/14303 - Loss:  0.085, Seconds: 14.23\n",
            "Epoch   2/100 Batch 12300/14303 - Loss:  0.087, Seconds: 14.49\n",
            "Epoch   2/100 Batch 12330/14303 - Loss:  0.083, Seconds: 14.51\n",
            "Epoch   2/100 Batch 12360/14303 - Loss:  0.087, Seconds: 14.47\n",
            "Epoch   2/100 Batch 12390/14303 - Loss:  0.087, Seconds: 14.66\n",
            "Epoch   2/100 Batch 12420/14303 - Loss:  0.084, Seconds: 14.77\n",
            "Epoch   2/100 Batch 12450/14303 - Loss:  0.083, Seconds: 14.72\n",
            "Epoch   2/100 Batch 12480/14303 - Loss:  0.085, Seconds: 14.85\n",
            "Epoch   2/100 Batch 12510/14303 - Loss:  0.085, Seconds: 14.93\n",
            "Epoch   2/100 Batch 12540/14303 - Loss:  0.085, Seconds: 14.90\n",
            "Epoch   2/100 Batch 12570/14303 - Loss:  0.085, Seconds: 14.97\n",
            "Epoch   2/100 Batch 12600/14303 - Loss:  0.084, Seconds: 15.37\n",
            "Epoch   2/100 Batch 12630/14303 - Loss:  0.086, Seconds: 15.13\n",
            "Epoch   2/100 Batch 12660/14303 - Loss:  0.084, Seconds: 15.37\n",
            "Epoch   2/100 Batch 12690/14303 - Loss:  0.084, Seconds: 15.39\n",
            "Epoch   2/100 Batch 12720/14303 - Loss:  0.083, Seconds: 15.45\n",
            "Epoch   2/100 Batch 12750/14303 - Loss:  0.085, Seconds: 15.41\n",
            "Epoch   2/100 Batch 12780/14303 - Loss:  0.086, Seconds: 15.61\n",
            "Epoch   2/100 Batch 12810/14303 - Loss:  0.084, Seconds: 15.72\n",
            "Epoch   2/100 Batch 12840/14303 - Loss:  0.085, Seconds: 15.79\n",
            "Epoch   2/100 Batch 12870/14303 - Loss:  0.082, Seconds: 15.72\n",
            "Epoch   2/100 Batch 12900/14303 - Loss:  0.084, Seconds: 15.87\n",
            "Epoch   2/100 Batch 12930/14303 - Loss:  0.083, Seconds: 15.96\n",
            "Epoch   2/100 Batch 12960/14303 - Loss:  0.085, Seconds: 16.08\n",
            "Epoch   2/100 Batch 12990/14303 - Loss:  0.082, Seconds: 16.06\n",
            "Epoch   2/100 Batch 13020/14303 - Loss:  0.085, Seconds: 16.04\n",
            "Epoch   2/100 Batch 13050/14303 - Loss:  0.082, Seconds: 16.35\n",
            "Epoch   2/100 Batch 13080/14303 - Loss:  0.083, Seconds: 16.37\n",
            "Epoch   2/100 Batch 13110/14303 - Loss:  0.082, Seconds: 16.45\n",
            "Epoch   2/100 Batch 13140/14303 - Loss:  0.085, Seconds: 16.66\n",
            "Epoch   2/100 Batch 13170/14303 - Loss:  0.082, Seconds: 16.89\n",
            "Epoch   2/100 Batch 13200/14303 - Loss:  0.082, Seconds: 16.91\n",
            "Epoch   2/100 Batch 13230/14303 - Loss:  0.082, Seconds: 17.13\n",
            "Epoch   2/100 Batch 13260/14303 - Loss:  0.294, Seconds: 17.01\n",
            "Epoch   2/100 Batch 13290/14303 - Loss:  0.131, Seconds: 17.17\n",
            "Epoch   2/100 Batch 13320/14303 - Loss:  0.110, Seconds: 17.26\n",
            "Epoch   2/100 Batch 13350/14303 - Loss:  0.102, Seconds: 17.45\n",
            "Epoch   2/100 Batch 13380/14303 - Loss:  0.102, Seconds: 17.42\n",
            "Epoch   2/100 Batch 13410/14303 - Loss:  0.096, Seconds: 17.61\n",
            "Epoch   2/100 Batch 13440/14303 - Loss:  0.093, Seconds: 17.80\n",
            "Epoch   2/100 Batch 13470/14303 - Loss:  0.092, Seconds: 17.78\n",
            "Epoch   2/100 Batch 13500/14303 - Loss:  0.089, Seconds: 18.00\n",
            "Epoch   2/100 Batch 13530/14303 - Loss:  0.091, Seconds: 18.16\n",
            "Epoch   2/100 Batch 13560/14303 - Loss:  0.090, Seconds: 18.38\n",
            "Epoch   2/100 Batch 13590/14303 - Loss:  0.088, Seconds: 18.43\n",
            "Epoch   2/100 Batch 13620/14303 - Loss:  0.088, Seconds: 18.64\n",
            "Epoch   2/100 Batch 13650/14303 - Loss:  0.089, Seconds: 19.01\n",
            "Epoch   2/100 Batch 13680/14303 - Loss:  0.086, Seconds: 19.06\n",
            "Epoch   2/100 Batch 13710/14303 - Loss:  0.088, Seconds: 19.33\n",
            "Epoch   2/100 Batch 13740/14303 - Loss:  0.085, Seconds: 19.30\n",
            "Epoch   2/100 Batch 13770/14303 - Loss:  0.084, Seconds: 19.52\n",
            "Epoch   2/100 Batch 13800/14303 - Loss:  0.088, Seconds: 19.83\n",
            "Epoch   2/100 Batch 13830/14303 - Loss:  0.087, Seconds: 20.05\n",
            "Epoch   2/100 Batch 13860/14303 - Loss:  0.082, Seconds: 20.35\n",
            "Epoch   2/100 Batch 13890/14303 - Loss:  0.080, Seconds: 20.56\n",
            "Epoch   2/100 Batch 13920/14303 - Loss:  0.083, Seconds: 20.79\n",
            "Epoch   2/100 Batch 13950/14303 - Loss:  0.082, Seconds: 21.18\n",
            "Epoch   2/100 Batch 13980/14303 - Loss:  0.085, Seconds: 21.49\n",
            "Epoch   2/100 Batch 14010/14303 - Loss:  0.081, Seconds: 21.68\n",
            "Epoch   2/100 Batch 14040/14303 - Loss:  0.084, Seconds: 22.19\n",
            "Epoch   2/100 Batch 14070/14303 - Loss:  0.084, Seconds: 22.78\n",
            "Epoch   2/100 Batch 14100/14303 - Loss:  0.084, Seconds: 23.26\n",
            "Epoch   2/100 Batch 14130/14303 - Loss:  0.084, Seconds: 23.75\n",
            "Epoch   2/100 Batch 14160/14303 - Loss:  0.083, Seconds: 24.28\n",
            "Epoch   2/100 Batch 14190/14303 - Loss:  0.084, Seconds: 25.09\n",
            "Epoch   2/100 Batch 14220/14303 - Loss:  0.085, Seconds: 26.17\n",
            "Epoch   2/100 Batch 14250/14303 - Loss:  0.083, Seconds: 27.64\n",
            "Epoch   2/100 Batch 14280/14303 - Loss:  0.085, Seconds: 30.08\n",
            "  Validation Input: not nas bad as ne sandl tnik\n",
            "  Validation Output: not and not and as bad as one b\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: iti  salso called granth sahib\n",
            "  Validation Output: it is also also calls to called g\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: i siitnended for adults only\n",
            "  Validation Output: is insistended for aisistended\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reasozn things outfor oyurself\n",
            "  Validation Output: reason things on things out thing\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it happened tow ork quite well\n",
            "  Validation Output: it happened to work quite work q\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: cihef andb i are doing veyetll\n",
            "  Validation Output: chief and i are doing are doing v\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: center fotkhe perforimn garts\n",
            "  Validation Output: center of the performing of the\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: spoo ninto a hot bowl toi serve\n",
            "  Validation Output: spoon into a hot bowl into a hot\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: peopleb meet uher an dthy wsjoon\n",
            "  Validation Output: people be people bement here and t\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my pork little head is boieling\n",
            "  Validation Output: my pork little head is blittle hea\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this had becmo a comon scpne\n",
            "  Validation Output: this had become this had become\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: even wetn throughr the special\n",
            "  Validation Output: even went went through through th\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: preheat a steamer o high heat\n",
            "  Validation Output: preheat a streetheat as streamer\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lol wells omeon had to sya it\n",
            "  Validation Output: lollow wells comeon had someone\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he wmas weavingmats on th lst\n",
            "  Validation Output: he was weams weaving mast weavin\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this is wrnog for many easons\n",
            "  Validation Output: this is wrong this is wrong for\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: oday we headed again to cubao\n",
            "  Validation Output: today we head we headed again to\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep an maemebr status rquired\n",
            "  Validation Output: rep and merp and meamer status r\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it snowed middlingh ard to ady\n",
            "  Validation Output: it snowled middling he middling h\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: they both wear leathe asndals\n",
            "  Validation Output: they both they both wear lead th\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  1.674, Seconds: 0.00\n",
            "No Improvement.\n",
            "\n",
            "Training Model: 3\n",
            "Epoch   3/100 Batch   30/14303 - Loss:  0.216, Seconds: 5.79\n",
            "Epoch   3/100 Batch   60/14303 - Loss:  0.107, Seconds: 5.62\n",
            "Epoch   3/100 Batch   90/14303 - Loss:  0.098, Seconds: 5.63\n",
            "Epoch   3/100 Batch  120/14303 - Loss:  0.104, Seconds: 5.65\n",
            "Epoch   3/100 Batch  150/14303 - Loss:  0.098, Seconds: 5.58\n",
            "Epoch   3/100 Batch  180/14303 - Loss:  0.096, Seconds: 5.62\n",
            "Epoch   3/100 Batch  210/14303 - Loss:  0.095, Seconds: 5.59\n",
            "Epoch   3/100 Batch  240/14303 - Loss:  0.092, Seconds: 5.61\n",
            "Epoch   3/100 Batch  270/14303 - Loss:  0.095, Seconds: 5.67\n",
            "Epoch   3/100 Batch  300/14303 - Loss:  0.125, Seconds: 5.61\n",
            "Epoch   3/100 Batch  330/14303 - Loss:  0.093, Seconds: 5.80\n",
            "Epoch   3/100 Batch  360/14303 - Loss:  0.091, Seconds: 5.82\n",
            "Epoch   3/100 Batch  390/14303 - Loss:  0.093, Seconds: 5.78\n",
            "Epoch   3/100 Batch  420/14303 - Loss:  0.089, Seconds: 5.81\n",
            "Epoch   3/100 Batch  450/14303 - Loss:  0.092, Seconds: 5.77\n",
            "Epoch   3/100 Batch  480/14303 - Loss:  0.088, Seconds: 5.77\n",
            "Epoch   3/100 Batch  510/14303 - Loss:  0.093, Seconds: 5.77\n",
            "Epoch   3/100 Batch  540/14303 - Loss:  0.096, Seconds: 5.80\n",
            "Epoch   3/100 Batch  570/14303 - Loss:  0.093, Seconds: 5.80\n",
            "Epoch   3/100 Batch  600/14303 - Loss:  0.096, Seconds: 5.75\n",
            "Epoch   3/100 Batch  630/14303 - Loss:  0.089, Seconds: 5.91\n",
            "Epoch   3/100 Batch  660/14303 - Loss:  0.090, Seconds: 5.93\n",
            "Epoch   3/100 Batch  690/14303 - Loss:  0.087, Seconds: 5.91\n",
            "Epoch   3/100 Batch  720/14303 - Loss:  0.086, Seconds: 6.06\n",
            "Epoch   3/100 Batch  750/14303 - Loss:  0.087, Seconds: 6.06\n",
            "Epoch   3/100 Batch  780/14303 - Loss:  0.091, Seconds: 5.93\n",
            "Epoch   3/100 Batch  810/14303 - Loss:  0.090, Seconds: 5.93\n",
            "Epoch   3/100 Batch  840/14303 - Loss:  0.082, Seconds: 6.04\n",
            "Epoch   3/100 Batch  870/14303 - Loss:  0.089, Seconds: 5.96\n",
            "Epoch   3/100 Batch  900/14303 - Loss:  0.081, Seconds: 5.91\n",
            "Epoch   3/100 Batch  930/14303 - Loss:  0.097, Seconds: 6.01\n",
            "Epoch   3/100 Batch  960/14303 - Loss:  0.089, Seconds: 6.11\n",
            "Epoch   3/100 Batch  990/14303 - Loss:  0.090, Seconds: 6.12\n",
            "Epoch   3/100 Batch 1020/14303 - Loss:  0.088, Seconds: 6.03\n",
            "Epoch   3/100 Batch 1050/14303 - Loss:  0.093, Seconds: 6.10\n",
            "Epoch   3/100 Batch 1080/14303 - Loss:  0.084, Seconds: 6.10\n",
            "Epoch   3/100 Batch 1110/14303 - Loss:  0.086, Seconds: 6.15\n",
            "Epoch   3/100 Batch 1140/14303 - Loss:  0.085, Seconds: 6.14\n",
            "Epoch   3/100 Batch 1170/14303 - Loss:  0.083, Seconds: 6.06\n",
            "Epoch   3/100 Batch 1200/14303 - Loss:  0.089, Seconds: 6.12\n",
            "Epoch   3/100 Batch 1230/14303 - Loss:  0.089, Seconds: 6.13\n",
            "Epoch   3/100 Batch 1260/14303 - Loss:  0.086, Seconds: 6.23\n",
            "Epoch   3/100 Batch 1290/14303 - Loss:  0.085, Seconds: 6.26\n",
            "Epoch   3/100 Batch 1320/14303 - Loss:  0.088, Seconds: 6.30\n",
            "Epoch   3/100 Batch 1350/14303 - Loss:  0.083, Seconds: 6.31\n",
            "Epoch   3/100 Batch 1380/14303 - Loss:  0.082, Seconds: 6.23\n",
            "Epoch   3/100 Batch 1410/14303 - Loss:  0.088, Seconds: 6.27\n",
            "Epoch   3/100 Batch 1440/14303 - Loss:  0.083, Seconds: 6.24\n",
            "Epoch   3/100 Batch 1470/14303 - Loss:  0.088, Seconds: 6.33\n",
            "Epoch   3/100 Batch 1500/14303 - Loss:  0.085, Seconds: 6.31\n",
            "Epoch   3/100 Batch 1530/14303 - Loss:  0.083, Seconds: 6.38\n",
            "Epoch   3/100 Batch 1560/14303 - Loss:  0.098, Seconds: 6.39\n",
            "Epoch   3/100 Batch 1590/14303 - Loss:  0.083, Seconds: 6.41\n",
            "Epoch   3/100 Batch 1620/14303 - Loss:  0.084, Seconds: 6.45\n",
            "Epoch   3/100 Batch 1650/14303 - Loss:  0.085, Seconds: 6.47\n",
            "Epoch   3/100 Batch 1680/14303 - Loss:  0.086, Seconds: 6.49\n",
            "Epoch   3/100 Batch 1710/14303 - Loss:  0.087, Seconds: 6.52\n",
            "Epoch   3/100 Batch 1740/14303 - Loss:  0.085, Seconds: 6.41\n",
            "Epoch   3/100 Batch 1770/14303 - Loss:  0.080, Seconds: 6.45\n",
            "Epoch   3/100 Batch 1800/14303 - Loss:  0.086, Seconds: 6.47\n",
            "Epoch   3/100 Batch 1830/14303 - Loss:  0.081, Seconds: 6.45\n",
            "Epoch   3/100 Batch 1860/14303 - Loss:  0.090, Seconds: 6.44\n",
            "Epoch   3/100 Batch 1890/14303 - Loss:  0.088, Seconds: 6.64\n",
            "Epoch   3/100 Batch 1920/14303 - Loss:  0.085, Seconds: 6.60\n",
            "Epoch   3/100 Batch 1950/14303 - Loss:  0.082, Seconds: 6.63\n",
            "Epoch   3/100 Batch 1980/14303 - Loss:  0.084, Seconds: 6.64\n",
            "Epoch   3/100 Batch 2010/14303 - Loss:  0.084, Seconds: 6.64\n",
            "Epoch   3/100 Batch 2040/14303 - Loss:  0.084, Seconds: 6.66\n",
            "Epoch   3/100 Batch 2070/14303 - Loss:  0.080, Seconds: 6.63\n",
            "Epoch   3/100 Batch 2100/14303 - Loss:  0.080, Seconds: 6.74\n",
            "Epoch   3/100 Batch 2130/14303 - Loss:  0.081, Seconds: 6.73\n",
            "Epoch   3/100 Batch 2160/14303 - Loss:  0.081, Seconds: 6.71\n",
            "Epoch   3/100 Batch 2190/14303 - Loss:  0.084, Seconds: 6.78\n",
            "Epoch   3/100 Batch 2220/14303 - Loss:  0.087, Seconds: 6.84\n",
            "Epoch   3/100 Batch 2250/14303 - Loss:  0.083, Seconds: 6.73\n",
            "Epoch   3/100 Batch 2280/14303 - Loss:  0.080, Seconds: 6.76\n",
            "Epoch   3/100 Batch 2310/14303 - Loss:  0.082, Seconds: 7.05\n",
            "Epoch   3/100 Batch 2340/14303 - Loss:  0.082, Seconds: 6.75\n",
            "Epoch   3/100 Batch 2370/14303 - Loss:  0.083, Seconds: 6.77\n",
            "Epoch   3/100 Batch 2400/14303 - Loss:  0.081, Seconds: 6.77\n",
            "Epoch   3/100 Batch 2430/14303 - Loss:  0.083, Seconds: 6.82\n",
            "Epoch   3/100 Batch 2460/14303 - Loss:  0.087, Seconds: 6.82\n",
            "Epoch   3/100 Batch 2490/14303 - Loss:  0.084, Seconds: 6.84\n",
            "Epoch   3/100 Batch 2520/14303 - Loss:  0.084, Seconds: 6.94\n",
            "Epoch   3/100 Batch 2550/14303 - Loss:  0.087, Seconds: 6.93\n",
            "Epoch   3/100 Batch 2580/14303 - Loss:  0.084, Seconds: 7.00\n",
            "Epoch   3/100 Batch 2610/14303 - Loss:  0.086, Seconds: 6.91\n",
            "Epoch   3/100 Batch 2640/14303 - Loss:  0.080, Seconds: 6.94\n",
            "Epoch   3/100 Batch 2670/14303 - Loss:  0.083, Seconds: 6.93\n",
            "Epoch   3/100 Batch 2700/14303 - Loss:  0.083, Seconds: 6.86\n",
            "Epoch   3/100 Batch 2730/14303 - Loss:  0.080, Seconds: 6.89\n",
            "Epoch   3/100 Batch 2760/14303 - Loss:  0.086, Seconds: 7.03\n",
            "Epoch   3/100 Batch 2790/14303 - Loss:  0.079, Seconds: 7.00\n",
            "Epoch   3/100 Batch 2820/14303 - Loss:  0.083, Seconds: 7.08\n",
            "Epoch   3/100 Batch 2850/14303 - Loss:  0.080, Seconds: 7.24\n",
            "Epoch   3/100 Batch 2880/14303 - Loss:  0.083, Seconds: 7.20\n",
            "Epoch   3/100 Batch 2910/14303 - Loss:  0.086, Seconds: 7.17\n",
            "Epoch   3/100 Batch 2940/14303 - Loss:  0.076, Seconds: 7.15\n",
            "Epoch   3/100 Batch 2970/14303 - Loss:  0.084, Seconds: 7.11\n",
            "Epoch   3/100 Batch 3000/14303 - Loss:  0.083, Seconds: 7.18\n",
            "Epoch   3/100 Batch 3030/14303 - Loss:  0.084, Seconds: 7.06\n",
            "Epoch   3/100 Batch 3060/14303 - Loss:  0.083, Seconds: 7.10\n",
            "Epoch   3/100 Batch 3090/14303 - Loss:  0.082, Seconds: 7.11\n",
            "Epoch   3/100 Batch 3120/14303 - Loss:  0.082, Seconds: 7.11\n",
            "Epoch   3/100 Batch 3150/14303 - Loss:  0.081, Seconds: 7.21\n",
            "Epoch   3/100 Batch 3180/14303 - Loss:  0.079, Seconds: 7.33\n",
            "Epoch   3/100 Batch 3210/14303 - Loss:  0.083, Seconds: 7.25\n",
            "Epoch   3/100 Batch 3240/14303 - Loss:  0.082, Seconds: 7.30\n",
            "Epoch   3/100 Batch 3270/14303 - Loss:  0.078, Seconds: 7.30\n",
            "Epoch   3/100 Batch 3300/14303 - Loss:  0.083, Seconds: 7.35\n",
            "Epoch   3/100 Batch 3330/14303 - Loss:  0.084, Seconds: 7.36\n",
            "Epoch   3/100 Batch 3360/14303 - Loss:  0.083, Seconds: 7.47\n",
            "Epoch   3/100 Batch 3390/14303 - Loss:  0.082, Seconds: 7.34\n",
            "Epoch   3/100 Batch 3420/14303 - Loss:  0.084, Seconds: 7.44\n",
            "Epoch   3/100 Batch 3450/14303 - Loss:  0.084, Seconds: 7.39\n",
            "Epoch   3/100 Batch 3480/14303 - Loss:  0.083, Seconds: 7.32\n",
            "Epoch   3/100 Batch 3510/14303 - Loss:  0.080, Seconds: 7.49\n",
            "Epoch   3/100 Batch 3540/14303 - Loss:  0.081, Seconds: 7.48\n",
            "Epoch   3/100 Batch 3570/14303 - Loss:  0.082, Seconds: 7.45\n",
            "Epoch   3/100 Batch 3600/14303 - Loss:  0.082, Seconds: 7.45\n",
            "Epoch   3/100 Batch 3630/14303 - Loss:  0.084, Seconds: 7.43\n",
            "Epoch   3/100 Batch 3660/14303 - Loss:  0.079, Seconds: 7.53\n",
            "Epoch   3/100 Batch 3690/14303 - Loss:  0.080, Seconds: 7.41\n",
            "Epoch   3/100 Batch 3720/14303 - Loss:  0.077, Seconds: 7.53\n",
            "Epoch   3/100 Batch 3750/14303 - Loss:  0.080, Seconds: 7.51\n",
            "Epoch   3/100 Batch 3780/14303 - Loss:  0.081, Seconds: 7.53\n",
            "Epoch   3/100 Batch 3810/14303 - Loss:  0.085, Seconds: 7.61\n",
            "Epoch   3/100 Batch 3840/14303 - Loss:  0.083, Seconds: 7.69\n",
            "Epoch   3/100 Batch 3870/14303 - Loss:  0.085, Seconds: 7.65\n",
            "Epoch   3/100 Batch 3900/14303 - Loss:  0.079, Seconds: 7.65\n",
            "Epoch   3/100 Batch 3930/14303 - Loss:  0.081, Seconds: 7.73\n",
            "Epoch   3/100 Batch 3960/14303 - Loss:  0.084, Seconds: 7.60\n",
            "Epoch   3/100 Batch 3990/14303 - Loss:  0.082, Seconds: 7.66\n",
            "Epoch   3/100 Batch 4020/14303 - Loss:  0.083, Seconds: 7.66\n",
            "Epoch   3/100 Batch 4050/14303 - Loss:  0.080, Seconds: 7.68\n",
            "Epoch   3/100 Batch 4080/14303 - Loss:  0.081, Seconds: 7.66\n",
            "Epoch   3/100 Batch 4110/14303 - Loss:  0.077, Seconds: 7.65\n",
            "Epoch   3/100 Batch 4140/14303 - Loss:  0.085, Seconds: 7.82\n",
            "Epoch   3/100 Batch 4170/14303 - Loss:  0.080, Seconds: 7.74\n",
            "Epoch   3/100 Batch 4200/14303 - Loss:  0.081, Seconds: 7.71\n",
            "Epoch   3/100 Batch 4230/14303 - Loss:  0.079, Seconds: 7.76\n",
            "Epoch   3/100 Batch 4260/14303 - Loss:  0.081, Seconds: 7.77\n",
            "Epoch   3/100 Batch 4290/14303 - Loss:  0.081, Seconds: 7.85\n",
            "Epoch   3/100 Batch 4320/14303 - Loss:  0.080, Seconds: 7.84\n",
            "Epoch   3/100 Batch 4350/14303 - Loss:  0.081, Seconds: 7.87\n",
            "Epoch   3/100 Batch 4380/14303 - Loss:  0.085, Seconds: 7.77\n",
            "Epoch   3/100 Batch 4410/14303 - Loss:  0.081, Seconds: 7.83\n",
            "Epoch   3/100 Batch 4440/14303 - Loss:  0.084, Seconds: 7.86\n",
            "Epoch   3/100 Batch 4470/14303 - Loss:  0.080, Seconds: 8.11\n",
            "Epoch   3/100 Batch 4500/14303 - Loss:  0.081, Seconds: 8.18\n",
            "Epoch   3/100 Batch 4530/14303 - Loss:  0.085, Seconds: 8.16\n",
            "Epoch   3/100 Batch 4560/14303 - Loss:  0.086, Seconds: 7.99\n",
            "Epoch   3/100 Batch 4590/14303 - Loss:  0.078, Seconds: 8.01\n",
            "Epoch   3/100 Batch 4620/14303 - Loss:  0.079, Seconds: 8.09\n",
            "Epoch   3/100 Batch 4650/14303 - Loss:  0.090, Seconds: 8.02\n",
            "Epoch   3/100 Batch 4680/14303 - Loss:  0.086, Seconds: 7.94\n",
            "Epoch   3/100 Batch 4710/14303 - Loss:  0.086, Seconds: 7.98\n",
            "Epoch   3/100 Batch 4740/14303 - Loss:  0.089, Seconds: 7.98\n",
            "Epoch   3/100 Batch 4770/14303 - Loss:  0.083, Seconds: 8.20\n",
            "Epoch   3/100 Batch 4800/14303 - Loss:  0.083, Seconds: 8.14\n",
            "Epoch   3/100 Batch 4830/14303 - Loss:  0.084, Seconds: 8.09\n",
            "Epoch   3/100 Batch 4860/14303 - Loss:  0.083, Seconds: 8.15\n",
            "Epoch   3/100 Batch 4890/14303 - Loss:  0.083, Seconds: 8.17\n",
            "Epoch   3/100 Batch 4920/14303 - Loss:  0.080, Seconds: 8.23\n",
            "Epoch   3/100 Batch 4950/14303 - Loss:  0.085, Seconds: 9.28\n",
            "Epoch   3/100 Batch 4980/14303 - Loss:  0.080, Seconds: 8.18\n",
            "Epoch   3/100 Batch 5010/14303 - Loss:  0.079, Seconds: 8.16\n",
            "Epoch   3/100 Batch 5040/14303 - Loss:  0.077, Seconds: 8.13\n",
            "Epoch   3/100 Batch 5070/14303 - Loss:  0.081, Seconds: 8.33\n",
            "Epoch   3/100 Batch 5100/14303 - Loss:  0.081, Seconds: 8.39\n",
            "Epoch   3/100 Batch 5130/14303 - Loss:  0.086, Seconds: 8.36\n",
            "Epoch   3/100 Batch 5160/14303 - Loss:  0.079, Seconds: 8.32\n",
            "Epoch   3/100 Batch 5190/14303 - Loss:  0.082, Seconds: 8.38\n",
            "Epoch   3/100 Batch 5220/14303 - Loss:  0.078, Seconds: 8.31\n",
            "Epoch   3/100 Batch 5250/14303 - Loss:  0.079, Seconds: 8.32\n",
            "Epoch   3/100 Batch 5280/14303 - Loss:  0.083, Seconds: 8.27\n",
            "Epoch   3/100 Batch 5310/14303 - Loss:  0.080, Seconds: 8.28\n",
            "Epoch   3/100 Batch 5340/14303 - Loss:  0.079, Seconds: 8.33\n",
            "Epoch   3/100 Batch 5370/14303 - Loss:  0.084, Seconds: 8.41\n",
            "Epoch   3/100 Batch 5400/14303 - Loss:  0.084, Seconds: 8.43\n",
            "Epoch   3/100 Batch 5430/14303 - Loss:  0.082, Seconds: 8.44\n",
            "Epoch   3/100 Batch 5460/14303 - Loss:  0.079, Seconds: 8.47\n",
            "Epoch   3/100 Batch 5490/14303 - Loss:  0.080, Seconds: 8.49\n",
            "Epoch   3/100 Batch 5520/14303 - Loss:  0.081, Seconds: 8.43\n",
            "Epoch   3/100 Batch 5550/14303 - Loss:  0.079, Seconds: 8.63\n",
            "Epoch   3/100 Batch 5580/14303 - Loss:  0.080, Seconds: 8.66\n",
            "Epoch   3/100 Batch 5610/14303 - Loss:  0.078, Seconds: 8.47\n",
            "Epoch   3/100 Batch 5640/14303 - Loss:  0.081, Seconds: 8.55\n",
            "Epoch   3/100 Batch 5670/14303 - Loss:  0.082, Seconds: 8.65\n",
            "Epoch   3/100 Batch 5700/14303 - Loss:  0.078, Seconds: 8.60\n",
            "Epoch   3/100 Batch 5730/14303 - Loss:  0.081, Seconds: 8.72\n",
            "Epoch   3/100 Batch 5760/14303 - Loss:  0.079, Seconds: 8.64\n",
            "Epoch   3/100 Batch 5790/14303 - Loss:  0.077, Seconds: 8.70\n",
            "Epoch   3/100 Batch 5820/14303 - Loss:  0.082, Seconds: 8.64\n",
            "Epoch   3/100 Batch 5850/14303 - Loss:  0.079, Seconds: 8.65\n",
            "Epoch   3/100 Batch 5880/14303 - Loss:  0.078, Seconds: 8.63\n",
            "Epoch   3/100 Batch 5910/14303 - Loss:  0.082, Seconds: 8.63\n",
            "Epoch   3/100 Batch 5940/14303 - Loss:  0.081, Seconds: 8.65\n",
            "Epoch   3/100 Batch 5970/14303 - Loss:  0.080, Seconds: 8.81\n",
            "Epoch   3/100 Batch 6000/14303 - Loss:  0.084, Seconds: 8.86\n",
            "Epoch   3/100 Batch 6030/14303 - Loss:  0.081, Seconds: 8.79\n",
            "Epoch   3/100 Batch 6060/14303 - Loss:  0.083, Seconds: 8.77\n",
            "Epoch   3/100 Batch 6090/14303 - Loss:  0.078, Seconds: 8.75\n",
            "Epoch   3/100 Batch 6120/14303 - Loss:  0.079, Seconds: 8.79\n",
            "Epoch   3/100 Batch 6150/14303 - Loss:  0.080, Seconds: 8.78\n",
            "Epoch   3/100 Batch 6180/14303 - Loss:  0.082, Seconds: 8.88\n",
            "Epoch   3/100 Batch 6210/14303 - Loss:  0.082, Seconds: 8.79\n",
            "Epoch   3/100 Batch 6240/14303 - Loss:  0.082, Seconds: 9.08\n",
            "Epoch   3/100 Batch 6270/14303 - Loss:  0.080, Seconds: 8.99\n",
            "Epoch   3/100 Batch 6300/14303 - Loss:  0.082, Seconds: 8.98\n",
            "Epoch   3/100 Batch 6330/14303 - Loss:  0.084, Seconds: 8.98\n",
            "Epoch   3/100 Batch 6360/14303 - Loss:  0.078, Seconds: 8.94\n",
            "Epoch   3/100 Batch 6390/14303 - Loss:  0.080, Seconds: 9.01\n",
            "Epoch   3/100 Batch 6420/14303 - Loss:  0.080, Seconds: 8.96\n",
            "Epoch   3/100 Batch 6450/14303 - Loss:  0.079, Seconds: 8.99\n",
            "Epoch   3/100 Batch 6480/14303 - Loss:  0.081, Seconds: 9.01\n",
            "Epoch   3/100 Batch 6510/14303 - Loss:  0.077, Seconds: 9.02\n",
            "Epoch   3/100 Batch 6540/14303 - Loss:  0.077, Seconds: 9.30\n",
            "Epoch   3/100 Batch 6570/14303 - Loss:  0.077, Seconds: 9.27\n",
            "Epoch   3/100 Batch 6600/14303 - Loss:  0.081, Seconds: 9.17\n",
            "Epoch   3/100 Batch 6630/14303 - Loss:  0.080, Seconds: 9.12\n",
            "Epoch   3/100 Batch 6660/14303 - Loss:  0.080, Seconds: 9.18\n",
            "Epoch   3/100 Batch 6690/14303 - Loss:  0.080, Seconds: 9.18\n",
            "Epoch   3/100 Batch 6720/14303 - Loss:  0.079, Seconds: 9.14\n",
            "Epoch   3/100 Batch 6750/14303 - Loss:  0.079, Seconds: 9.23\n",
            "Epoch   3/100 Batch 6780/14303 - Loss:  0.077, Seconds: 9.11\n",
            "Epoch   3/100 Batch 6810/14303 - Loss:  0.082, Seconds: 9.24\n",
            "Epoch   3/100 Batch 6840/14303 - Loss:  0.082, Seconds: 9.24\n",
            "Epoch   3/100 Batch 6870/14303 - Loss:  0.077, Seconds: 9.34\n",
            "Epoch   3/100 Batch 6900/14303 - Loss:  0.077, Seconds: 9.37\n",
            "Epoch   3/100 Batch 6930/14303 - Loss:  0.079, Seconds: 9.30\n",
            "Epoch   3/100 Batch 6960/14303 - Loss:  0.081, Seconds: 9.31\n",
            "Epoch   3/100 Batch 6990/14303 - Loss:  0.081, Seconds: 9.30\n",
            "Epoch   3/100 Batch 7020/14303 - Loss:  0.080, Seconds: 9.29\n",
            "Epoch   3/100 Batch 7050/14303 - Loss:  0.078, Seconds: 9.31\n",
            "Epoch   3/100 Batch 7080/14303 - Loss:  0.077, Seconds: 9.46\n",
            "Epoch   3/100 Batch 7110/14303 - Loss:  0.080, Seconds: 9.51\n",
            "Epoch   3/100 Batch 7140/14303 - Loss:  0.080, Seconds: 9.48\n",
            "Epoch   3/100 Batch 7170/14303 - Loss:  0.080, Seconds: 9.47\n",
            "Epoch   3/100 Batch 7200/14303 - Loss:  0.082, Seconds: 9.52\n",
            "Epoch   3/100 Batch 7230/14303 - Loss:  0.079, Seconds: 9.61\n",
            "Epoch   3/100 Batch 7260/14303 - Loss:  0.079, Seconds: 9.49\n",
            "Epoch   3/100 Batch 7290/14303 - Loss:  0.080, Seconds: 9.42\n",
            "Epoch   3/100 Batch 7320/14303 - Loss:  0.081, Seconds: 9.47\n",
            "Epoch   3/100 Batch 7350/14303 - Loss:  0.078, Seconds: 9.58\n",
            "Epoch   3/100 Batch 7380/14303 - Loss:  0.077, Seconds: 9.73\n",
            "Epoch   3/100 Batch 7410/14303 - Loss:  0.080, Seconds: 9.71\n",
            "Epoch   3/100 Batch 7440/14303 - Loss:  0.080, Seconds: 9.71\n",
            "Epoch   3/100 Batch 7470/14303 - Loss:  0.080, Seconds: 9.85\n",
            "Epoch   3/100 Batch 7500/14303 - Loss:  0.078, Seconds: 9.75\n",
            "Epoch   3/100 Batch 7530/14303 - Loss:  0.077, Seconds: 9.65\n",
            "Epoch   3/100 Batch 7560/14303 - Loss:  0.081, Seconds: 9.64\n",
            "Epoch   3/100 Batch 7590/14303 - Loss:  0.077, Seconds: 9.68\n",
            "Epoch   3/100 Batch 7620/14303 - Loss:  0.082, Seconds: 9.88\n",
            "Epoch   3/100 Batch 7650/14303 - Loss:  0.081, Seconds: 9.85\n",
            "Epoch   3/100 Batch 7680/14303 - Loss:  0.079, Seconds: 9.87\n",
            "Epoch   3/100 Batch 7710/14303 - Loss:  0.079, Seconds: 9.85\n",
            "Epoch   3/100 Batch 7740/14303 - Loss:  0.080, Seconds: 9.80\n",
            "Epoch   3/100 Batch 7770/14303 - Loss:  0.080, Seconds: 9.85\n",
            "Epoch   3/100 Batch 7800/14303 - Loss:  0.079, Seconds: 9.88\n",
            "Epoch   3/100 Batch 7830/14303 - Loss:  0.078, Seconds: 9.87\n",
            "Epoch   3/100 Batch 7860/14303 - Loss:  0.079, Seconds: 9.93\n",
            "Epoch   3/100 Batch 7890/14303 - Loss:  0.078, Seconds: 10.01\n",
            "Epoch   3/100 Batch 7920/14303 - Loss:  0.078, Seconds: 10.07\n",
            "Epoch   3/100 Batch 7950/14303 - Loss:  0.080, Seconds: 10.05\n",
            "Epoch   3/100 Batch 7980/14303 - Loss:  0.084, Seconds: 9.99\n",
            "Epoch   3/100 Batch 8010/14303 - Loss:  0.078, Seconds: 9.94\n",
            "Epoch   3/100 Batch 8040/14303 - Loss:  0.075, Seconds: 10.04\n",
            "Epoch   3/100 Batch 8070/14303 - Loss:  0.076, Seconds: 9.97\n",
            "Epoch   3/100 Batch 8100/14303 - Loss:  0.078, Seconds: 10.04\n",
            "Epoch   3/100 Batch 8130/14303 - Loss:  0.080, Seconds: 10.25\n",
            "Epoch   3/100 Batch 8160/14303 - Loss:  0.078, Seconds: 10.27\n",
            "Epoch   3/100 Batch 8190/14303 - Loss:  0.079, Seconds: 10.24\n",
            "Epoch   3/100 Batch 8220/14303 - Loss:  0.078, Seconds: 10.15\n",
            "Epoch   3/100 Batch 8250/14303 - Loss:  0.077, Seconds: 10.15\n",
            "Epoch   3/100 Batch 8280/14303 - Loss:  0.081, Seconds: 10.12\n",
            "Epoch   3/100 Batch 8310/14303 - Loss:  0.077, Seconds: 10.24\n",
            "Epoch   3/100 Batch 8340/14303 - Loss:  0.078, Seconds: 10.35\n",
            "Epoch   3/100 Batch 8370/14303 - Loss:  0.081, Seconds: 10.53\n",
            "Epoch   3/100 Batch 8400/14303 - Loss:  0.075, Seconds: 10.38\n",
            "Epoch   3/100 Batch 8430/14303 - Loss:  0.080, Seconds: 10.38\n",
            "Epoch   3/100 Batch 8460/14303 - Loss:  0.078, Seconds: 10.38\n",
            "Epoch   3/100 Batch 8490/14303 - Loss:  0.077, Seconds: 10.42\n",
            "Epoch   3/100 Batch 8520/14303 - Loss:  0.078, Seconds: 10.31\n",
            "Epoch   3/100 Batch 8550/14303 - Loss:  0.073, Seconds: 10.35\n",
            "Epoch   3/100 Batch 8580/14303 - Loss:  0.077, Seconds: 10.53\n",
            "Epoch   3/100 Batch 8610/14303 - Loss:  0.079, Seconds: 10.68\n",
            "Epoch   3/100 Batch 8640/14303 - Loss:  0.079, Seconds: 10.56\n",
            "Epoch   3/100 Batch 8670/14303 - Loss:  0.080, Seconds: 10.47\n",
            "Epoch   3/100 Batch 8700/14303 - Loss:  0.076, Seconds: 10.47\n",
            "Epoch   3/100 Batch 8730/14303 - Loss:  0.077, Seconds: 10.54\n",
            "Epoch   3/100 Batch 8760/14303 - Loss:  0.077, Seconds: 10.51\n",
            "Epoch   3/100 Batch 8790/14303 - Loss:  0.077, Seconds: 10.51\n",
            "Epoch   3/100 Batch 8820/14303 - Loss:  0.079, Seconds: 10.61\n",
            "Epoch   3/100 Batch 8850/14303 - Loss:  0.080, Seconds: 10.70\n",
            "Epoch   3/100 Batch 8880/14303 - Loss:  0.078, Seconds: 10.73\n",
            "Epoch   3/100 Batch 8910/14303 - Loss:  0.076, Seconds: 10.70\n",
            "Epoch   3/100 Batch 8940/14303 - Loss:  0.079, Seconds: 10.68\n",
            "Epoch   3/100 Batch 8970/14303 - Loss:  0.078, Seconds: 10.73\n",
            "Epoch   3/100 Batch 9000/14303 - Loss:  0.075, Seconds: 10.71\n",
            "Epoch   3/100 Batch 9030/14303 - Loss:  0.079, Seconds: 10.72\n",
            "Epoch   3/100 Batch 9060/14303 - Loss:  0.076, Seconds: 10.83\n",
            "Epoch   3/100 Batch 9090/14303 - Loss:  0.079, Seconds: 10.85\n",
            "Epoch   3/100 Batch 9120/14303 - Loss:  0.078, Seconds: 10.93\n",
            "Epoch   3/100 Batch 9150/14303 - Loss:  0.076, Seconds: 11.05\n",
            "Epoch   3/100 Batch 9180/14303 - Loss:  0.076, Seconds: 11.12\n",
            "Epoch   3/100 Batch 9210/14303 - Loss:  0.075, Seconds: 11.07\n",
            "Epoch   3/100 Batch 9240/14303 - Loss:  0.078, Seconds: 11.00\n",
            "Epoch   3/100 Batch 9270/14303 - Loss:  0.077, Seconds: 11.05\n",
            "Epoch   3/100 Batch 9300/14303 - Loss:  0.078, Seconds: 11.13\n",
            "Epoch   3/100 Batch 9330/14303 - Loss:  0.078, Seconds: 11.08\n",
            "Epoch   3/100 Batch 9360/14303 - Loss:  0.079, Seconds: 11.11\n",
            "Epoch   3/100 Batch 9390/14303 - Loss:  0.078, Seconds: 11.02\n",
            "Epoch   3/100 Batch 9420/14303 - Loss:  0.077, Seconds: 11.14\n",
            "Epoch   3/100 Batch 9450/14303 - Loss:  0.079, Seconds: 11.12\n",
            "Epoch   3/100 Batch 9480/14303 - Loss:  0.078, Seconds: 11.22\n",
            "Epoch   3/100 Batch 9510/14303 - Loss:  0.077, Seconds: 11.30\n",
            "Epoch   3/100 Batch 9540/14303 - Loss:  0.077, Seconds: 11.27\n",
            "Epoch   3/100 Batch 9570/14303 - Loss:  0.076, Seconds: 11.27\n",
            "Epoch   3/100 Batch 9600/14303 - Loss:  0.076, Seconds: 11.20\n",
            "Epoch   3/100 Batch 9630/14303 - Loss:  0.077, Seconds: 11.29\n",
            "Epoch   3/100 Batch 9660/14303 - Loss:  0.075, Seconds: 11.22\n",
            "Epoch   3/100 Batch 9690/14303 - Loss:  0.076, Seconds: 11.45\n",
            "Epoch   3/100 Batch 9720/14303 - Loss:  0.076, Seconds: 11.54\n",
            "Epoch   3/100 Batch 9750/14303 - Loss:  0.077, Seconds: 11.51\n",
            "Epoch   3/100 Batch 9780/14303 - Loss:  0.075, Seconds: 11.48\n",
            "Epoch   3/100 Batch 9810/14303 - Loss:  0.078, Seconds: 11.40\n",
            "Epoch   3/100 Batch 9840/14303 - Loss:  0.074, Seconds: 11.53\n",
            "Epoch   3/100 Batch 9870/14303 - Loss:  0.079, Seconds: 11.49\n",
            "Epoch   3/100 Batch 9900/14303 - Loss:  0.080, Seconds: 11.68\n",
            "Epoch   3/100 Batch 9930/14303 - Loss:  0.078, Seconds: 11.59\n",
            "Epoch   3/100 Batch 9960/14303 - Loss:  0.076, Seconds: 11.82\n",
            "Epoch   3/100 Batch 9990/14303 - Loss:  0.078, Seconds: 11.60\n",
            "Epoch   3/100 Batch 10020/14303 - Loss:  0.077, Seconds: 11.62\n",
            "Epoch   3/100 Batch 10050/14303 - Loss:  0.076, Seconds: 11.67\n",
            "Epoch   3/100 Batch 10080/14303 - Loss:  0.082, Seconds: 11.77\n",
            "Epoch   3/100 Batch 10110/14303 - Loss:  0.078, Seconds: 11.77\n",
            "Epoch   3/100 Batch 10140/14303 - Loss:  0.076, Seconds: 11.71\n",
            "Epoch   3/100 Batch 10170/14303 - Loss:  0.074, Seconds: 11.78\n",
            "Epoch   3/100 Batch 10200/14303 - Loss:  0.074, Seconds: 11.80\n",
            "Epoch   3/100 Batch 10230/14303 - Loss:  0.074, Seconds: 11.82\n",
            "Epoch   3/100 Batch 10260/14303 - Loss:  0.077, Seconds: 11.94\n",
            "Epoch   3/100 Batch 10290/14303 - Loss:  0.076, Seconds: 11.89\n",
            "Epoch   3/100 Batch 10320/14303 - Loss:  0.078, Seconds: 12.05\n",
            "Epoch   3/100 Batch 10350/14303 - Loss:  0.072, Seconds: 11.93\n",
            "Epoch   3/100 Batch 10380/14303 - Loss:  0.073, Seconds: 12.01\n",
            "Epoch   3/100 Batch 10410/14303 - Loss:  0.076, Seconds: 12.03\n",
            "Epoch   3/100 Batch 10440/14303 - Loss:  0.077, Seconds: 12.04\n",
            "Epoch   3/100 Batch 10470/14303 - Loss:  0.075, Seconds: 12.17\n",
            "Epoch   3/100 Batch 10500/14303 - Loss:  0.074, Seconds: 12.02\n",
            "Epoch   3/100 Batch 10530/14303 - Loss:  0.074, Seconds: 12.05\n",
            "Epoch   3/100 Batch 10560/14303 - Loss:  0.075, Seconds: 12.10\n",
            "Epoch   3/100 Batch 10590/14303 - Loss:  0.078, Seconds: 12.25\n",
            "Epoch   3/100 Batch 10620/14303 - Loss:  0.076, Seconds: 12.29\n",
            "Epoch   3/100 Batch 10650/14303 - Loss:  0.077, Seconds: 12.26\n",
            "Epoch   3/100 Batch 10680/14303 - Loss:  0.079, Seconds: 12.50\n",
            "Epoch   3/100 Batch 10710/14303 - Loss:  0.080, Seconds: 12.33\n",
            "Epoch   3/100 Batch 10740/14303 - Loss:  0.076, Seconds: 12.31\n",
            "Epoch   3/100 Batch 10770/14303 - Loss:  0.079, Seconds: 12.44\n",
            "Epoch   3/100 Batch 10800/14303 - Loss:  0.074, Seconds: 12.39\n",
            "Epoch   3/100 Batch 10830/14303 - Loss:  0.078, Seconds: 12.40\n",
            "Epoch   3/100 Batch 10860/14303 - Loss:  0.075, Seconds: 12.48\n",
            "Epoch   3/100 Batch 10890/14303 - Loss:  0.077, Seconds: 12.42\n",
            "Epoch   3/100 Batch 10920/14303 - Loss:  0.077, Seconds: 12.64\n",
            "Epoch   3/100 Batch 10950/14303 - Loss:  0.077, Seconds: 12.63\n",
            "Epoch   3/100 Batch 10980/14303 - Loss:  0.075, Seconds: 12.68\n",
            "Epoch   3/100 Batch 11010/14303 - Loss:  0.078, Seconds: 12.63\n",
            "Epoch   3/100 Batch 11040/14303 - Loss:  0.074, Seconds: 12.70\n",
            "Epoch   3/100 Batch 11070/14303 - Loss:  0.075, Seconds: 12.80\n",
            "Epoch   3/100 Batch 11100/14303 - Loss:  0.079, Seconds: 12.75\n",
            "Epoch   3/100 Batch 11130/14303 - Loss:  0.076, Seconds: 12.92\n",
            "Epoch   3/100 Batch 11160/14303 - Loss:  0.076, Seconds: 12.92\n",
            "Epoch   3/100 Batch 11190/14303 - Loss:  0.074, Seconds: 12.87\n",
            "Epoch   3/100 Batch 11220/14303 - Loss:  0.076, Seconds: 12.96\n",
            "Epoch   3/100 Batch 11250/14303 - Loss:  0.075, Seconds: 13.00\n",
            "Epoch   3/100 Batch 11280/14303 - Loss:  0.072, Seconds: 13.17\n",
            "Epoch   3/100 Batch 11310/14303 - Loss:  0.079, Seconds: 13.11\n",
            "Epoch   3/100 Batch 11340/14303 - Loss:  0.076, Seconds: 13.04\n",
            "Epoch   3/100 Batch 11370/14303 - Loss:  0.078, Seconds: 13.36\n",
            "Epoch   3/100 Batch 11400/14303 - Loss:  0.078, Seconds: 13.27\n",
            "Epoch   3/100 Batch 11430/14303 - Loss:  0.077, Seconds: 13.10\n",
            "Epoch   3/100 Batch 11460/14303 - Loss:  0.074, Seconds: 13.18\n",
            "Epoch   3/100 Batch 11490/14303 - Loss:  0.079, Seconds: 13.29\n",
            "Epoch   3/100 Batch 11520/14303 - Loss:  0.085, Seconds: 13.27\n",
            "Epoch   3/100 Batch 11550/14303 - Loss:  0.078, Seconds: 13.36\n",
            "Epoch   3/100 Batch 11580/14303 - Loss:  0.079, Seconds: 13.37\n",
            "Epoch   3/100 Batch 11610/14303 - Loss:  0.077, Seconds: 13.36\n",
            "Epoch   3/100 Batch 11640/14303 - Loss:  0.078, Seconds: 13.55\n",
            "Epoch   3/100 Batch 11670/14303 - Loss:  0.076, Seconds: 13.59\n",
            "Epoch   3/100 Batch 11700/14303 - Loss:  0.078, Seconds: 13.51\n",
            "Epoch   3/100 Batch 11730/14303 - Loss:  0.075, Seconds: 13.48\n",
            "Epoch   3/100 Batch 11760/14303 - Loss:  0.074, Seconds: 13.80\n",
            "Epoch   3/100 Batch 11790/14303 - Loss:  0.077, Seconds: 13.80\n",
            "Epoch   3/100 Batch 11820/14303 - Loss:  0.074, Seconds: 13.80\n",
            "Epoch   3/100 Batch 11850/14303 - Loss:  0.076, Seconds: 13.76\n",
            "Epoch   3/100 Batch 11880/14303 - Loss:  0.074, Seconds: 13.85\n",
            "Epoch   3/100 Batch 11910/14303 - Loss:  0.073, Seconds: 13.96\n",
            "Epoch   3/100 Batch 11940/14303 - Loss:  0.073, Seconds: 13.97\n",
            "Epoch   3/100 Batch 11970/14303 - Loss:  0.074, Seconds: 14.19\n",
            "Epoch   3/100 Batch 12000/14303 - Loss:  0.076, Seconds: 14.24\n",
            "Epoch   3/100 Batch 12030/14303 - Loss:  0.075, Seconds: 14.42\n",
            "Epoch   3/100 Batch 12060/14303 - Loss:  0.076, Seconds: 14.10\n",
            "Epoch   3/100 Batch 12090/14303 - Loss:  0.074, Seconds: 14.17\n",
            "Epoch   3/100 Batch 12120/14303 - Loss:  0.075, Seconds: 14.24\n",
            "Epoch   3/100 Batch 12150/14303 - Loss:  0.077, Seconds: 14.38\n",
            "Epoch   3/100 Batch 12180/14303 - Loss:  0.074, Seconds: 14.17\n",
            "Epoch   3/100 Batch 12210/14303 - Loss:  0.076, Seconds: 14.52\n",
            "Epoch   3/100 Batch 12240/14303 - Loss:  0.075, Seconds: 14.54\n",
            "Epoch   3/100 Batch 12270/14303 - Loss:  0.074, Seconds: 14.48\n",
            "Epoch   3/100 Batch 12300/14303 - Loss:  0.077, Seconds: 14.79\n",
            "Epoch   3/100 Batch 12330/14303 - Loss:  0.072, Seconds: 14.76\n",
            "Epoch   3/100 Batch 12360/14303 - Loss:  0.076, Seconds: 14.81\n",
            "Epoch   3/100 Batch 12390/14303 - Loss:  0.076, Seconds: 14.72\n",
            "Epoch   3/100 Batch 12420/14303 - Loss:  0.070, Seconds: 14.86\n",
            "Epoch   3/100 Batch 12450/14303 - Loss:  0.073, Seconds: 14.97\n",
            "Epoch   3/100 Batch 12480/14303 - Loss:  0.074, Seconds: 14.95\n",
            "Epoch   3/100 Batch 12510/14303 - Loss:  0.074, Seconds: 15.10\n",
            "Epoch   3/100 Batch 12540/14303 - Loss:  0.076, Seconds: 15.20\n",
            "Epoch   3/100 Batch 12570/14303 - Loss:  0.074, Seconds: 15.01\n",
            "Epoch   3/100 Batch 12600/14303 - Loss:  0.074, Seconds: 15.37\n",
            "Epoch   3/100 Batch 12630/14303 - Loss:  0.073, Seconds: 15.62\n",
            "Epoch   3/100 Batch 12660/14303 - Loss:  0.074, Seconds: 15.35\n",
            "Epoch   3/100 Batch 12690/14303 - Loss:  0.073, Seconds: 15.51\n",
            "Epoch   3/100 Batch 12720/14303 - Loss:  0.076, Seconds: 15.61\n",
            "Epoch   3/100 Batch 12750/14303 - Loss:  0.074, Seconds: 15.71\n",
            "Epoch   3/100 Batch 12780/14303 - Loss:  0.073, Seconds: 15.68\n",
            "Epoch   3/100 Batch 12810/14303 - Loss:  0.073, Seconds: 15.70\n",
            "Epoch   3/100 Batch 12840/14303 - Loss:  0.075, Seconds: 15.79\n",
            "Epoch   3/100 Batch 12870/14303 - Loss:  0.071, Seconds: 15.80\n",
            "Epoch   3/100 Batch 12900/14303 - Loss:  0.074, Seconds: 16.03\n",
            "Epoch   3/100 Batch 12930/14303 - Loss:  0.073, Seconds: 16.03\n",
            "Epoch   3/100 Batch 12960/14303 - Loss:  0.078, Seconds: 16.20\n",
            "Epoch   3/100 Batch 12990/14303 - Loss:  0.074, Seconds: 16.26\n",
            "Epoch   3/100 Batch 13020/14303 - Loss:  0.073, Seconds: 16.22\n",
            "Epoch   3/100 Batch 13050/14303 - Loss:  0.072, Seconds: 16.57\n",
            "Epoch   3/100 Batch 13080/14303 - Loss:  0.075, Seconds: 16.63\n",
            "Epoch   3/100 Batch 13110/14303 - Loss:  0.073, Seconds: 16.63\n",
            "Epoch   3/100 Batch 13140/14303 - Loss:  0.074, Seconds: 16.58\n",
            "Epoch   3/100 Batch 13170/14303 - Loss:  0.074, Seconds: 17.08\n",
            "Epoch   3/100 Batch 13200/14303 - Loss:  0.072, Seconds: 16.91\n",
            "Epoch   3/100 Batch 13230/14303 - Loss:  0.074, Seconds: 17.16\n",
            "Epoch   3/100 Batch 13260/14303 - Loss:  0.074, Seconds: 17.10\n",
            "Epoch   3/100 Batch 13290/14303 - Loss:  0.070, Seconds: 17.29\n",
            "Epoch   3/100 Batch 13320/14303 - Loss:  0.075, Seconds: 17.37\n",
            "Epoch   3/100 Batch 13350/14303 - Loss:  0.420, Seconds: 17.51\n",
            "Epoch   3/100 Batch 13380/14303 - Loss:  0.122, Seconds: 17.51\n",
            "Epoch   3/100 Batch 13410/14303 - Loss:  0.097, Seconds: 17.58\n",
            "Epoch   3/100 Batch 13440/14303 - Loss:  0.090, Seconds: 17.95\n",
            "Epoch   3/100 Batch 13470/14303 - Loss:  0.084, Seconds: 17.86\n",
            "Epoch   3/100 Batch 13500/14303 - Loss:  0.085, Seconds: 18.11\n",
            "Epoch   3/100 Batch 13530/14303 - Loss:  0.085, Seconds: 18.23\n",
            "Epoch   3/100 Batch 13560/14303 - Loss:  0.083, Seconds: 18.48\n",
            "Epoch   3/100 Batch 13590/14303 - Loss:  0.081, Seconds: 18.67\n",
            "Epoch   3/100 Batch 13620/14303 - Loss:  0.079, Seconds: 18.67\n",
            "Epoch   3/100 Batch 13650/14303 - Loss:  0.080, Seconds: 18.97\n",
            "Epoch   3/100 Batch 13680/14303 - Loss:  0.078, Seconds: 19.26\n",
            "Epoch   3/100 Batch 13710/14303 - Loss:  0.080, Seconds: 19.29\n",
            "Epoch   3/100 Batch 13740/14303 - Loss:  0.078, Seconds: 19.46\n",
            "Epoch   3/100 Batch 13770/14303 - Loss:  0.078, Seconds: 19.71\n",
            "Epoch   3/100 Batch 13800/14303 - Loss:  0.079, Seconds: 19.96\n",
            "Epoch   3/100 Batch 13830/14303 - Loss:  0.080, Seconds: 20.15\n",
            "Epoch   3/100 Batch 13860/14303 - Loss:  0.078, Seconds: 20.30\n",
            "Epoch   3/100 Batch 13890/14303 - Loss:  0.074, Seconds: 20.68\n",
            "Epoch   3/100 Batch 13920/14303 - Loss:  0.074, Seconds: 20.93\n",
            "Epoch   3/100 Batch 13950/14303 - Loss:  0.074, Seconds: 21.19\n",
            "Epoch   3/100 Batch 13980/14303 - Loss:  0.079, Seconds: 21.52\n",
            "Epoch   3/100 Batch 14010/14303 - Loss:  0.074, Seconds: 21.98\n",
            "Epoch   3/100 Batch 14040/14303 - Loss:  0.076, Seconds: 22.37\n",
            "Epoch   3/100 Batch 14070/14303 - Loss:  0.078, Seconds: 22.70\n",
            "Epoch   3/100 Batch 14100/14303 - Loss:  0.077, Seconds: 23.43\n",
            "Epoch   3/100 Batch 14130/14303 - Loss:  0.075, Seconds: 23.90\n",
            "Epoch   3/100 Batch 14160/14303 - Loss:  0.075, Seconds: 24.36\n",
            "Epoch   3/100 Batch 14190/14303 - Loss:  0.075, Seconds: 25.22\n",
            "Epoch   3/100 Batch 14220/14303 - Loss:  0.076, Seconds: 26.47\n",
            "Epoch   3/100 Batch 14250/14303 - Loss:  0.075, Seconds: 27.84\n",
            "Epoch   3/100 Batch 14280/14303 - Loss:  0.076, Seconds: 30.25\n",
            "  Validation Input: not as bad as noe sadhalr think\n",
            "  Validation Output: not as bad as one bad as one sad ha\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is also claley granzth sahib\n",
            "  Validation Output: it is also called granth saley gran\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is intended for adults only\n",
            "  Validation Output: it is intended for adults only\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: reason thingso ut for yourself\n",
            "  Validation Output: reason things out for yourself\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: i happened towaork quaite well\n",
            "  Validation Output: i happened to work qualite work qu\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: chef andi are doing very wello\n",
            "  Validation Output: chef and i are doing very well\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: centker for the performing art\n",
            "  Validation Output: center for the performing art\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: spoo nito a hot ol to serve\n",
            "  Validation Output: spoon into a hot oil to serve\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: people meet her and thye swoon\n",
            "  Validation Output: people meet her and they see there\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my poo litle head is boiliwng\n",
            "  Validation Output: my pool little head head his boi\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this had became a common scene\n",
            "  Validation Output: this had became a common scene\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: even went through th epecilas\n",
            "  Validation Output: even went through the specials\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: preheat a steamer on high heat\n",
            "  Validation Output: preheat a steamer on high heat\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: lol wll omeone hda to say it\n",
            "  Validation Output: lol will someone had to say it\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: the was weaving mats on the lts\n",
            "  Validation Output: they was weaving mats on the last o\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this is wrogn for manyg rasons\n",
            "  Validation Output: this is wrong for many reasons\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: today we headaed again toc uabo\n",
            "  Validation Output: today we headed again to day we hea\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep and member statusa requiredx\n",
            "  Validation Output: rep and member and member and member\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it snowd middling hardt o day\n",
            "  Validation Output: its now middling hard to middling\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: they obth wear leatqher sandals\n",
            "  Validation Output: they both wear leather leather leat\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  0.465, Seconds: 0.00\n",
            "New Record!\n",
            "\n",
            "Training Model: 4\n",
            "Epoch   4/100 Batch   30/14303 - Loss:  0.129, Seconds: 5.96\n",
            "Epoch   4/100 Batch   60/14303 - Loss:  0.092, Seconds: 5.79\n",
            "Epoch   4/100 Batch   90/14303 - Loss:  0.086, Seconds: 5.79\n",
            "Epoch   4/100 Batch  120/14303 - Loss:  0.084, Seconds: 5.77\n",
            "Epoch   4/100 Batch  150/14303 - Loss:  0.085, Seconds: 5.78\n",
            "Epoch   4/100 Batch  180/14303 - Loss:  0.084, Seconds: 5.80\n",
            "Epoch   4/100 Batch  210/14303 - Loss:  0.084, Seconds: 5.79\n",
            "Epoch   4/100 Batch  240/14303 - Loss:  0.081, Seconds: 5.79\n",
            "Epoch   4/100 Batch  270/14303 - Loss:  0.082, Seconds: 5.81\n",
            "Epoch   4/100 Batch  300/14303 - Loss:  0.109, Seconds: 5.79\n",
            "Epoch   4/100 Batch  330/14303 - Loss:  0.082, Seconds: 5.97\n",
            "Epoch   4/100 Batch  360/14303 - Loss:  0.079, Seconds: 5.94\n",
            "Epoch   4/100 Batch  390/14303 - Loss:  0.080, Seconds: 6.06\n",
            "Epoch   4/100 Batch  420/14303 - Loss:  0.083, Seconds: 6.00\n",
            "Epoch   4/100 Batch  450/14303 - Loss:  0.078, Seconds: 5.94\n",
            "Epoch   4/100 Batch  480/14303 - Loss:  0.081, Seconds: 5.97\n",
            "Epoch   4/100 Batch  510/14303 - Loss:  0.080, Seconds: 5.99\n",
            "Epoch   4/100 Batch  540/14303 - Loss:  0.074, Seconds: 6.01\n",
            "Epoch   4/100 Batch  570/14303 - Loss:  0.084, Seconds: 5.99\n",
            "Epoch   4/100 Batch  600/14303 - Loss:  0.090, Seconds: 5.93\n",
            "Epoch   4/100 Batch  630/14303 - Loss:  0.079, Seconds: 6.09\n",
            "Epoch   4/100 Batch  660/14303 - Loss:  0.079, Seconds: 6.11\n",
            "Epoch   4/100 Batch  690/14303 - Loss:  0.077, Seconds: 6.26\n",
            "Epoch   4/100 Batch  720/14303 - Loss:  0.078, Seconds: 6.33\n",
            "Epoch   4/100 Batch  750/14303 - Loss:  0.078, Seconds: 6.23\n",
            "Epoch   4/100 Batch  780/14303 - Loss:  0.079, Seconds: 6.11\n",
            "Epoch   4/100 Batch  810/14303 - Loss:  0.078, Seconds: 5.88\n",
            "Epoch   4/100 Batch  840/14303 - Loss:  0.079, Seconds: 5.95\n",
            "Epoch   4/100 Batch  870/14303 - Loss:  0.080, Seconds: 5.94\n",
            "Epoch   4/100 Batch  900/14303 - Loss:  0.074, Seconds: 5.95\n",
            "Epoch   4/100 Batch  930/14303 - Loss:  0.088, Seconds: 6.03\n",
            "Epoch   4/100 Batch  960/14303 - Loss:  0.081, Seconds: 6.14\n",
            "Epoch   4/100 Batch  990/14303 - Loss:  0.081, Seconds: 6.19\n",
            "Epoch   4/100 Batch 1020/14303 - Loss:  0.078, Seconds: 6.10\n",
            "Epoch   4/100 Batch 1050/14303 - Loss:  0.081, Seconds: 6.18\n",
            "Epoch   4/100 Batch 1080/14303 - Loss:  0.075, Seconds: 6.19\n",
            "Epoch   4/100 Batch 1110/14303 - Loss:  0.074, Seconds: 6.09\n",
            "Epoch   4/100 Batch 1140/14303 - Loss:  0.079, Seconds: 6.07\n",
            "Epoch   4/100 Batch 1170/14303 - Loss:  0.079, Seconds: 6.09\n",
            "Epoch   4/100 Batch 1200/14303 - Loss:  0.077, Seconds: 6.14\n",
            "Epoch   4/100 Batch 1230/14303 - Loss:  0.080, Seconds: 6.14\n",
            "Epoch   4/100 Batch 1260/14303 - Loss:  0.074, Seconds: 6.37\n",
            "Epoch   4/100 Batch 1290/14303 - Loss:  0.077, Seconds: 6.32\n",
            "Epoch   4/100 Batch 1320/14303 - Loss:  0.078, Seconds: 6.35\n",
            "Epoch   4/100 Batch 1350/14303 - Loss:  0.078, Seconds: 6.24\n",
            "Epoch   4/100 Batch 1380/14303 - Loss:  0.074, Seconds: 6.22\n",
            "Epoch   4/100 Batch 1410/14303 - Loss:  0.080, Seconds: 6.24\n",
            "Epoch   4/100 Batch 1440/14303 - Loss:  0.080, Seconds: 6.26\n",
            "Epoch   4/100 Batch 1470/14303 - Loss:  0.076, Seconds: 6.23\n",
            "Epoch   4/100 Batch 1500/14303 - Loss:  0.073, Seconds: 6.22\n",
            "Epoch   4/100 Batch 1530/14303 - Loss:  0.073, Seconds: 6.23\n",
            "Epoch   4/100 Batch 1560/14303 - Loss:  0.091, Seconds: 6.40\n",
            "Epoch   4/100 Batch 1590/14303 - Loss:  0.077, Seconds: 6.38\n",
            "Epoch   4/100 Batch 1620/14303 - Loss:  0.079, Seconds: 6.39\n",
            "Epoch   4/100 Batch 1650/14303 - Loss:  0.076, Seconds: 6.45\n",
            "Epoch   4/100 Batch 1680/14303 - Loss:  0.075, Seconds: 6.45\n",
            "Epoch   4/100 Batch 1710/14303 - Loss:  0.074, Seconds: 6.39\n",
            "Epoch   4/100 Batch 1740/14303 - Loss:  0.071, Seconds: 6.41\n",
            "Epoch   4/100 Batch 1770/14303 - Loss:  0.076, Seconds: 6.53\n",
            "Epoch   4/100 Batch 1800/14303 - Loss:  0.078, Seconds: 6.48\n",
            "Epoch   4/100 Batch 1830/14303 - Loss:  0.071, Seconds: 6.45\n",
            "Epoch   4/100 Batch 1860/14303 - Loss:  0.078, Seconds: 6.42\n",
            "Epoch   4/100 Batch 1890/14303 - Loss:  0.081, Seconds: 6.56\n",
            "Epoch   4/100 Batch 1920/14303 - Loss:  0.076, Seconds: 6.69\n",
            "Epoch   4/100 Batch 1950/14303 - Loss:  0.075, Seconds: 6.56\n",
            "Epoch   4/100 Batch 1980/14303 - Loss:  0.076, Seconds: 6.66\n",
            "Epoch   4/100 Batch 2010/14303 - Loss:  0.074, Seconds: 6.63\n",
            "Epoch   4/100 Batch 2040/14303 - Loss:  0.074, Seconds: 6.62\n",
            "Epoch   4/100 Batch 2070/14303 - Loss:  0.074, Seconds: 6.62\n",
            "Epoch   4/100 Batch 2100/14303 - Loss:  0.074, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2130/14303 - Loss:  0.074, Seconds: 6.68\n",
            "Epoch   4/100 Batch 2160/14303 - Loss:  0.074, Seconds: 6.51\n",
            "Epoch   4/100 Batch 2190/14303 - Loss:  0.080, Seconds: 6.53\n",
            "Epoch   4/100 Batch 2220/14303 - Loss:  0.072, Seconds: 6.81\n",
            "Epoch   4/100 Batch 2250/14303 - Loss:  0.075, Seconds: 6.75\n",
            "Epoch   4/100 Batch 2280/14303 - Loss:  0.071, Seconds: 6.75\n",
            "Epoch   4/100 Batch 2310/14303 - Loss:  0.076, Seconds: 6.79\n",
            "Epoch   4/100 Batch 2340/14303 - Loss:  0.077, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2370/14303 - Loss:  0.074, Seconds: 6.89\n",
            "Epoch   4/100 Batch 2400/14303 - Loss:  0.074, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2430/14303 - Loss:  0.073, Seconds: 6.75\n",
            "Epoch   4/100 Batch 2460/14303 - Loss:  0.076, Seconds: 6.71\n",
            "Epoch   4/100 Batch 2490/14303 - Loss:  0.076, Seconds: 6.76\n",
            "Epoch   4/100 Batch 2520/14303 - Loss:  0.077, Seconds: 6.82\n",
            "Epoch   4/100 Batch 2550/14303 - Loss:  0.081, Seconds: 6.90\n",
            "Epoch   4/100 Batch 2580/14303 - Loss:  0.074, Seconds: 7.00\n",
            "Epoch   4/100 Batch 2610/14303 - Loss:  0.080, Seconds: 6.95\n",
            "Epoch   4/100 Batch 2640/14303 - Loss:  0.074, Seconds: 6.91\n",
            "Epoch   4/100 Batch 2670/14303 - Loss:  0.078, Seconds: 7.05\n",
            "Epoch   4/100 Batch 2700/14303 - Loss:  0.077, Seconds: 7.03\n",
            "Epoch   4/100 Batch 2730/14303 - Loss:  0.075, Seconds: 6.98\n",
            "Epoch   4/100 Batch 2760/14303 - Loss:  0.074, Seconds: 6.98\n",
            "Epoch   4/100 Batch 2790/14303 - Loss:  0.075, Seconds: 6.95\n",
            "Epoch   4/100 Batch 2820/14303 - Loss:  0.074, Seconds: 6.95\n",
            "Epoch   4/100 Batch 2850/14303 - Loss:  0.083, Seconds: 7.08\n",
            "Epoch   4/100 Batch 2880/14303 - Loss:  0.069, Seconds: 7.11\n",
            "Epoch   4/100 Batch 2910/14303 - Loss:  0.074, Seconds: 7.13\n",
            "Epoch   4/100 Batch 2940/14303 - Loss:  0.069, Seconds: 7.02\n",
            "Epoch   4/100 Batch 2970/14303 - Loss:  0.071, Seconds: 7.12\n",
            "Epoch   4/100 Batch 3000/14303 - Loss:  0.074, Seconds: 7.14\n",
            "Epoch   4/100 Batch 3030/14303 - Loss:  0.076, Seconds: 7.24\n",
            "Epoch   4/100 Batch 3060/14303 - Loss:  0.073, Seconds: 7.21\n",
            "Epoch   4/100 Batch 3090/14303 - Loss:  0.070, Seconds: 7.12\n",
            "Epoch   4/100 Batch 3120/14303 - Loss:  0.070, Seconds: 7.18\n",
            "Epoch   4/100 Batch 3150/14303 - Loss:  0.073, Seconds: 7.17\n",
            "Epoch   4/100 Batch 3180/14303 - Loss:  0.080, Seconds: 7.37\n",
            "Epoch   4/100 Batch 3210/14303 - Loss:  0.074, Seconds: 7.37\n",
            "Epoch   4/100 Batch 3240/14303 - Loss:  0.073, Seconds: 7.26\n",
            "Epoch   4/100 Batch 3270/14303 - Loss:  0.074, Seconds: 7.27\n",
            "Epoch   4/100 Batch 3300/14303 - Loss:  0.075, Seconds: 7.34\n",
            "Epoch   4/100 Batch 3330/14303 - Loss:  0.073, Seconds: 7.44\n",
            "Epoch   4/100 Batch 3360/14303 - Loss:  0.075, Seconds: 7.44\n",
            "Epoch   4/100 Batch 3390/14303 - Loss:  0.074, Seconds: 7.20\n",
            "Epoch   4/100 Batch 3420/14303 - Loss:  0.077, Seconds: 7.25\n",
            "Epoch   4/100 Batch 3450/14303 - Loss:  0.071, Seconds: 7.21\n",
            "Epoch   4/100 Batch 3480/14303 - Loss:  0.077, Seconds: 7.27\n",
            "Epoch   4/100 Batch 3510/14303 - Loss:  0.074, Seconds: 7.48\n",
            "Epoch   4/100 Batch 3540/14303 - Loss:  0.074, Seconds: 7.50\n",
            "Epoch   4/100 Batch 3570/14303 - Loss:  0.076, Seconds: 7.48\n",
            "Epoch   4/100 Batch 3600/14303 - Loss:  0.074, Seconds: 7.54\n",
            "Epoch   4/100 Batch 3630/14303 - Loss:  0.076, Seconds: 7.60\n",
            "Epoch   4/100 Batch 3660/14303 - Loss:  0.071, Seconds: 7.52\n",
            "Epoch   4/100 Batch 3690/14303 - Loss:  0.071, Seconds: 7.46\n",
            "Epoch   4/100 Batch 3720/14303 - Loss:  0.070, Seconds: 7.50\n",
            "Epoch   4/100 Batch 3750/14303 - Loss:  0.072, Seconds: 7.50\n",
            "Epoch   4/100 Batch 3780/14303 - Loss:  0.073, Seconds: 7.54\n",
            "Epoch   4/100 Batch 3810/14303 - Loss:  0.076, Seconds: 7.64\n",
            "Epoch   4/100 Batch 3840/14303 - Loss:  0.072, Seconds: 7.66\n",
            "Epoch   4/100 Batch 3870/14303 - Loss:  0.074, Seconds: 7.68\n",
            "Epoch   4/100 Batch 3900/14303 - Loss:  0.073, Seconds: 7.59\n",
            "Epoch   4/100 Batch 3930/14303 - Loss:  0.074, Seconds: 7.69\n",
            "Epoch   4/100 Batch 3960/14303 - Loss:  0.075, Seconds: 7.70\n",
            "Epoch   4/100 Batch 3990/14303 - Loss:  0.075, Seconds: 7.65\n",
            "Epoch   4/100 Batch 4020/14303 - Loss:  0.073, Seconds: 7.61\n",
            "Epoch   4/100 Batch 4050/14303 - Loss:  0.069, Seconds: 7.73\n",
            "Epoch   4/100 Batch 4080/14303 - Loss:  0.089, Seconds: 7.65\n",
            "Epoch   4/100 Batch 4110/14303 - Loss:  0.089, Seconds: 7.70\n",
            "Epoch   4/100 Batch 4140/14303 - Loss:  0.093, Seconds: 7.83\n",
            "Epoch   4/100 Batch 4170/14303 - Loss:  0.078, Seconds: 7.87\n",
            "Epoch   4/100 Batch 4200/14303 - Loss:  0.074, Seconds: 7.87\n",
            "Epoch   4/100 Batch 4230/14303 - Loss:  0.082, Seconds: 7.89\n",
            "Epoch   4/100 Batch 4260/14303 - Loss:  0.081, Seconds: 7.74\n",
            "Epoch   4/100 Batch 4290/14303 - Loss:  0.078, Seconds: 7.75\n",
            "Epoch   4/100 Batch 4320/14303 - Loss:  0.074, Seconds: 7.78\n",
            "Epoch   4/100 Batch 4350/14303 - Loss:  0.072, Seconds: 7.76\n",
            "Epoch   4/100 Batch 4380/14303 - Loss:  0.075, Seconds: 7.87\n",
            "Epoch   4/100 Batch 4410/14303 - Loss:  0.073, Seconds: 7.82\n",
            "Epoch   4/100 Batch 4440/14303 - Loss:  0.081, Seconds: 7.79\n",
            "Epoch   4/100 Batch 4470/14303 - Loss:  0.076, Seconds: 8.05\n",
            "Epoch   4/100 Batch 4500/14303 - Loss:  0.075, Seconds: 8.13\n",
            "Epoch   4/100 Batch 4530/14303 - Loss:  0.073, Seconds: 7.96\n",
            "Epoch   4/100 Batch 4560/14303 - Loss:  0.075, Seconds: 7.87\n",
            "Epoch   4/100 Batch 4590/14303 - Loss:  0.072, Seconds: 7.89\n",
            "Epoch   4/100 Batch 4620/14303 - Loss:  0.072, Seconds: 7.87\n",
            "Epoch   4/100 Batch 4650/14303 - Loss:  0.073, Seconds: 7.85\n",
            "Epoch   4/100 Batch 4680/14303 - Loss:  0.073, Seconds: 7.90\n",
            "Epoch   4/100 Batch 4710/14303 - Loss:  0.075, Seconds: 7.93\n",
            "Epoch   4/100 Batch 4740/14303 - Loss:  0.076, Seconds: 8.06\n",
            "Epoch   4/100 Batch 4770/14303 - Loss:  0.072, Seconds: 8.08\n",
            "Epoch   4/100 Batch 4800/14303 - Loss:  0.073, Seconds: 8.22\n",
            "Epoch   4/100 Batch 4830/14303 - Loss:  0.072, Seconds: 8.04\n",
            "Epoch   4/100 Batch 4860/14303 - Loss:  0.071, Seconds: 8.14\n",
            "Epoch   4/100 Batch 4890/14303 - Loss:  0.072, Seconds: 8.01\n",
            "Epoch   4/100 Batch 4920/14303 - Loss:  0.071, Seconds: 8.09\n",
            "Epoch   4/100 Batch 4950/14303 - Loss:  0.076, Seconds: 8.13\n",
            "Epoch   4/100 Batch 4980/14303 - Loss:  0.073, Seconds: 8.06\n",
            "Epoch   4/100 Batch 5010/14303 - Loss:  0.073, Seconds: 7.99\n",
            "Epoch   4/100 Batch 5040/14303 - Loss:  0.073, Seconds: 8.05\n",
            "Epoch   4/100 Batch 5070/14303 - Loss:  0.076, Seconds: 8.32\n",
            "Epoch   4/100 Batch 5100/14303 - Loss:  0.071, Seconds: 8.22\n",
            "Epoch   4/100 Batch 5130/14303 - Loss:  0.076, Seconds: 8.22\n",
            "Epoch   4/100 Batch 5160/14303 - Loss:  0.072, Seconds: 8.25\n",
            "Epoch   4/100 Batch 5190/14303 - Loss:  0.072, Seconds: 8.21\n",
            "Epoch   4/100 Batch 5220/14303 - Loss:  0.072, Seconds: 8.30\n",
            "Epoch   4/100 Batch 5250/14303 - Loss:  0.071, Seconds: 8.43\n",
            "Epoch   4/100 Batch 5280/14303 - Loss:  0.073, Seconds: 8.26\n",
            "Epoch   4/100 Batch 5310/14303 - Loss:  0.070, Seconds: 8.27\n",
            "Epoch   4/100 Batch 5340/14303 - Loss:  0.070, Seconds: 8.32\n",
            "Epoch   4/100 Batch 5370/14303 - Loss:  0.074, Seconds: 8.43\n",
            "Epoch   4/100 Batch 5400/14303 - Loss:  0.077, Seconds: 8.51\n",
            "Epoch   4/100 Batch 5430/14303 - Loss:  0.072, Seconds: 8.52\n",
            "Epoch   4/100 Batch 5460/14303 - Loss:  0.072, Seconds: 8.38\n",
            "Epoch   4/100 Batch 5490/14303 - Loss:  0.072, Seconds: 8.47\n",
            "Epoch   4/100 Batch 5520/14303 - Loss:  0.079, Seconds: 8.49\n",
            "Epoch   4/100 Batch 5550/14303 - Loss:  0.074, Seconds: 8.68\n",
            "Epoch   4/100 Batch 5580/14303 - Loss:  0.074, Seconds: 8.41\n",
            "Epoch   4/100 Batch 5610/14303 - Loss:  0.072, Seconds: 8.50\n",
            "Epoch   4/100 Batch 5640/14303 - Loss:  0.073, Seconds: 8.43\n",
            "Epoch   4/100 Batch 5670/14303 - Loss:  0.074, Seconds: 8.58\n",
            "Epoch   4/100 Batch 5700/14303 - Loss:  0.070, Seconds: 8.59\n",
            "Epoch   4/100 Batch 5730/14303 - Loss:  0.074, Seconds: 8.68\n",
            "Epoch   4/100 Batch 5760/14303 - Loss:  0.070, Seconds: 8.66\n",
            "Epoch   4/100 Batch 5790/14303 - Loss:  0.083, Seconds: 8.65\n",
            "Epoch   4/100 Batch 5820/14303 - Loss:  0.084, Seconds: 8.78\n",
            "Epoch   4/100 Batch 5850/14303 - Loss:  0.075, Seconds: 8.73\n",
            "Epoch   4/100 Batch 5880/14303 - Loss:  0.077, Seconds: 8.75\n",
            "Epoch   4/100 Batch 5910/14303 - Loss:  0.078, Seconds: 8.63\n",
            "Epoch   4/100 Batch 5940/14303 - Loss:  0.071, Seconds: 8.69\n",
            "Epoch   4/100 Batch 5970/14303 - Loss:  0.074, Seconds: 8.84\n",
            "Epoch   4/100 Batch 6000/14303 - Loss:  0.076, Seconds: 8.78\n",
            "Epoch   4/100 Batch 6030/14303 - Loss:  0.072, Seconds: 8.80\n",
            "Epoch   4/100 Batch 6060/14303 - Loss:  0.076, Seconds: 8.82\n",
            "Epoch   4/100 Batch 6090/14303 - Loss:  0.071, Seconds: 8.83\n",
            "Epoch   4/100 Batch 6120/14303 - Loss:  0.077, Seconds: 8.98\n",
            "Epoch   4/100 Batch 6150/14303 - Loss:  0.071, Seconds: 8.88\n",
            "Epoch   4/100 Batch 6180/14303 - Loss:  0.074, Seconds: 8.80\n",
            "Epoch   4/100 Batch 6210/14303 - Loss:  0.071, Seconds: 8.79\n",
            "Epoch   4/100 Batch 6240/14303 - Loss:  0.073, Seconds: 8.89\n",
            "Epoch   4/100 Batch 6270/14303 - Loss:  0.073, Seconds: 8.89\n",
            "Epoch   4/100 Batch 6300/14303 - Loss:  0.073, Seconds: 8.87\n",
            "Epoch   4/100 Batch 6330/14303 - Loss:  0.073, Seconds: 8.98\n",
            "Epoch   4/100 Batch 6360/14303 - Loss:  0.070, Seconds: 9.01\n",
            "Epoch   4/100 Batch 6390/14303 - Loss:  0.073, Seconds: 8.98\n",
            "Epoch   4/100 Batch 6420/14303 - Loss:  0.073, Seconds: 8.97\n",
            "Epoch   4/100 Batch 6450/14303 - Loss:  0.070, Seconds: 8.98\n",
            "Epoch   4/100 Batch 6480/14303 - Loss:  0.073, Seconds: 9.01\n",
            "Epoch   4/100 Batch 6510/14303 - Loss:  0.071, Seconds: 9.10\n",
            "Epoch   4/100 Batch 6540/14303 - Loss:  0.070, Seconds: 9.36\n",
            "Epoch   4/100 Batch 6570/14303 - Loss:  0.070, Seconds: 9.17\n",
            "Epoch   4/100 Batch 6600/14303 - Loss:  0.070, Seconds: 9.24\n",
            "Epoch   4/100 Batch 6630/14303 - Loss:  0.074, Seconds: 9.30\n",
            "Epoch   4/100 Batch 6660/14303 - Loss:  0.076, Seconds: 9.21\n",
            "Epoch   4/100 Batch 6690/14303 - Loss:  0.073, Seconds: 9.36\n",
            "Epoch   4/100 Batch 6720/14303 - Loss:  0.072, Seconds: 9.23\n",
            "Epoch   4/100 Batch 6750/14303 - Loss:  0.074, Seconds: 9.33\n",
            "Epoch   4/100 Batch 6780/14303 - Loss:  0.069, Seconds: 9.23\n",
            "Epoch   4/100 Batch 6810/14303 - Loss:  0.071, Seconds: 9.33\n",
            "Epoch   4/100 Batch 6840/14303 - Loss:  0.072, Seconds: 9.51\n",
            "Epoch   4/100 Batch 6870/14303 - Loss:  0.071, Seconds: 9.58\n",
            "Epoch   4/100 Batch 6900/14303 - Loss:  0.072, Seconds: 9.47\n",
            "Epoch   4/100 Batch 6930/14303 - Loss:  0.072, Seconds: 9.50\n",
            "Epoch   4/100 Batch 6960/14303 - Loss:  0.075, Seconds: 9.50\n",
            "Epoch   4/100 Batch 6990/14303 - Loss:  0.074, Seconds: 9.54\n",
            "Epoch   4/100 Batch 7020/14303 - Loss:  0.073, Seconds: 9.50\n",
            "Epoch   4/100 Batch 7050/14303 - Loss:  0.072, Seconds: 9.58\n",
            "Epoch   4/100 Batch 7080/14303 - Loss:  0.073, Seconds: 9.65\n",
            "Epoch   4/100 Batch 7110/14303 - Loss:  0.071, Seconds: 9.79\n",
            "Epoch   4/100 Batch 7140/14303 - Loss:  0.073, Seconds: 9.74\n",
            "Epoch   4/100 Batch 7170/14303 - Loss:  0.073, Seconds: 9.65\n",
            "Epoch   4/100 Batch 7200/14303 - Loss:  0.072, Seconds: 9.66\n",
            "Epoch   4/100 Batch 7230/14303 - Loss:  0.073, Seconds: 9.69\n",
            "Epoch   4/100 Batch 7260/14303 - Loss:  0.068, Seconds: 9.65\n",
            "Epoch   4/100 Batch 7290/14303 - Loss:  0.072, Seconds: 9.75\n",
            "Epoch   4/100 Batch 7320/14303 - Loss:  0.074, Seconds: 9.76\n",
            "Epoch   4/100 Batch 7350/14303 - Loss:  0.074, Seconds: 9.77\n",
            "Epoch   4/100 Batch 7380/14303 - Loss:  0.072, Seconds: 9.88\n",
            "Epoch   4/100 Batch 7410/14303 - Loss:  0.073, Seconds: 9.95\n",
            "Epoch   4/100 Batch 7440/14303 - Loss:  0.074, Seconds: 10.11\n",
            "Epoch   4/100 Batch 7470/14303 - Loss:  0.074, Seconds: 9.87\n",
            "Epoch   4/100 Batch 7500/14303 - Loss:  0.070, Seconds: 9.85\n",
            "Epoch   4/100 Batch 7530/14303 - Loss:  0.069, Seconds: 9.87\n",
            "Epoch   4/100 Batch 7560/14303 - Loss:  0.071, Seconds: 10.02\n",
            "Epoch   4/100 Batch 7590/14303 - Loss:  0.072, Seconds: 9.85\n",
            "Epoch   4/100 Batch 7620/14303 - Loss:  0.073, Seconds: 9.97\n",
            "Epoch   4/100 Batch 7650/14303 - Loss:  0.069, Seconds: 10.02\n",
            "Epoch   4/100 Batch 7680/14303 - Loss:  0.071, Seconds: 10.01\n",
            "Epoch   4/100 Batch 7710/14303 - Loss:  0.072, Seconds: 10.00\n",
            "Epoch   4/100 Batch 7740/14303 - Loss:  0.075, Seconds: 9.99\n",
            "Epoch   4/100 Batch 7770/14303 - Loss:  0.071, Seconds: 10.13\n",
            "Epoch   4/100 Batch 7800/14303 - Loss:  0.069, Seconds: 10.04\n",
            "Epoch   4/100 Batch 7830/14303 - Loss:  0.071, Seconds: 10.03\n",
            "Epoch   4/100 Batch 7860/14303 - Loss:  0.068, Seconds: 10.07\n",
            "Epoch   4/100 Batch 7890/14303 - Loss:  0.071, Seconds: 10.31\n",
            "Epoch   4/100 Batch 7920/14303 - Loss:  0.069, Seconds: 10.30\n",
            "Epoch   4/100 Batch 7950/14303 - Loss:  0.073, Seconds: 10.28\n",
            "Epoch   4/100 Batch 7980/14303 - Loss:  0.071, Seconds: 10.24\n",
            "Epoch   4/100 Batch 8010/14303 - Loss:  0.072, Seconds: 10.37\n",
            "Epoch   4/100 Batch 8040/14303 - Loss:  0.070, Seconds: 10.26\n",
            "Epoch   4/100 Batch 8070/14303 - Loss:  0.069, Seconds: 10.28\n",
            "Epoch   4/100 Batch 8100/14303 - Loss:  0.071, Seconds: 10.16\n",
            "Epoch   4/100 Batch 8130/14303 - Loss:  0.072, Seconds: 10.31\n",
            "Epoch   4/100 Batch 8160/14303 - Loss:  0.069, Seconds: 10.27\n",
            "Epoch   4/100 Batch 8190/14303 - Loss:  0.070, Seconds: 10.32\n",
            "Epoch   4/100 Batch 8220/14303 - Loss:  0.072, Seconds: 10.43\n",
            "Epoch   4/100 Batch 8250/14303 - Loss:  0.070, Seconds: 10.35\n",
            "Epoch   4/100 Batch 8280/14303 - Loss:  0.072, Seconds: 10.39\n",
            "Epoch   4/100 Batch 8310/14303 - Loss:  0.072, Seconds: 10.44\n",
            "Epoch   4/100 Batch 8340/14303 - Loss:  0.072, Seconds: 10.32\n",
            "Epoch   4/100 Batch 8370/14303 - Loss:  0.073, Seconds: 10.51\n",
            "Epoch   4/100 Batch 8400/14303 - Loss:  0.073, Seconds: 10.58\n",
            "Epoch   4/100 Batch 8430/14303 - Loss:  0.071, Seconds: 10.57\n",
            "Epoch   4/100 Batch 8460/14303 - Loss:  0.069, Seconds: 10.53\n",
            "Epoch   4/100 Batch 8490/14303 - Loss:  0.069, Seconds: 10.55\n",
            "Epoch   4/100 Batch 8520/14303 - Loss:  0.070, Seconds: 10.53\n",
            "Epoch   4/100 Batch 8550/14303 - Loss:  0.068, Seconds: 10.56\n",
            "Epoch   4/100 Batch 8580/14303 - Loss:  0.068, Seconds: 10.52\n",
            "Epoch   4/100 Batch 8610/14303 - Loss:  0.071, Seconds: 10.77\n",
            "Epoch   4/100 Batch 8640/14303 - Loss:  0.072, Seconds: 10.80\n",
            "Epoch   4/100 Batch 8670/14303 - Loss:  0.071, Seconds: 10.73\n",
            "Epoch   4/100 Batch 8700/14303 - Loss:  0.068, Seconds: 10.74\n",
            "Epoch   4/100 Batch 8730/14303 - Loss:  0.068, Seconds: 10.72\n",
            "Epoch   4/100 Batch 8760/14303 - Loss:  0.069, Seconds: 10.72\n",
            "Epoch   4/100 Batch 8790/14303 - Loss:  0.070, Seconds: 10.67\n",
            "Epoch   4/100 Batch 8820/14303 - Loss:  0.069, Seconds: 10.88\n",
            "Epoch   4/100 Batch 8850/14303 - Loss:  0.073, Seconds: 10.92\n",
            "Epoch   4/100 Batch 8880/14303 - Loss:  0.071, Seconds: 10.97\n",
            "Epoch   4/100 Batch 8910/14303 - Loss:  0.071, Seconds: 10.88\n",
            "Epoch   4/100 Batch 8940/14303 - Loss:  0.069, Seconds: 10.90\n",
            "Epoch   4/100 Batch 8970/14303 - Loss:  0.067, Seconds: 10.86\n",
            "Epoch   4/100 Batch 9000/14303 - Loss:  0.069, Seconds: 10.85\n",
            "Epoch   4/100 Batch 9030/14303 - Loss:  0.074, Seconds: 10.87\n",
            "Epoch   4/100 Batch 9060/14303 - Loss:  0.069, Seconds: 11.15\n",
            "Epoch   4/100 Batch 9090/14303 - Loss:  0.072, Seconds: 11.18\n",
            "Epoch   4/100 Batch 9120/14303 - Loss:  0.073, Seconds: 11.24\n",
            "Epoch   4/100 Batch 9150/14303 - Loss:  0.070, Seconds: 11.02\n",
            "Epoch   4/100 Batch 9180/14303 - Loss:  0.070, Seconds: 11.14\n",
            "Epoch   4/100 Batch 9210/14303 - Loss:  0.069, Seconds: 11.06\n",
            "Epoch   4/100 Batch 9240/14303 - Loss:  0.068, Seconds: 11.18\n",
            "Epoch   4/100 Batch 9270/14303 - Loss:  0.070, Seconds: 11.34\n",
            "Epoch   4/100 Batch 9300/14303 - Loss:  0.069, Seconds: 11.28\n",
            "Epoch   4/100 Batch 9330/14303 - Loss:  0.069, Seconds: 11.27\n",
            "Epoch   4/100 Batch 9360/14303 - Loss:  0.073, Seconds: 11.27\n",
            "Epoch   4/100 Batch 9390/14303 - Loss:  0.071, Seconds: 11.19\n",
            "Epoch   4/100 Batch 9420/14303 - Loss:  0.068, Seconds: 11.25\n",
            "Epoch   4/100 Batch 9450/14303 - Loss:  0.071, Seconds: 11.21\n",
            "Epoch   4/100 Batch 9480/14303 - Loss:  0.070, Seconds: 11.52\n",
            "Epoch   4/100 Batch 9510/14303 - Loss:  0.071, Seconds: 11.43\n",
            "Epoch   4/100 Batch 9540/14303 - Loss:  0.068, Seconds: 11.44\n",
            "Epoch   4/100 Batch 9570/14303 - Loss:  0.070, Seconds: 11.33\n",
            "Epoch   4/100 Batch 9600/14303 - Loss:  0.070, Seconds: 11.56\n",
            "Epoch   4/100 Batch 9630/14303 - Loss:  0.074, Seconds: 11.34\n",
            "Epoch   4/100 Batch 9660/14303 - Loss:  0.073, Seconds: 11.58\n",
            "Epoch   4/100 Batch 9690/14303 - Loss:  0.071, Seconds: 11.72\n",
            "Epoch   4/100 Batch 9720/14303 - Loss:  0.066, Seconds: 11.64\n",
            "Epoch   4/100 Batch 9750/14303 - Loss:  0.070, Seconds: 11.54\n",
            "Epoch   4/100 Batch 9780/14303 - Loss:  0.070, Seconds: 11.58\n",
            "Epoch   4/100 Batch 9810/14303 - Loss:  0.071, Seconds: 11.59\n",
            "Epoch   4/100 Batch 9840/14303 - Loss:  0.069, Seconds: 11.54\n",
            "Epoch   4/100 Batch 9870/14303 - Loss:  0.070, Seconds: 11.91\n",
            "Epoch   4/100 Batch 9900/14303 - Loss:  0.070, Seconds: 11.81\n",
            "Epoch   4/100 Batch 9930/14303 - Loss:  0.071, Seconds: 11.78\n",
            "Epoch   4/100 Batch 9960/14303 - Loss:  0.068, Seconds: 11.72\n",
            "Epoch   4/100 Batch 9990/14303 - Loss:  0.071, Seconds: 11.77\n",
            "Epoch   4/100 Batch 10020/14303 - Loss:  0.074, Seconds: 11.72\n",
            "Epoch   4/100 Batch 10050/14303 - Loss:  0.071, Seconds: 11.97\n",
            "Epoch   4/100 Batch 10080/14303 - Loss:  0.069, Seconds: 12.09\n",
            "Epoch   4/100 Batch 10110/14303 - Loss:  0.070, Seconds: 11.98\n",
            "Epoch   4/100 Batch 10140/14303 - Loss:  0.070, Seconds: 11.91\n",
            "Epoch   4/100 Batch 10170/14303 - Loss:  0.067, Seconds: 11.99\n",
            "Epoch   4/100 Batch 10200/14303 - Loss:  0.067, Seconds: 11.92\n",
            "Epoch   4/100 Batch 10230/14303 - Loss:  0.072, Seconds: 12.12\n",
            "Epoch   4/100 Batch 10260/14303 - Loss:  0.070, Seconds: 12.16\n",
            "Epoch   4/100 Batch 10290/14303 - Loss:  0.071, Seconds: 12.06\n",
            "Epoch   4/100 Batch 10320/14303 - Loss:  0.070, Seconds: 12.18\n",
            "Epoch   4/100 Batch 10350/14303 - Loss:  0.069, Seconds: 12.19\n",
            "Epoch   4/100 Batch 10380/14303 - Loss:  0.068, Seconds: 12.09\n",
            "Epoch   4/100 Batch 10410/14303 - Loss:  0.070, Seconds: 12.21\n",
            "Epoch   4/100 Batch 10440/14303 - Loss:  0.073, Seconds: 12.37\n",
            "Epoch   4/100 Batch 10470/14303 - Loss:  0.070, Seconds: 12.35\n",
            "Epoch   4/100 Batch 10500/14303 - Loss:  0.068, Seconds: 12.34\n",
            "Epoch   4/100 Batch 10530/14303 - Loss:  0.069, Seconds: 12.34\n",
            "Epoch   4/100 Batch 10560/14303 - Loss:  0.068, Seconds: 12.36\n",
            "Epoch   4/100 Batch 10590/14303 - Loss:  0.071, Seconds: 12.47\n",
            "Epoch   4/100 Batch 10620/14303 - Loss:  0.070, Seconds: 12.73\n",
            "Epoch   4/100 Batch 10650/14303 - Loss:  0.072, Seconds: 12.48\n",
            "Epoch   4/100 Batch 10680/14303 - Loss:  0.072, Seconds: 12.43\n",
            "Epoch   4/100 Batch 10710/14303 - Loss:  0.073, Seconds: 12.45\n",
            "Epoch   4/100 Batch 10740/14303 - Loss:  0.069, Seconds: 12.57\n",
            "Epoch   4/100 Batch 10770/14303 - Loss:  0.071, Seconds: 12.70\n",
            "Epoch   4/100 Batch 10800/14303 - Loss:  0.071, Seconds: 12.69\n",
            "Epoch   4/100 Batch 10830/14303 - Loss:  0.071, Seconds: 12.75\n",
            "Epoch   4/100 Batch 10860/14303 - Loss:  0.070, Seconds: 12.67\n",
            "Epoch   4/100 Batch 10890/14303 - Loss:  0.074, Seconds: 12.67\n",
            "Epoch   4/100 Batch 10920/14303 - Loss:  0.072, Seconds: 12.85\n",
            "Epoch   4/100 Batch 10950/14303 - Loss:  0.081, Seconds: 12.83\n",
            "Epoch   4/100 Batch 10980/14303 - Loss:  0.076, Seconds: 12.97\n",
            "Epoch   4/100 Batch 11010/14303 - Loss:  0.075, Seconds: 12.97\n",
            "Epoch   4/100 Batch 11040/14303 - Loss:  0.070, Seconds: 13.01\n",
            "Epoch   4/100 Batch 11070/14303 - Loss:  0.069, Seconds: 12.99\n",
            "Epoch   4/100 Batch 11100/14303 - Loss:  0.072, Seconds: 13.04\n",
            "Epoch   4/100 Batch 11130/14303 - Loss:  0.073, Seconds: 13.02\n",
            "Epoch   4/100 Batch 11160/14303 - Loss:  0.071, Seconds: 13.10\n",
            "Epoch   4/100 Batch 11190/14303 - Loss:  0.067, Seconds: 13.04\n",
            "Epoch   4/100 Batch 11220/14303 - Loss:  0.069, Seconds: 13.15\n",
            "Epoch   4/100 Batch 11250/14303 - Loss:  0.070, Seconds: 13.21\n",
            "Epoch   4/100 Batch 11280/14303 - Loss:  0.067, Seconds: 13.46\n",
            "Epoch   4/100 Batch 11310/14303 - Loss:  0.072, Seconds: 13.29\n",
            "Epoch   4/100 Batch 11340/14303 - Loss:  0.071, Seconds: 13.26\n",
            "Epoch   4/100 Batch 11370/14303 - Loss:  0.069, Seconds: 13.30\n",
            "Epoch   4/100 Batch 11400/14303 - Loss:  0.068, Seconds: 13.33\n",
            "Epoch   4/100 Batch 11430/14303 - Loss:  0.070, Seconds: 13.36\n",
            "Epoch   4/100 Batch 11460/14303 - Loss:  0.069, Seconds: 13.42\n",
            "Epoch   4/100 Batch 11490/14303 - Loss:  0.067, Seconds: 13.56\n",
            "Epoch   4/100 Batch 11520/14303 - Loss:  0.070, Seconds: 13.61\n",
            "Epoch   4/100 Batch 11550/14303 - Loss:  0.069, Seconds: 13.55\n",
            "Epoch   4/100 Batch 11580/14303 - Loss:  0.069, Seconds: 13.55\n",
            "Epoch   4/100 Batch 11610/14303 - Loss:  0.070, Seconds: 13.60\n",
            "Epoch   4/100 Batch 11640/14303 - Loss:  0.072, Seconds: 13.83\n",
            "Epoch   4/100 Batch 11670/14303 - Loss:  0.068, Seconds: 13.98\n",
            "Epoch   4/100 Batch 11700/14303 - Loss:  0.069, Seconds: 13.78\n",
            "Epoch   4/100 Batch 11730/14303 - Loss:  0.068, Seconds: 13.63\n",
            "Epoch   4/100 Batch 11760/14303 - Loss:  0.067, Seconds: 13.85\n",
            "Epoch   4/100 Batch 11790/14303 - Loss:  0.070, Seconds: 14.05\n",
            "Epoch   4/100 Batch 11820/14303 - Loss:  0.070, Seconds: 13.93\n",
            "Epoch   4/100 Batch 11850/14303 - Loss:  0.071, Seconds: 13.94\n",
            "Epoch   4/100 Batch 11880/14303 - Loss:  0.067, Seconds: 14.09\n",
            "Epoch   4/100 Batch 11910/14303 - Loss:  0.068, Seconds: 14.40\n",
            "Epoch   4/100 Batch 11940/14303 - Loss:  0.067, Seconds: 14.40\n",
            "Epoch   4/100 Batch 11970/14303 - Loss:  0.067, Seconds: 14.36\n",
            "Epoch   4/100 Batch 12000/14303 - Loss:  0.071, Seconds: 14.43\n",
            "Epoch   4/100 Batch 12030/14303 - Loss:  0.072, Seconds: 14.35\n",
            "Epoch   4/100 Batch 12060/14303 - Loss:  0.069, Seconds: 14.38\n",
            "Epoch   4/100 Batch 12090/14303 - Loss:  0.070, Seconds: 14.79\n",
            "Epoch   4/100 Batch 12120/14303 - Loss:  0.068, Seconds: 15.25\n",
            "Epoch   4/100 Batch 12150/14303 - Loss:  0.071, Seconds: 15.44\n",
            "Epoch   4/100 Batch 12180/14303 - Loss:  0.068, Seconds: 15.09\n",
            "Epoch   4/100 Batch 12210/14303 - Loss:  0.069, Seconds: 14.85\n",
            "Epoch   4/100 Batch 12240/14303 - Loss:  0.069, Seconds: 14.89\n",
            "Epoch   4/100 Batch 12270/14303 - Loss:  0.071, Seconds: 15.06\n",
            "Epoch   4/100 Batch 12300/14303 - Loss:  0.068, Seconds: 15.01\n",
            "Epoch   4/100 Batch 12330/14303 - Loss:  0.067, Seconds: 15.00\n",
            "Epoch   4/100 Batch 12360/14303 - Loss:  0.070, Seconds: 15.04\n",
            "Epoch   4/100 Batch 12390/14303 - Loss:  0.070, Seconds: 15.12\n",
            "Epoch   4/100 Batch 12420/14303 - Loss:  0.067, Seconds: 15.22\n",
            "Epoch   4/100 Batch 12450/14303 - Loss:  0.065, Seconds: 15.23\n",
            "Epoch   4/100 Batch 12480/14303 - Loss:  0.068, Seconds: 15.09\n",
            "Epoch   4/100 Batch 12510/14303 - Loss:  0.070, Seconds: 15.64\n",
            "Epoch   4/100 Batch 12540/14303 - Loss:  0.068, Seconds: 15.31\n",
            "Epoch   4/100 Batch 12570/14303 - Loss:  0.067, Seconds: 15.31\n",
            "Epoch   4/100 Batch 12600/14303 - Loss:  0.069, Seconds: 15.54\n",
            "Epoch   4/100 Batch 12630/14303 - Loss:  0.069, Seconds: 15.45\n",
            "Epoch   4/100 Batch 12660/14303 - Loss:  0.068, Seconds: 15.56\n",
            "Epoch   4/100 Batch 12690/14303 - Loss:  0.068, Seconds: 15.64\n",
            "Epoch   4/100 Batch 12720/14303 - Loss:  0.069, Seconds: 15.65\n",
            "Epoch   4/100 Batch 12750/14303 - Loss:  0.066, Seconds: 15.81\n",
            "Epoch   4/100 Batch 12780/14303 - Loss:  0.068, Seconds: 15.72\n",
            "Epoch   4/100 Batch 12810/14303 - Loss:  0.067, Seconds: 16.01\n",
            "Epoch   4/100 Batch 12840/14303 - Loss:  0.070, Seconds: 15.91\n",
            "Epoch   4/100 Batch 12870/14303 - Loss:  0.066, Seconds: 15.97\n",
            "Epoch   4/100 Batch 12900/14303 - Loss:  0.068, Seconds: 16.14\n",
            "Epoch   4/100 Batch 12930/14303 - Loss:  0.065, Seconds: 16.21\n",
            "Epoch   4/100 Batch 12960/14303 - Loss:  0.070, Seconds: 16.28\n",
            "Epoch   4/100 Batch 12990/14303 - Loss:  0.068, Seconds: 16.43\n",
            "Epoch   4/100 Batch 13020/14303 - Loss:  0.069, Seconds: 16.48\n",
            "Epoch   4/100 Batch 13050/14303 - Loss:  0.068, Seconds: 16.83\n",
            "Epoch   4/100 Batch 13080/14303 - Loss:  0.065, Seconds: 16.72\n",
            "Epoch   4/100 Batch 13110/14303 - Loss:  0.067, Seconds: 16.73\n",
            "Epoch   4/100 Batch 13140/14303 - Loss:  0.069, Seconds: 16.80\n",
            "Epoch   4/100 Batch 13170/14303 - Loss:  0.068, Seconds: 17.04\n",
            "Epoch   4/100 Batch 13200/14303 - Loss:  0.065, Seconds: 16.99\n",
            "Epoch   4/100 Batch 13230/14303 - Loss:  0.068, Seconds: 17.31\n",
            "Epoch   4/100 Batch 13260/14303 - Loss:  0.069, Seconds: 17.20\n",
            "Epoch   4/100 Batch 13290/14303 - Loss:  0.066, Seconds: 17.51\n",
            "Epoch   4/100 Batch 13320/14303 - Loss:  0.065, Seconds: 17.54\n",
            "Epoch   4/100 Batch 13350/14303 - Loss:  0.067, Seconds: 17.70\n",
            "Epoch   4/100 Batch 13380/14303 - Loss:  0.070, Seconds: 17.76\n",
            "Epoch   4/100 Batch 13410/14303 - Loss:  0.067, Seconds: 17.95\n",
            "Epoch   4/100 Batch 13440/14303 - Loss:  0.067, Seconds: 18.02\n",
            "Epoch   4/100 Batch 13470/14303 - Loss:  0.067, Seconds: 18.06\n",
            "Epoch   4/100 Batch 13500/14303 - Loss:  0.069, Seconds: 18.26\n",
            "Epoch   4/100 Batch 13530/14303 - Loss:  0.070, Seconds: 18.53\n",
            "Epoch   4/100 Batch 13560/14303 - Loss:  0.068, Seconds: 18.77\n",
            "Epoch   4/100 Batch 13590/14303 - Loss:  0.069, Seconds: 18.81\n",
            "Epoch   4/100 Batch 13620/14303 - Loss:  0.065, Seconds: 18.91\n",
            "Epoch   4/100 Batch 13650/14303 - Loss:  0.070, Seconds: 18.98\n",
            "Epoch   4/100 Batch 13680/14303 - Loss:  0.069, Seconds: 19.32\n",
            "Epoch   4/100 Batch 13710/14303 - Loss:  0.067, Seconds: 19.47\n",
            "Epoch   4/100 Batch 13740/14303 - Loss:  0.066, Seconds: 19.72\n",
            "Epoch   4/100 Batch 13770/14303 - Loss:  0.066, Seconds: 19.98\n",
            "Epoch   4/100 Batch 13800/14303 - Loss:  0.067, Seconds: 20.23\n",
            "Epoch   4/100 Batch 13830/14303 - Loss:  0.068, Seconds: 20.31\n",
            "Epoch   4/100 Batch 13860/14303 - Loss:  0.078, Seconds: 20.45\n",
            "Epoch   4/100 Batch 13890/14303 - Loss:  0.082, Seconds: 20.78\n",
            "Epoch   4/100 Batch 13920/14303 - Loss:  0.075, Seconds: 21.17\n",
            "Epoch   4/100 Batch 13950/14303 - Loss:  0.071, Seconds: 21.39\n",
            "Epoch   4/100 Batch 13980/14303 - Loss:  0.074, Seconds: 21.73\n",
            "Epoch   4/100 Batch 14010/14303 - Loss:  0.069, Seconds: 22.16\n",
            "Epoch   4/100 Batch 14040/14303 - Loss:  0.071, Seconds: 22.29\n",
            "Epoch   4/100 Batch 14070/14303 - Loss:  0.069, Seconds: 22.98\n",
            "Epoch   4/100 Batch 14100/14303 - Loss:  0.071, Seconds: 23.55\n",
            "Epoch   4/100 Batch 14130/14303 - Loss:  0.072, Seconds: 24.02\n",
            "Epoch   4/100 Batch 14160/14303 - Loss:  0.071, Seconds: 24.70\n",
            "Epoch   4/100 Batch 14190/14303 - Loss:  0.070, Seconds: 25.72\n",
            "Epoch   4/100 Batch 14220/14303 - Loss:  0.074, Seconds: 26.60\n",
            "Epoch   4/100 Batch 14250/14303 - Loss:  0.068, Seconds: 28.26\n",
            "Epoch   4/100 Batch 14280/14303 - Loss:  0.072, Seconds: 30.64\n",
            "  Validation Input: not asm bad as one sandal think\n",
            "  Validation Output: not as mast bad as one sandal think\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: i tis also aclle granth shib\n",
            "  Validation Output: it is also called granth shib\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: itf is ntended fo adutlts only\n",
            "  Validation Output: it is intended for adults only\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: reason thngs out for yourself\n",
            "  Validation Output: reason things out for yourself\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: it hppenedt ow ork uqiet well\n",
            "  Validation Output: it happened to work quiet work qui\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: chef and ia r doing very wellr\n",
            "  Validation Output: chef and i are doing very well\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: center forthe peformijg arst\n",
            "  Validation Output: center for the performing arts\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: sono nito a hot ojwl to serve\n",
            "  Validation Output: soon into a hot jowl to serve\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: people mefet her and they swojonz\n",
            "  Validation Output: people meet her and they swoon\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: ym poro lttle heahd is boilign\n",
            "  Validation Output: my poor little head is boiling\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: xthis hads become  acommon scene\n",
            "  Validation Output: this had become a common scene\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: eveno went through teh specialsl\n",
            "  Validation Output: even went through the specials\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: preheat ia setamer on hig ehat\n",
            "  Validation Output: preheat in a stammer on high heat\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: lolc well someon hadc to saye it\n",
            "  Validation Output: lolic well someone had to say it\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he waw ejavig mbats o nthe lst\n",
            "  Validation Output: he was weaving mats on the last on\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: this i wrong form any reasosn\n",
            "  Validation Output: this is wrong for many reasons\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: teoday we headedo again to cba\n",
            "  Validation Output: today we headed to again to cabalia\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep nad umember status reqguired\n",
            "  Validation Output: rep and member status required\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: it snowed midling hard to dayu\n",
            "  Validation Output: it snowed middling hard to day\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: True\n",
            "\n",
            "  Validation Input: they baoth wear leather sandals\n",
            "  Validation Output: they both wear leather standals\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  0.143, Seconds: 0.00\n",
            "New Record!\n",
            "\n",
            "Training Model: 5\n",
            "Epoch   5/100 Batch   30/14303 - Loss:  0.095, Seconds: 6.22\n",
            "Epoch   5/100 Batch   60/14303 - Loss:  0.076, Seconds: 5.99\n",
            "Epoch   5/100 Batch   90/14303 - Loss:  0.079, Seconds: 5.94\n",
            "Epoch   5/100 Batch  120/14303 - Loss:  0.076, Seconds: 5.96\n",
            "Epoch   5/100 Batch  150/14303 - Loss:  0.076, Seconds: 5.90\n",
            "Epoch   5/100 Batch  180/14303 - Loss:  0.076, Seconds: 5.96\n",
            "Epoch   5/100 Batch  210/14303 - Loss:  0.072, Seconds: 5.94\n",
            "Epoch   5/100 Batch  240/14303 - Loss:  0.072, Seconds: 5.88\n",
            "Epoch   5/100 Batch  270/14303 - Loss:  0.074, Seconds: 5.95\n",
            "Epoch   5/100 Batch  300/14303 - Loss:  0.103, Seconds: 5.99\n",
            "Epoch   5/100 Batch  330/14303 - Loss:  0.077, Seconds: 6.21\n",
            "Epoch   5/100 Batch  360/14303 - Loss:  0.075, Seconds: 6.03\n",
            "Epoch   5/100 Batch  390/14303 - Loss:  0.074, Seconds: 6.08\n",
            "Epoch   5/100 Batch  420/14303 - Loss:  0.071, Seconds: 6.11\n",
            "Epoch   5/100 Batch  450/14303 - Loss:  0.075, Seconds: 6.12\n",
            "Epoch   5/100 Batch  480/14303 - Loss:  0.071, Seconds: 6.04\n",
            "Epoch   5/100 Batch  510/14303 - Loss:  0.070, Seconds: 5.97\n",
            "Epoch   5/100 Batch  540/14303 - Loss:  0.072, Seconds: 5.99\n",
            "Epoch   5/100 Batch  570/14303 - Loss:  0.074, Seconds: 6.01\n",
            "Epoch   5/100 Batch  600/14303 - Loss:  0.079, Seconds: 6.03\n",
            "Epoch   5/100 Batch  630/14303 - Loss:  0.071, Seconds: 6.24\n",
            "Epoch   5/100 Batch  660/14303 - Loss:  0.070, Seconds: 6.25\n",
            "Epoch   5/100 Batch  690/14303 - Loss:  0.071, Seconds: 6.18\n",
            "Epoch   5/100 Batch  720/14303 - Loss:  0.074, Seconds: 6.27\n",
            "Epoch   5/100 Batch  750/14303 - Loss:  0.073, Seconds: 6.19\n",
            "Epoch   5/100 Batch  780/14303 - Loss:  0.071, Seconds: 6.26\n",
            "Epoch   5/100 Batch  810/14303 - Loss:  0.072, Seconds: 6.24\n",
            "Epoch   5/100 Batch  840/14303 - Loss:  0.072, Seconds: 6.25\n",
            "Epoch   5/100 Batch  870/14303 - Loss:  0.076, Seconds: 6.15\n",
            "Epoch   5/100 Batch  900/14303 - Loss:  0.067, Seconds: 6.14\n",
            "Epoch   5/100 Batch  930/14303 - Loss:  0.077, Seconds: 6.28\n",
            "Epoch   5/100 Batch  960/14303 - Loss:  0.072, Seconds: 6.41\n",
            "Epoch   5/100 Batch  990/14303 - Loss:  0.073, Seconds: 6.46\n",
            "Epoch   5/100 Batch 1020/14303 - Loss:  0.069, Seconds: 6.37\n",
            "Epoch   5/100 Batch 1050/14303 - Loss:  0.076, Seconds: 6.37\n",
            "Epoch   5/100 Batch 1080/14303 - Loss:  0.072, Seconds: 6.34\n",
            "Epoch   5/100 Batch 1110/14303 - Loss:  0.069, Seconds: 6.37\n",
            "Epoch   5/100 Batch 1140/14303 - Loss:  0.068, Seconds: 6.41\n",
            "Epoch   5/100 Batch 1170/14303 - Loss:  0.068, Seconds: 6.39\n",
            "Epoch   5/100 Batch 1200/14303 - Loss:  0.068, Seconds: 6.35\n",
            "Epoch   5/100 Batch 1230/14303 - Loss:  0.070, Seconds: 6.35\n",
            "Epoch   5/100 Batch 1260/14303 - Loss:  0.072, Seconds: 6.58\n",
            "Epoch   5/100 Batch 1290/14303 - Loss:  0.069, Seconds: 6.59\n",
            "Epoch   5/100 Batch 1320/14303 - Loss:  0.069, Seconds: 6.50\n",
            "Epoch   5/100 Batch 1350/14303 - Loss:  0.070, Seconds: 6.54\n",
            "Epoch   5/100 Batch 1380/14303 - Loss:  0.065, Seconds: 6.50\n",
            "Epoch   5/100 Batch 1410/14303 - Loss:  0.071, Seconds: 6.56\n",
            "Epoch   5/100 Batch 1440/14303 - Loss:  0.067, Seconds: 6.51\n",
            "Epoch   5/100 Batch 1470/14303 - Loss:  0.069, Seconds: 6.53\n",
            "Epoch   5/100 Batch 1500/14303 - Loss:  0.070, Seconds: 6.57\n",
            "Epoch   5/100 Batch 1530/14303 - Loss:  0.068, Seconds: 6.53\n",
            "Epoch   5/100 Batch 1560/14303 - Loss:  0.081, Seconds: 6.57\n",
            "Epoch   5/100 Batch 1590/14303 - Loss:  0.067, Seconds: 6.72\n",
            "Epoch   5/100 Batch 1620/14303 - Loss:  0.071, Seconds: 6.71\n",
            "Epoch   5/100 Batch 1650/14303 - Loss:  0.069, Seconds: 6.71\n",
            "Epoch   5/100 Batch 1680/14303 - Loss:  0.069, Seconds: 6.85\n",
            "Epoch   5/100 Batch 1710/14303 - Loss:  0.069, Seconds: 6.85\n",
            "Epoch   5/100 Batch 1740/14303 - Loss:  0.069, Seconds: 6.69\n",
            "Epoch   5/100 Batch 1770/14303 - Loss:  0.068, Seconds: 6.72\n",
            "Epoch   5/100 Batch 1800/14303 - Loss:  0.068, Seconds: 6.75\n",
            "Epoch   5/100 Batch 1830/14303 - Loss:  0.069, Seconds: 6.66\n",
            "Epoch   5/100 Batch 1860/14303 - Loss:  0.074, Seconds: 6.67\n",
            "Epoch   5/100 Batch 1890/14303 - Loss:  0.075, Seconds: 6.83\n",
            "Epoch   5/100 Batch 1920/14303 - Loss:  0.067, Seconds: 6.83\n",
            "Epoch   5/100 Batch 1950/14303 - Loss:  0.067, Seconds: 6.83\n",
            "Epoch   5/100 Batch 1980/14303 - Loss:  0.071, Seconds: 6.84\n",
            "Epoch   5/100 Batch 2010/14303 - Loss:  0.067, Seconds: 6.85\n",
            "Epoch   5/100 Batch 2040/14303 - Loss:  0.071, Seconds: 6.87\n",
            "Epoch   5/100 Batch 2070/14303 - Loss:  0.069, Seconds: 6.86\n",
            "Epoch   5/100 Batch 2100/14303 - Loss:  0.066, Seconds: 6.90\n",
            "Epoch   5/100 Batch 2130/14303 - Loss:  0.065, Seconds: 6.85\n",
            "Epoch   5/100 Batch 2160/14303 - Loss:  0.070, Seconds: 6.94\n",
            "Epoch   5/100 Batch 2190/14303 - Loss:  0.072, Seconds: 6.91\n",
            "Epoch   5/100 Batch 2220/14303 - Loss:  0.066, Seconds: 6.98\n",
            "Epoch   5/100 Batch 2250/14303 - Loss:  0.069, Seconds: 7.13\n",
            "Epoch   5/100 Batch 2280/14303 - Loss:  0.071, Seconds: 7.04\n",
            "Epoch   5/100 Batch 2310/14303 - Loss:  0.073, Seconds: 7.00\n",
            "Epoch   5/100 Batch 2340/14303 - Loss:  0.071, Seconds: 7.03\n",
            "Epoch   5/100 Batch 2370/14303 - Loss:  0.070, Seconds: 6.97\n",
            "Epoch   5/100 Batch 2400/14303 - Loss:  0.069, Seconds: 7.06\n",
            "Epoch   5/100 Batch 2430/14303 - Loss:  0.068, Seconds: 7.09\n",
            "Epoch   5/100 Batch 2460/14303 - Loss:  0.069, Seconds: 7.05\n",
            "Epoch   5/100 Batch 2490/14303 - Loss:  0.070, Seconds: 7.05\n",
            "Epoch   5/100 Batch 2520/14303 - Loss:  0.069, Seconds: 7.20\n",
            "Epoch   5/100 Batch 2550/14303 - Loss:  0.072, Seconds: 7.21\n",
            "Epoch   5/100 Batch 2580/14303 - Loss:  0.066, Seconds: 7.20\n",
            "Epoch   5/100 Batch 2610/14303 - Loss:  0.071, Seconds: 7.24\n",
            "Epoch   5/100 Batch 2640/14303 - Loss:  0.068, Seconds: 7.22\n",
            "Epoch   5/100 Batch 2670/14303 - Loss:  0.068, Seconds: 7.19\n",
            "Epoch   5/100 Batch 2700/14303 - Loss:  0.064, Seconds: 7.22\n",
            "Epoch   5/100 Batch 2730/14303 - Loss:  0.067, Seconds: 7.20\n",
            "Epoch   5/100 Batch 2760/14303 - Loss:  0.068, Seconds: 7.19\n",
            "Epoch   5/100 Batch 2790/14303 - Loss:  0.068, Seconds: 7.23\n",
            "Epoch   5/100 Batch 2820/14303 - Loss:  0.066, Seconds: 7.30\n",
            "Epoch   5/100 Batch 2850/14303 - Loss:  0.077, Seconds: 7.35\n",
            "Epoch   5/100 Batch 2880/14303 - Loss:  0.068, Seconds: 7.52\n",
            "Epoch   5/100 Batch 2910/14303 - Loss:  0.068, Seconds: 7.35\n",
            "Epoch   5/100 Batch 2940/14303 - Loss:  0.064, Seconds: 7.57\n",
            "Epoch   5/100 Batch 2970/14303 - Loss:  0.066, Seconds: 7.43\n",
            "Epoch   5/100 Batch 3000/14303 - Loss:  0.067, Seconds: 7.37\n",
            "Epoch   5/100 Batch 3030/14303 - Loss:  0.068, Seconds: 7.38\n",
            "Epoch   5/100 Batch 3060/14303 - Loss:  0.066, Seconds: 7.39\n",
            "Epoch   5/100 Batch 3090/14303 - Loss:  0.065, Seconds: 7.44\n",
            "Epoch   5/100 Batch 3120/14303 - Loss:  0.068, Seconds: 7.40\n",
            "Epoch   5/100 Batch 3150/14303 - Loss:  0.069, Seconds: 7.35\n",
            "Epoch   5/100 Batch 3180/14303 - Loss:  0.077, Seconds: 7.50\n",
            "Epoch   5/100 Batch 3210/14303 - Loss:  0.069, Seconds: 7.58\n",
            "Epoch   5/100 Batch 3240/14303 - Loss:  0.066, Seconds: 7.48\n",
            "Epoch   5/100 Batch 3270/14303 - Loss:  0.065, Seconds: 7.52\n",
            "Epoch   5/100 Batch 3300/14303 - Loss:  0.071, Seconds: 7.59\n",
            "Epoch   5/100 Batch 3330/14303 - Loss:  0.068, Seconds: 7.65\n",
            "Epoch   5/100 Batch 3360/14303 - Loss:  0.070, Seconds: 7.53\n",
            "Epoch   5/100 Batch 3390/14303 - Loss:  0.070, Seconds: 7.60\n",
            "Epoch   5/100 Batch 3420/14303 - Loss:  0.067, Seconds: 7.67\n",
            "Epoch   5/100 Batch 3450/14303 - Loss:  0.069, Seconds: 7.58\n",
            "Epoch   5/100 Batch 3480/14303 - Loss:  0.071, Seconds: 7.56\n",
            "Epoch   5/100 Batch 3510/14303 - Loss:  0.068, Seconds: 7.73\n",
            "Epoch   5/100 Batch 3540/14303 - Loss:  0.067, Seconds: 7.66\n",
            "Epoch   5/100 Batch 3570/14303 - Loss:  0.067, Seconds: 7.73\n",
            "Epoch   5/100 Batch 3600/14303 - Loss:  0.065, Seconds: 7.72\n",
            "Epoch   5/100 Batch 3630/14303 - Loss:  0.069, Seconds: 7.75\n",
            "Epoch   5/100 Batch 3660/14303 - Loss:  0.062, Seconds: 7.78\n",
            "Epoch   5/100 Batch 3690/14303 - Loss:  0.064, Seconds: 7.72\n",
            "Epoch   5/100 Batch 3720/14303 - Loss:  0.066, Seconds: 7.79\n",
            "Epoch   5/100 Batch 3750/14303 - Loss:  0.068, Seconds: 7.74\n",
            "Epoch   5/100 Batch 3780/14303 - Loss:  0.067, Seconds: 7.72\n",
            "Epoch   5/100 Batch 3810/14303 - Loss:  0.068, Seconds: 7.72\n",
            "Epoch   5/100 Batch 3840/14303 - Loss:  0.069, Seconds: 7.85\n",
            "Epoch   5/100 Batch 3870/14303 - Loss:  0.067, Seconds: 7.84\n",
            "Epoch   5/100 Batch 3900/14303 - Loss:  0.069, Seconds: 7.89\n",
            "Epoch   5/100 Batch 3930/14303 - Loss:  0.068, Seconds: 7.88\n",
            "Epoch   5/100 Batch 3960/14303 - Loss:  0.068, Seconds: 7.89\n",
            "Epoch   5/100 Batch 3990/14303 - Loss:  0.065, Seconds: 7.96\n",
            "Epoch   5/100 Batch 4020/14303 - Loss:  0.069, Seconds: 7.83\n",
            "Epoch   5/100 Batch 4050/14303 - Loss:  0.065, Seconds: 7.86\n",
            "Epoch   5/100 Batch 4080/14303 - Loss:  0.068, Seconds: 8.06\n",
            "Epoch   5/100 Batch 4110/14303 - Loss:  0.063, Seconds: 7.95\n",
            "Epoch   5/100 Batch 4140/14303 - Loss:  0.075, Seconds: 8.11\n",
            "Epoch   5/100 Batch 4170/14303 - Loss:  0.063, Seconds: 8.02\n",
            "Epoch   5/100 Batch 4200/14303 - Loss:  0.067, Seconds: 8.07\n",
            "Epoch   5/100 Batch 4230/14303 - Loss:  0.068, Seconds: 8.04\n",
            "Epoch   5/100 Batch 4260/14303 - Loss:  0.071, Seconds: 8.04\n",
            "Epoch   5/100 Batch 4290/14303 - Loss:  0.067, Seconds: 8.12\n",
            "Epoch   5/100 Batch 4320/14303 - Loss:  0.065, Seconds: 8.04\n",
            "Epoch   5/100 Batch 4350/14303 - Loss:  0.067, Seconds: 8.00\n",
            "Epoch   5/100 Batch 4380/14303 - Loss:  0.069, Seconds: 8.01\n",
            "Epoch   5/100 Batch 4410/14303 - Loss:  0.074, Seconds: 7.97\n",
            "Epoch   5/100 Batch 4440/14303 - Loss:  0.076, Seconds: 8.24\n",
            "Epoch   5/100 Batch 4470/14303 - Loss:  0.071, Seconds: 8.32\n",
            "Epoch   5/100 Batch 4500/14303 - Loss:  0.069, Seconds: 8.23\n",
            "Epoch   5/100 Batch 4530/14303 - Loss:  0.067, Seconds: 8.32\n",
            "Epoch   5/100 Batch 4560/14303 - Loss:  0.068, Seconds: 8.24\n",
            "Epoch   5/100 Batch 4590/14303 - Loss:  0.066, Seconds: 8.18\n",
            "Epoch   5/100 Batch 4620/14303 - Loss:  0.062, Seconds: 8.24\n",
            "Epoch   5/100 Batch 4650/14303 - Loss:  0.069, Seconds: 8.21\n",
            "Epoch   5/100 Batch 4680/14303 - Loss:  0.066, Seconds: 8.23\n",
            "Epoch   5/100 Batch 4710/14303 - Loss:  0.069, Seconds: 8.17\n",
            "Epoch   5/100 Batch 4740/14303 - Loss:  0.071, Seconds: 8.26\n",
            "Epoch   5/100 Batch 4770/14303 - Loss:  0.068, Seconds: 8.38\n",
            "Epoch   5/100 Batch 4800/14303 - Loss:  0.070, Seconds: 8.37\n",
            "Epoch   5/100 Batch 4830/14303 - Loss:  0.065, Seconds: 8.49\n",
            "Epoch   5/100 Batch 4860/14303 - Loss:  0.066, Seconds: 8.30\n",
            "Epoch   5/100 Batch 4890/14303 - Loss:  0.069, Seconds: 8.38\n",
            "Epoch   5/100 Batch 4920/14303 - Loss:  0.065, Seconds: 8.38\n",
            "Epoch   5/100 Batch 4950/14303 - Loss:  0.065, Seconds: 8.37\n",
            "Epoch   5/100 Batch 4980/14303 - Loss:  0.065, Seconds: 8.30\n",
            "Epoch   5/100 Batch 5010/14303 - Loss:  0.066, Seconds: 8.35\n",
            "Epoch   5/100 Batch 5040/14303 - Loss:  0.069, Seconds: 8.36\n",
            "Epoch   5/100 Batch 5070/14303 - Loss:  0.077, Seconds: 8.57\n",
            "Epoch   5/100 Batch 5100/14303 - Loss:  0.064, Seconds: 8.62\n",
            "Epoch   5/100 Batch 5130/14303 - Loss:  0.070, Seconds: 8.75\n",
            "Epoch   5/100 Batch 5160/14303 - Loss:  0.063, Seconds: 8.64\n",
            "Epoch   5/100 Batch 5190/14303 - Loss:  0.066, Seconds: 8.59\n",
            "Epoch   5/100 Batch 5220/14303 - Loss:  0.063, Seconds: 8.60\n",
            "Epoch   5/100 Batch 5250/14303 - Loss:  0.067, Seconds: 8.61\n",
            "Epoch   5/100 Batch 5280/14303 - Loss:  0.071, Seconds: 8.55\n",
            "Epoch   5/100 Batch 5310/14303 - Loss:  0.068, Seconds: 8.50\n",
            "Epoch   5/100 Batch 5340/14303 - Loss:  0.067, Seconds: 8.55\n",
            "Epoch   5/100 Batch 5370/14303 - Loss:  0.068, Seconds: 8.76\n",
            "Epoch   5/100 Batch 5400/14303 - Loss:  0.069, Seconds: 8.65\n",
            "Epoch   5/100 Batch 5430/14303 - Loss:  0.068, Seconds: 8.72\n",
            "Epoch   5/100 Batch 5460/14303 - Loss:  0.065, Seconds: 8.65\n",
            "Epoch   5/100 Batch 5490/14303 - Loss:  0.069, Seconds: 8.72\n",
            "Epoch   5/100 Batch 5520/14303 - Loss:  0.068, Seconds: 8.75\n",
            "Epoch   5/100 Batch 5550/14303 - Loss:  0.066, Seconds: 8.75\n",
            "Epoch   5/100 Batch 5580/14303 - Loss:  0.065, Seconds: 8.72\n",
            "Epoch   5/100 Batch 5610/14303 - Loss:  0.064, Seconds: 8.73\n",
            "Epoch   5/100 Batch 5640/14303 - Loss:  0.067, Seconds: 8.70\n",
            "Epoch   5/100 Batch 5670/14303 - Loss:  0.068, Seconds: 8.88\n",
            "Epoch   5/100 Batch 5700/14303 - Loss:  0.065, Seconds: 8.92\n",
            "Epoch   5/100 Batch 5730/14303 - Loss:  0.071, Seconds: 8.90\n",
            "Epoch   5/100 Batch 5760/14303 - Loss:  0.066, Seconds: 8.91\n",
            "Epoch   5/100 Batch 5790/14303 - Loss:  0.067, Seconds: 8.87\n",
            "Epoch   5/100 Batch 5820/14303 - Loss:  0.069, Seconds: 8.97\n",
            "Epoch   5/100 Batch 5850/14303 - Loss:  0.065, Seconds: 8.82\n",
            "Epoch   5/100 Batch 5880/14303 - Loss:  0.069, Seconds: 8.91\n",
            "Epoch   5/100 Batch 5910/14303 - Loss:  0.067, Seconds: 8.86\n",
            "Epoch   5/100 Batch 5940/14303 - Loss:  0.067, Seconds: 8.89\n",
            "Epoch   5/100 Batch 5970/14303 - Loss:  0.069, Seconds: 9.01\n",
            "Epoch   5/100 Batch 6000/14303 - Loss:  0.068, Seconds: 9.08\n",
            "Epoch   5/100 Batch 6030/14303 - Loss:  0.068, Seconds: 9.10\n",
            "Epoch   5/100 Batch 6060/14303 - Loss:  0.068, Seconds: 9.06\n",
            "Epoch   5/100 Batch 6090/14303 - Loss:  0.065, Seconds: 9.06\n",
            "Epoch   5/100 Batch 6120/14303 - Loss:  0.068, Seconds: 9.26\n",
            "Epoch   5/100 Batch 6150/14303 - Loss:  0.067, Seconds: 9.20\n",
            "Epoch   5/100 Batch 6180/14303 - Loss:  0.067, Seconds: 9.12\n",
            "Epoch   5/100 Batch 6210/14303 - Loss:  0.065, Seconds: 9.03\n",
            "Epoch   5/100 Batch 6240/14303 - Loss:  0.071, Seconds: 9.30\n",
            "Epoch   5/100 Batch 6270/14303 - Loss:  0.067, Seconds: 9.23\n",
            "Epoch   5/100 Batch 6300/14303 - Loss:  0.066, Seconds: 9.38\n",
            "Epoch   5/100 Batch 6330/14303 - Loss:  0.066, Seconds: 9.33\n",
            "Epoch   5/100 Batch 6360/14303 - Loss:  0.069, Seconds: 9.31\n",
            "Epoch   5/100 Batch 6390/14303 - Loss:  0.065, Seconds: 9.22\n",
            "Epoch   5/100 Batch 6420/14303 - Loss:  0.066, Seconds: 9.19\n",
            "Epoch   5/100 Batch 6450/14303 - Loss:  0.068, Seconds: 9.25\n",
            "Epoch   5/100 Batch 6480/14303 - Loss:  0.070, Seconds: 9.34\n",
            "Epoch   5/100 Batch 6510/14303 - Loss:  0.067, Seconds: 9.24\n",
            "Epoch   5/100 Batch 6540/14303 - Loss:  0.065, Seconds: 9.36\n",
            "Epoch   5/100 Batch 6570/14303 - Loss:  0.066, Seconds: 9.44\n",
            "Epoch   5/100 Batch 6600/14303 - Loss:  0.068, Seconds: 9.47\n",
            "Epoch   5/100 Batch 6630/14303 - Loss:  0.067, Seconds: 9.43\n",
            "Epoch   5/100 Batch 6660/14303 - Loss:  0.065, Seconds: 9.40\n",
            "Epoch   5/100 Batch 6690/14303 - Loss:  0.066, Seconds: 9.42\n",
            "Epoch   5/100 Batch 6720/14303 - Loss:  0.064, Seconds: 9.45\n",
            "Epoch   5/100 Batch 6750/14303 - Loss:  0.066, Seconds: 9.40\n",
            "Epoch   5/100 Batch 6780/14303 - Loss:  0.065, Seconds: 9.45\n",
            "Epoch   5/100 Batch 6810/14303 - Loss:  0.068, Seconds: 9.63\n",
            "Epoch   5/100 Batch 6840/14303 - Loss:  0.068, Seconds: 9.64\n",
            "Epoch   5/100 Batch 6870/14303 - Loss:  0.065, Seconds: 9.67\n",
            "Epoch   5/100 Batch 6900/14303 - Loss:  0.066, Seconds: 9.67\n",
            "Epoch   5/100 Batch 6930/14303 - Loss:  0.064, Seconds: 9.62\n",
            "Epoch   5/100 Batch 6960/14303 - Loss:  0.068, Seconds: 9.63\n",
            "Epoch   5/100 Batch 6990/14303 - Loss:  0.068, Seconds: 9.61\n",
            "Epoch   5/100 Batch 7020/14303 - Loss:  0.067, Seconds: 9.70\n",
            "Epoch   5/100 Batch 7050/14303 - Loss:  0.067, Seconds: 9.84\n",
            "Epoch   5/100 Batch 7080/14303 - Loss:  0.066, Seconds: 9.95\n",
            "Epoch   5/100 Batch 7110/14303 - Loss:  0.067, Seconds: 9.81\n",
            "Epoch   5/100 Batch 7140/14303 - Loss:  0.067, Seconds: 9.80\n",
            "Epoch   5/100 Batch 7170/14303 - Loss:  0.067, Seconds: 9.79\n",
            "Epoch   5/100 Batch 7200/14303 - Loss:  0.064, Seconds: 9.76\n",
            "Epoch   5/100 Batch 7230/14303 - Loss:  0.066, Seconds: 9.75\n",
            "Epoch   5/100 Batch 7260/14303 - Loss:  0.064, Seconds: 9.70\n",
            "Epoch   5/100 Batch 7290/14303 - Loss:  0.065, Seconds: 9.71\n",
            "Epoch   5/100 Batch 7320/14303 - Loss:  0.070, Seconds: 9.90\n",
            "Epoch   5/100 Batch 7350/14303 - Loss:  0.071, Seconds: 9.93\n",
            "Epoch   5/100 Batch 7380/14303 - Loss:  0.065, Seconds: 10.10\n",
            "Epoch   5/100 Batch 7410/14303 - Loss:  0.067, Seconds: 10.01\n",
            "Epoch   5/100 Batch 7440/14303 - Loss:  0.066, Seconds: 9.93\n",
            "Epoch   5/100 Batch 7470/14303 - Loss:  0.069, Seconds: 9.98\n",
            "Epoch   5/100 Batch 7500/14303 - Loss:  0.065, Seconds: 9.85\n",
            "Epoch   5/100 Batch 7530/14303 - Loss:  0.064, Seconds: 10.02\n",
            "Epoch   5/100 Batch 7560/14303 - Loss:  0.066, Seconds: 10.00\n",
            "Epoch   5/100 Batch 7590/14303 - Loss:  0.065, Seconds: 9.98\n",
            "Epoch   5/100 Batch 7620/14303 - Loss:  0.067, Seconds: 10.09\n",
            "Epoch   5/100 Batch 7650/14303 - Loss:  0.064, Seconds: 10.14\n",
            "Epoch   5/100 Batch 7680/14303 - Loss:  0.067, Seconds: 10.11\n",
            "Epoch   5/100 Batch 7710/14303 - Loss:  0.067, Seconds: 10.20\n",
            "Epoch   5/100 Batch 7740/14303 - Loss:  0.065, Seconds: 10.16\n",
            "Epoch   5/100 Batch 7770/14303 - Loss:  0.066, Seconds: 10.19\n",
            "Epoch   5/100 Batch 7800/14303 - Loss:  0.066, Seconds: 10.09\n",
            "Epoch   5/100 Batch 7830/14303 - Loss:  0.065, Seconds: 10.09\n",
            "Epoch   5/100 Batch 7860/14303 - Loss:  0.066, Seconds: 10.20\n",
            "Epoch   5/100 Batch 7890/14303 - Loss:  0.065, Seconds: 10.36\n",
            "Epoch   5/100 Batch 7920/14303 - Loss:  0.064, Seconds: 10.45\n",
            "Epoch   5/100 Batch 7950/14303 - Loss:  0.070, Seconds: 10.42\n",
            "Epoch   5/100 Batch 7980/14303 - Loss:  0.066, Seconds: 10.45\n",
            "Epoch   5/100 Batch 8010/14303 - Loss:  0.066, Seconds: 10.36\n",
            "Epoch   5/100 Batch 8040/14303 - Loss:  0.064, Seconds: 10.44\n",
            "Epoch   5/100 Batch 8070/14303 - Loss:  0.062, Seconds: 10.39\n",
            "Epoch   5/100 Batch 8100/14303 - Loss:  0.066, Seconds: 10.31\n",
            "Epoch   5/100 Batch 8130/14303 - Loss:  0.066, Seconds: 10.50\n",
            "Epoch   5/100 Batch 8160/14303 - Loss:  0.064, Seconds: 10.47\n",
            "Epoch   5/100 Batch 8190/14303 - Loss:  0.067, Seconds: 10.59\n",
            "Epoch   5/100 Batch 8220/14303 - Loss:  0.064, Seconds: 10.65\n",
            "Epoch   5/100 Batch 8250/14303 - Loss:  0.066, Seconds: 10.54\n",
            "Epoch   5/100 Batch 8280/14303 - Loss:  0.066, Seconds: 10.46\n",
            "Epoch   5/100 Batch 8310/14303 - Loss:  0.066, Seconds: 10.52\n",
            "Epoch   5/100 Batch 8340/14303 - Loss:  0.066, Seconds: 10.47\n",
            "Epoch   5/100 Batch 8370/14303 - Loss:  0.067, Seconds: 10.71\n",
            "Epoch   5/100 Batch 8400/14303 - Loss:  0.067, Seconds: 10.66\n",
            "Epoch   5/100 Batch 8430/14303 - Loss:  0.065, Seconds: 10.71\n",
            "Epoch   5/100 Batch 8460/14303 - Loss:  0.065, Seconds: 10.65\n",
            "Epoch   5/100 Batch 8490/14303 - Loss:  0.064, Seconds: 10.67\n",
            "Epoch   5/100 Batch 8520/14303 - Loss:  0.066, Seconds: 10.69\n",
            "Epoch   5/100 Batch 8550/14303 - Loss:  0.063, Seconds: 10.69\n",
            "Epoch   5/100 Batch 8580/14303 - Loss:  0.065, Seconds: 10.62\n",
            "Epoch   5/100 Batch 8610/14303 - Loss:  0.064, Seconds: 10.95\n",
            "Epoch   5/100 Batch 8640/14303 - Loss:  0.068, Seconds: 10.95\n",
            "Epoch   5/100 Batch 8670/14303 - Loss:  0.065, Seconds: 10.87\n",
            "Epoch   5/100 Batch 8700/14303 - Loss:  0.065, Seconds: 10.81\n",
            "Epoch   5/100 Batch 8730/14303 - Loss:  0.066, Seconds: 10.89\n",
            "Epoch   5/100 Batch 8760/14303 - Loss:  0.067, Seconds: 11.11\n",
            "Epoch   5/100 Batch 8790/14303 - Loss:  0.063, Seconds: 10.88\n",
            "Epoch   5/100 Batch 8820/14303 - Loss:  0.066, Seconds: 10.94\n",
            "Epoch   5/100 Batch 8850/14303 - Loss:  0.065, Seconds: 11.01\n",
            "Epoch   5/100 Batch 8880/14303 - Loss:  0.066, Seconds: 11.00\n",
            "Epoch   5/100 Batch 8910/14303 - Loss:  0.063, Seconds: 11.02\n",
            "Epoch   5/100 Batch 8940/14303 - Loss:  0.062, Seconds: 10.98\n",
            "Epoch   5/100 Batch 8970/14303 - Loss:  0.066, Seconds: 11.11\n",
            "Epoch   5/100 Batch 9000/14303 - Loss:  0.065, Seconds: 11.13\n",
            "Epoch   5/100 Batch 9030/14303 - Loss:  0.067, Seconds: 11.04\n",
            "Epoch   5/100 Batch 9060/14303 - Loss:  0.063, Seconds: 11.26\n",
            "Epoch   5/100 Batch 9090/14303 - Loss:  0.066, Seconds: 11.19\n",
            "Epoch   5/100 Batch 9120/14303 - Loss:  0.068, Seconds: 11.19\n",
            "Epoch   5/100 Batch 9150/14303 - Loss:  0.064, Seconds: 11.20\n",
            "Epoch   5/100 Batch 9180/14303 - Loss:  0.063, Seconds: 11.14\n",
            "Epoch   5/100 Batch 9210/14303 - Loss:  0.063, Seconds: 11.06\n",
            "Epoch   5/100 Batch 9240/14303 - Loss:  0.066, Seconds: 11.20\n",
            "Epoch   5/100 Batch 9270/14303 - Loss:  0.064, Seconds: 11.27\n",
            "Epoch   5/100 Batch 9300/14303 - Loss:  0.065, Seconds: 11.36\n",
            "Epoch   5/100 Batch 9330/14303 - Loss:  0.064, Seconds: 11.31\n",
            "Epoch   5/100 Batch 9360/14303 - Loss:  0.065, Seconds: 11.28\n",
            "Epoch   5/100 Batch 9390/14303 - Loss:  0.065, Seconds: 11.30\n",
            "Epoch   5/100 Batch 9420/14303 - Loss:  0.062, Seconds: 11.25\n",
            "Epoch   5/100 Batch 9450/14303 - Loss:  0.065, Seconds: 11.31\n",
            "Epoch   5/100 Batch 9480/14303 - Loss:  0.064, Seconds: 11.46\n",
            "Epoch   5/100 Batch 9510/14303 - Loss:  0.067, Seconds: 11.48\n",
            "Epoch   5/100 Batch 9540/14303 - Loss:  0.063, Seconds: 11.71\n",
            "Epoch   5/100 Batch 9570/14303 - Loss:  0.062, Seconds: 11.49\n",
            "Epoch   5/100 Batch 9600/14303 - Loss:  0.063, Seconds: 11.50\n",
            "Epoch   5/100 Batch 9630/14303 - Loss:  0.067, Seconds: 11.50\n",
            "Epoch   5/100 Batch 9660/14303 - Loss:  0.065, Seconds: 11.52\n",
            "Epoch   5/100 Batch 9690/14303 - Loss:  0.064, Seconds: 11.59\n",
            "Epoch   5/100 Batch 9720/14303 - Loss:  0.064, Seconds: 11.63\n",
            "Epoch   5/100 Batch 9750/14303 - Loss:  0.063, Seconds: 11.73\n",
            "Epoch   5/100 Batch 9780/14303 - Loss:  0.065, Seconds: 11.71\n",
            "Epoch   5/100 Batch 9810/14303 - Loss:  0.063, Seconds: 11.59\n",
            "Epoch   5/100 Batch 9840/14303 - Loss:  0.065, Seconds: 11.69\n",
            "Epoch   5/100 Batch 9870/14303 - Loss:  0.066, Seconds: 11.80\n",
            "Epoch   5/100 Batch 9900/14303 - Loss:  0.066, Seconds: 11.85\n",
            "Epoch   5/100 Batch 9930/14303 - Loss:  0.063, Seconds: 11.75\n",
            "Epoch   5/100 Batch 9960/14303 - Loss:  0.065, Seconds: 11.82\n",
            "Epoch   5/100 Batch 9990/14303 - Loss:  0.067, Seconds: 11.74\n",
            "Epoch   5/100 Batch 10020/14303 - Loss:  0.066, Seconds: 11.87\n",
            "Epoch   5/100 Batch 10050/14303 - Loss:  0.065, Seconds: 11.81\n",
            "Epoch   5/100 Batch 10080/14303 - Loss:  0.064, Seconds: 11.95\n",
            "Epoch   5/100 Batch 10110/14303 - Loss:  0.065, Seconds: 11.99\n",
            "Epoch   5/100 Batch 10140/14303 - Loss:  0.065, Seconds: 11.96\n",
            "Epoch   5/100 Batch 10170/14303 - Loss:  0.067, Seconds: 12.03\n",
            "Epoch   5/100 Batch 10200/14303 - Loss:  0.062, Seconds: 11.98\n",
            "Epoch   5/100 Batch 10230/14303 - Loss:  0.065, Seconds: 12.01\n",
            "Epoch   5/100 Batch 10260/14303 - Loss:  0.066, Seconds: 12.28\n",
            "Epoch   5/100 Batch 10290/14303 - Loss:  0.064, Seconds: 12.37\n",
            "Epoch   5/100 Batch 10320/14303 - Loss:  0.066, Seconds: 12.16\n",
            "Epoch   5/100 Batch 10350/14303 - Loss:  0.063, Seconds: 12.17\n",
            "Epoch   5/100 Batch 10380/14303 - Loss:  0.062, Seconds: 12.19\n",
            "Epoch   5/100 Batch 10410/14303 - Loss:  0.063, Seconds: 12.31\n",
            "Epoch   5/100 Batch 10440/14303 - Loss:  0.068, Seconds: 12.43\n",
            "Epoch   5/100 Batch 10470/14303 - Loss:  0.066, Seconds: 12.53\n",
            "Epoch   5/100 Batch 10500/14303 - Loss:  0.060, Seconds: 12.41\n",
            "Epoch   5/100 Batch 10530/14303 - Loss:  0.064, Seconds: 12.42\n",
            "Epoch   5/100 Batch 10560/14303 - Loss:  0.067, Seconds: 12.43\n",
            "Epoch   5/100 Batch 10590/14303 - Loss:  0.067, Seconds: 12.66\n",
            "Epoch   5/100 Batch 10620/14303 - Loss:  0.064, Seconds: 12.56\n",
            "Epoch   5/100 Batch 10650/14303 - Loss:  0.065, Seconds: 12.57\n",
            "Epoch   5/100 Batch 10680/14303 - Loss:  0.063, Seconds: 12.56\n",
            "Epoch   5/100 Batch 10710/14303 - Loss:  0.065, Seconds: 12.57\n",
            "Epoch   5/100 Batch 10740/14303 - Loss:  0.063, Seconds: 12.67\n",
            "Epoch   5/100 Batch 10770/14303 - Loss:  0.067, Seconds: 12.82\n",
            "Epoch   5/100 Batch 10800/14303 - Loss:  0.065, Seconds: 12.80\n",
            "Epoch   5/100 Batch 10830/14303 - Loss:  0.065, Seconds: 12.77\n",
            "Epoch   5/100 Batch 10860/14303 - Loss:  0.067, Seconds: 12.80\n",
            "Epoch   5/100 Batch 10890/14303 - Loss:  0.067, Seconds: 12.77\n",
            "Epoch   5/100 Batch 10920/14303 - Loss:  0.065, Seconds: 12.98\n",
            "Epoch   5/100 Batch 10950/14303 - Loss:  0.066, Seconds: 13.01\n",
            "Epoch   5/100 Batch 10980/14303 - Loss:  0.063, Seconds: 13.09\n",
            "Epoch   5/100 Batch 11010/14303 - Loss:  0.064, Seconds: 12.95\n",
            "Epoch   5/100 Batch 11040/14303 - Loss:  0.065, Seconds: 12.93\n",
            "Epoch   5/100 Batch 11070/14303 - Loss:  0.067, Seconds: 13.09\n",
            "Epoch   5/100 Batch 11100/14303 - Loss:  0.066, Seconds: 13.10\n",
            "Epoch   5/100 Batch 11130/14303 - Loss:  0.065, Seconds: 13.18\n",
            "Epoch   5/100 Batch 11160/14303 - Loss:  0.064, Seconds: 13.20\n",
            "Epoch   5/100 Batch 11190/14303 - Loss:  0.064, Seconds: 13.18\n",
            "Epoch   5/100 Batch 11220/14303 - Loss:  0.066, Seconds: 13.34\n",
            "Epoch   5/100 Batch 11250/14303 - Loss:  0.066, Seconds: 13.53\n",
            "Epoch   5/100 Batch 11280/14303 - Loss:  0.066, Seconds: 13.51\n",
            "Epoch   5/100 Batch 11310/14303 - Loss:  0.067, Seconds: 13.40\n",
            "Epoch   5/100 Batch 11340/14303 - Loss:  0.065, Seconds: 13.44\n",
            "Epoch   5/100 Batch 11370/14303 - Loss:  0.067, Seconds: 13.55\n",
            "Epoch   5/100 Batch 11400/14303 - Loss:  0.063, Seconds: 13.57\n",
            "Epoch   5/100 Batch 11430/14303 - Loss:  0.063, Seconds: 13.58\n",
            "Epoch   5/100 Batch 11460/14303 - Loss:  0.061, Seconds: 13.61\n",
            "Epoch   5/100 Batch 11490/14303 - Loss:  0.065, Seconds: 13.59\n",
            "Epoch   5/100 Batch 11520/14303 - Loss:  0.065, Seconds: 13.59\n",
            "Epoch   5/100 Batch 11550/14303 - Loss:  0.067, Seconds: 13.67\n",
            "Epoch   5/100 Batch 11580/14303 - Loss:  0.065, Seconds: 13.68\n",
            "Epoch   5/100 Batch 11610/14303 - Loss:  0.065, Seconds: 13.76\n",
            "Epoch   5/100 Batch 11640/14303 - Loss:  0.065, Seconds: 14.02\n",
            "Epoch   5/100 Batch 11670/14303 - Loss:  0.064, Seconds: 13.75\n",
            "Epoch   5/100 Batch 11700/14303 - Loss:  0.064, Seconds: 13.84\n",
            "Epoch   5/100 Batch 11730/14303 - Loss:  0.066, Seconds: 13.82\n",
            "Epoch   5/100 Batch 11760/14303 - Loss:  0.063, Seconds: 14.04\n",
            "Epoch   5/100 Batch 11790/14303 - Loss:  0.065, Seconds: 14.02\n",
            "Epoch   5/100 Batch 11820/14303 - Loss:  0.062, Seconds: 13.99\n",
            "Epoch   5/100 Batch 11850/14303 - Loss:  0.064, Seconds: 14.06\n",
            "Epoch   5/100 Batch 11880/14303 - Loss:  0.064, Seconds: 14.26\n",
            "Epoch   5/100 Batch 11910/14303 - Loss:  0.061, Seconds: 14.30\n",
            "Epoch   5/100 Batch 11940/14303 - Loss:  0.064, Seconds: 14.34\n",
            "Epoch   5/100 Batch 11970/14303 - Loss:  0.062, Seconds: 14.33\n",
            "Epoch   5/100 Batch 12000/14303 - Loss:  0.064, Seconds: 14.34\n",
            "Epoch   5/100 Batch 12030/14303 - Loss:  0.062, Seconds: 14.36\n",
            "Epoch   5/100 Batch 12060/14303 - Loss:  0.065, Seconds: 14.26\n",
            "Epoch   5/100 Batch 12090/14303 - Loss:  0.064, Seconds: 14.43\n",
            "Epoch   5/100 Batch 12120/14303 - Loss:  0.063, Seconds: 14.58\n",
            "Epoch   5/100 Batch 12150/14303 - Loss:  0.066, Seconds: 14.66\n",
            "Epoch   5/100 Batch 12180/14303 - Loss:  0.063, Seconds: 14.87\n",
            "Epoch   5/100 Batch 12210/14303 - Loss:  0.064, Seconds: 14.85\n",
            "Epoch   5/100 Batch 12240/14303 - Loss:  0.064, Seconds: 14.92\n",
            "Epoch   5/100 Batch 12270/14303 - Loss:  0.065, Seconds: 14.84\n",
            "Epoch   5/100 Batch 12300/14303 - Loss:  0.065, Seconds: 14.86\n",
            "Epoch   5/100 Batch 12330/14303 - Loss:  0.064, Seconds: 15.00\n",
            "Epoch   5/100 Batch 12360/14303 - Loss:  0.066, Seconds: 15.01\n",
            "Epoch   5/100 Batch 12390/14303 - Loss:  0.066, Seconds: 15.12\n",
            "Epoch   5/100 Batch 12420/14303 - Loss:  0.061, Seconds: 15.32\n",
            "Epoch   5/100 Batch 12450/14303 - Loss:  0.061, Seconds: 15.22\n",
            "Epoch   5/100 Batch 12480/14303 - Loss:  0.064, Seconds: 15.25\n",
            "Epoch   5/100 Batch 12510/14303 - Loss:  0.063, Seconds: 15.39\n",
            "Epoch   5/100 Batch 12540/14303 - Loss:  0.064, Seconds: 15.45\n",
            "Epoch   5/100 Batch 12570/14303 - Loss:  0.064, Seconds: 15.55\n",
            "Epoch   5/100 Batch 12600/14303 - Loss:  0.063, Seconds: 15.61\n",
            "Epoch   5/100 Batch 12630/14303 - Loss:  0.064, Seconds: 15.57\n",
            "Epoch   5/100 Batch 12660/14303 - Loss:  0.062, Seconds: 15.73\n",
            "Epoch   5/100 Batch 12690/14303 - Loss:  0.062, Seconds: 15.74\n",
            "Epoch   5/100 Batch 12720/14303 - Loss:  0.065, Seconds: 15.81\n",
            "Epoch   5/100 Batch 12750/14303 - Loss:  0.066, Seconds: 15.92\n",
            "Epoch   5/100 Batch 12780/14303 - Loss:  0.062, Seconds: 15.94\n",
            "Epoch   5/100 Batch 12810/14303 - Loss:  0.064, Seconds: 16.22\n",
            "Epoch   5/100 Batch 12840/14303 - Loss:  0.063, Seconds: 16.20\n",
            "Epoch   5/100 Batch 12870/14303 - Loss:  0.064, Seconds: 16.24\n",
            "Epoch   5/100 Batch 12900/14303 - Loss:  0.061, Seconds: 16.33\n",
            "Epoch   5/100 Batch 12930/14303 - Loss:  0.062, Seconds: 16.46\n",
            "Epoch   5/100 Batch 12960/14303 - Loss:  0.067, Seconds: 16.42\n",
            "Epoch   5/100 Batch 12990/14303 - Loss:  0.064, Seconds: 16.65\n",
            "Epoch   5/100 Batch 13020/14303 - Loss:  0.065, Seconds: 16.61\n",
            "Epoch   5/100 Batch 13050/14303 - Loss:  0.062, Seconds: 16.85\n",
            "Epoch   5/100 Batch 13080/14303 - Loss:  0.064, Seconds: 16.76\n",
            "Epoch   5/100 Batch 13110/14303 - Loss:  0.062, Seconds: 17.05\n",
            "Epoch   5/100 Batch 13140/14303 - Loss:  0.063, Seconds: 17.11\n",
            "Epoch   5/100 Batch 13170/14303 - Loss:  0.063, Seconds: 17.14\n",
            "Epoch   5/100 Batch 13200/14303 - Loss:  0.061, Seconds: 17.24\n",
            "Epoch   5/100 Batch 13230/14303 - Loss:  0.063, Seconds: 17.39\n",
            "Epoch   5/100 Batch 13260/14303 - Loss:  0.063, Seconds: 17.52\n",
            "Epoch   5/100 Batch 13290/14303 - Loss:  0.061, Seconds: 17.71\n",
            "Epoch   5/100 Batch 13320/14303 - Loss:  0.063, Seconds: 17.77\n",
            "Epoch   5/100 Batch 13350/14303 - Loss:  0.063, Seconds: 17.95\n",
            "Epoch   5/100 Batch 13380/14303 - Loss:  0.065, Seconds: 18.06\n",
            "Epoch   5/100 Batch 13410/14303 - Loss:  0.064, Seconds: 18.04\n",
            "Epoch   5/100 Batch 13440/14303 - Loss:  0.060, Seconds: 18.38\n",
            "Epoch   5/100 Batch 13470/14303 - Loss:  0.062, Seconds: 18.29\n",
            "Epoch   5/100 Batch 13500/14303 - Loss:  0.062, Seconds: 18.61\n",
            "Epoch   5/100 Batch 13530/14303 - Loss:  0.064, Seconds: 18.55\n",
            "Epoch   5/100 Batch 13560/14303 - Loss:  0.063, Seconds: 18.71\n",
            "Epoch   5/100 Batch 13590/14303 - Loss:  0.063, Seconds: 18.88\n",
            "Epoch   5/100 Batch 13620/14303 - Loss:  0.064, Seconds: 19.10\n",
            "Epoch   5/100 Batch 13650/14303 - Loss:  0.062, Seconds: 19.26\n",
            "Epoch   5/100 Batch 13680/14303 - Loss:  0.063, Seconds: 19.38\n",
            "Epoch   5/100 Batch 13710/14303 - Loss:  0.062, Seconds: 19.67\n",
            "Epoch   5/100 Batch 13740/14303 - Loss:  0.062, Seconds: 19.99\n",
            "Epoch   5/100 Batch 13770/14303 - Loss:  0.064, Seconds: 20.06\n",
            "Epoch   5/100 Batch 13800/14303 - Loss:  0.065, Seconds: 20.51\n",
            "Epoch   5/100 Batch 13830/14303 - Loss:  0.064, Seconds: 20.46\n",
            "Epoch   5/100 Batch 13860/14303 - Loss:  0.063, Seconds: 20.70\n",
            "Epoch   5/100 Batch 13890/14303 - Loss:  0.060, Seconds: 21.08\n",
            "Epoch   5/100 Batch 13920/14303 - Loss:  0.062, Seconds: 21.20\n",
            "Epoch   5/100 Batch 13950/14303 - Loss:  0.062, Seconds: 21.76\n",
            "Epoch   5/100 Batch 13980/14303 - Loss:  0.064, Seconds: 22.09\n",
            "Epoch   5/100 Batch 14010/14303 - Loss:  0.062, Seconds: 22.81\n",
            "Epoch   5/100 Batch 14040/14303 - Loss:  0.064, Seconds: 23.31\n",
            "Epoch   5/100 Batch 14070/14303 - Loss:  0.063, Seconds: 23.52\n",
            "Epoch   5/100 Batch 14100/14303 - Loss:  0.064, Seconds: 23.85\n",
            "Epoch   5/100 Batch 14130/14303 - Loss:  0.062, Seconds: 24.43\n",
            "Epoch   5/100 Batch 14160/14303 - Loss:  0.064, Seconds: 25.09\n",
            "Epoch   5/100 Batch 14190/14303 - Loss:  0.062, Seconds: 26.24\n",
            "Epoch   5/100 Batch 14220/14303 - Loss:  0.066, Seconds: 27.11\n",
            "Epoch   5/100 Batch 14250/14303 - Loss:  0.064, Seconds: 28.72\n",
            "Epoch   5/100 Batch 14280/14303 - Loss:  0.064, Seconds: 30.67\n",
            "  Validation Input: nto as bad asones andal thinkaa\n",
            "  Validation Output: not as bany to as band to as bad as o\n",
            "  Correct: not as bad as one sandal think\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it i also called granht sahib\n",
            "  Validation Output: it is also called it is also called\n",
            "  Correct: it is also called granth sahib\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it is intended for adults only\n",
            "  Validation Output: it is it is intended for intended fo\n",
            "  Correct: it is intended for adults only\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: reason things out for yourself\n",
            "  Validation Output: reason reason things on things out\n",
            "  Correct: reason things out for yourself\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: it happened to work quite well\n",
            "  Validation Output: it happened it happened to happened\n",
            "  Correct: it happened to work quite well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: chef and bi are donigf very well\n",
            "  Validation Output: chef and which efched and big and big\n",
            "  Correct: chef and i are doing very well\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: center or teh performing arts\n",
            "  Validation Output: center or center for the center or\n",
            "  Correct: center for the performing arts\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: poon ito a hot bwfl to serve\n",
            "  Validation Output: pour option into a hospoon into a\n",
            "  Correct: spoon into a hot bowl to serve\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: pepole mexet ehr and they swoon\n",
            "  Validation Output: people meet people meet her menter he\n",
            "  Correct: people meet her and they swoon\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: my por littl ehaed is boiling\n",
            "  Validation Output: my poor little my poor little heade\n",
            "  Correct: my poor little head is boiling\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: tihs had bcmomea  commonz scene\n",
            "  Validation Output: this had become this had become a com\n",
            "  Correct: this had become a common scene\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: evenwnt trhouhgh the speca\n",
            "  Validation Output: even want the went went through\n",
            "  Correct: even went through the specials\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: preheata  steamer n high haet\n",
            "  Validation Output: preheat a preheat at a stream at st\n",
            "  Correct: preheat a steamer on high heat\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: ol wesl lsomeone had hto say it\n",
            "  Validation Output: lo well well well well well someone h\n",
            "  Correct: lol well someone had to say it\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: he was weaving mats on thelst\n",
            "  Validation Output: he was weal he was weaving was weav\n",
            "  Correct: he was weaving mats on the lst\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: htisis wrogn for many reasons\n",
            "  Validation Output: this is wrong this is wrong for man\n",
            "  Correct: this is wrong for many reasons\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: toeday we heaedd again toc ubbao\n",
            "  Validation Output: today we heady we heady we headed aga\n",
            "  Correct: today we headed again to cubao\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: rep and member tsatusrequired\n",
            "  Validation Output: rep and mere and mere and member an\n",
            "  Correct: rep and member status required\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: ti snowed mhiddling hard to day\n",
            "  Validation Output: it is now be snowed snowed middling h\n",
            "  Correct: it snowed middling hard to day\n",
            "  Is Correct: False\n",
            "\n",
            "  Validation Input: sthey bot hwear leaher sandals\n",
            "  Validation Output: they both they both they both wear l\n",
            "  Correct: they both wear leather sandals\n",
            "  Is Correct: False\n",
            "\n",
            "Testing Loss:  2.495, Seconds: 0.00\n",
            "No Improvement.\n",
            "\n",
            "Training Model: 6\n",
            "Epoch   6/100 Batch   30/14303 - Loss:  0.283, Seconds: 6.39\n",
            "Epoch   6/100 Batch   60/14303 - Loss:  0.103, Seconds: 6.10\n",
            "Epoch   6/100 Batch   90/14303 - Loss:  0.089, Seconds: 6.32\n",
            "Epoch   6/100 Batch  120/14303 - Loss:  0.081, Seconds: 6.39\n",
            "Epoch   6/100 Batch  150/14303 - Loss:  0.083, Seconds: 6.49\n",
            "Epoch   6/100 Batch  180/14303 - Loss:  0.077, Seconds: 6.32\n",
            "Epoch   6/100 Batch  210/14303 - Loss:  0.074, Seconds: 6.45\n",
            "Epoch   6/100 Batch  240/14303 - Loss:  0.071, Seconds: 6.47\n",
            "Epoch   6/100 Batch  270/14303 - Loss:  0.072, Seconds: 6.43\n",
            "Epoch   6/100 Batch  300/14303 - Loss:  0.092, Seconds: 6.36\n",
            "Epoch   6/100 Batch  330/14303 - Loss:  0.072, Seconds: 6.51\n",
            "Epoch   6/100 Batch  360/14303 - Loss:  0.070, Seconds: 6.46\n",
            "Epoch   6/100 Batch  390/14303 - Loss:  0.075, Seconds: 6.21\n",
            "Epoch   6/100 Batch  420/14303 - Loss:  0.068, Seconds: 6.19\n",
            "Epoch   6/100 Batch  450/14303 - Loss:  0.070, Seconds: 6.20\n",
            "Epoch   6/100 Batch  480/14303 - Loss:  0.069, Seconds: 6.21\n",
            "Epoch   6/100 Batch  510/14303 - Loss:  0.071, Seconds: 6.26\n",
            "Epoch   6/100 Batch  540/14303 - Loss:  0.068, Seconds: 6.17\n",
            "Epoch   6/100 Batch  570/14303 - Loss:  0.070, Seconds: 6.25\n",
            "Epoch   6/100 Batch  600/14303 - Loss:  0.073, Seconds: 6.30\n",
            "Epoch   6/100 Batch  630/14303 - Loss:  0.064, Seconds: 6.44\n",
            "Epoch   6/100 Batch  660/14303 - Loss:  0.069, Seconds: 6.37\n",
            "Epoch   6/100 Batch  690/14303 - Loss:  0.069, Seconds: 6.40\n",
            "Epoch   6/100 Batch  720/14303 - Loss:  0.070, Seconds: 6.36\n",
            "Epoch   6/100 Batch  750/14303 - Loss:  0.066, Seconds: 6.40\n",
            "Epoch   6/100 Batch  780/14303 - Loss:  0.067, Seconds: 6.24\n",
            "Epoch   6/100 Batch  810/14303 - Loss:  0.070, Seconds: 6.35\n",
            "Epoch   6/100 Batch  840/14303 - Loss:  0.066, Seconds: 6.31\n",
            "Epoch   6/100 Batch  870/14303 - Loss:  0.070, Seconds: 6.37\n",
            "Epoch   6/100 Batch  900/14303 - Loss:  0.064, Seconds: 6.32\n",
            "Epoch   6/100 Batch  930/14303 - Loss:  0.073, Seconds: 6.42\n",
            "Epoch   6/100 Batch  960/14303 - Loss:  0.070, Seconds: 6.69\n",
            "Epoch   6/100 Batch  990/14303 - Loss:  0.068, Seconds: 6.66\n",
            "Epoch   6/100 Batch 1020/14303 - Loss:  0.068, Seconds: 6.56\n",
            "Epoch   6/100 Batch 1050/14303 - Loss:  0.072, Seconds: 6.46\n",
            "Epoch   6/100 Batch 1080/14303 - Loss:  0.063, Seconds: 6.51\n",
            "Epoch   6/100 Batch 1110/14303 - Loss:  0.064, Seconds: 6.53\n",
            "Epoch   6/100 Batch 1140/14303 - Loss:  0.069, Seconds: 6.56\n",
            "Epoch   6/100 Batch 1170/14303 - Loss:  0.068, Seconds: 6.47\n",
            "Epoch   6/100 Batch 1200/14303 - Loss:  0.064, Seconds: 6.40\n",
            "Epoch   6/100 Batch 1230/14303 - Loss:  0.071, Seconds: 6.43\n",
            "Epoch   6/100 Batch 1260/14303 - Loss:  0.065, Seconds: 6.66\n",
            "Epoch   6/100 Batch 1290/14303 - Loss:  0.068, Seconds: 6.69\n",
            "Epoch   6/100 Batch 1320/14303 - Loss:  0.067, Seconds: 6.73\n",
            "Epoch   6/100 Batch 1350/14303 - Loss:  0.063, Seconds: 6.66\n",
            "Epoch   6/100 Batch 1380/14303 - Loss:  0.064, Seconds: 6.67\n",
            "Epoch   6/100 Batch 1410/14303 - Loss:  0.064, Seconds: 6.70\n",
            "Epoch   6/100 Batch 1440/14303 - Loss:  0.065, Seconds: 6.63\n",
            "Epoch   6/100 Batch 1470/14303 - Loss:  0.066, Seconds: 6.64\n",
            "Epoch   6/100 Batch 1500/14303 - Loss:  0.064, Seconds: 6.61\n",
            "Epoch   6/100 Batch 1530/14303 - Loss:  0.067, Seconds: 6.70\n",
            "Epoch   6/100 Batch 1560/14303 - Loss:  0.077, Seconds: 6.76\n",
            "Epoch   6/100 Batch 1590/14303 - Loss:  0.063, Seconds: 6.89\n",
            "Epoch   6/100 Batch 1620/14303 - Loss:  0.062, Seconds: 6.84\n",
            "Epoch   6/100 Batch 1650/14303 - Loss:  0.067, Seconds: 6.86\n",
            "Epoch   6/100 Batch 1680/14303 - Loss:  0.065, Seconds: 6.83\n",
            "Epoch   6/100 Batch 1710/14303 - Loss:  0.061, Seconds: 6.73\n",
            "Epoch   6/100 Batch 1740/14303 - Loss:  0.065, Seconds: 6.79\n",
            "Epoch   6/100 Batch 1770/14303 - Loss:  0.062, Seconds: 6.86\n",
            "Epoch   6/100 Batch 1800/14303 - Loss:  0.063, Seconds: 6.97\n",
            "Epoch   6/100 Batch 1830/14303 - Loss:  0.059, Seconds: 6.93\n",
            "Epoch   6/100 Batch 1860/14303 - Loss:  0.066, Seconds: 6.96\n",
            "Epoch   6/100 Batch 1890/14303 - Loss:  0.071, Seconds: 7.06\n",
            "Epoch   6/100 Batch 1920/14303 - Loss:  0.066, Seconds: 7.07\n",
            "Epoch   6/100 Batch 1950/14303 - Loss:  0.065, Seconds: 7.12\n",
            "Epoch   6/100 Batch 1980/14303 - Loss:  0.066, Seconds: 7.10\n",
            "Epoch   6/100 Batch 2010/14303 - Loss:  0.066, Seconds: 7.09\n",
            "Epoch   6/100 Batch 2040/14303 - Loss:  0.064, Seconds: 7.11\n",
            "Epoch   6/100 Batch 2070/14303 - Loss:  0.063, Seconds: 7.05\n",
            "Epoch   6/100 Batch 2100/14303 - Loss:  0.063, Seconds: 7.07\n",
            "Epoch   6/100 Batch 2130/14303 - Loss:  0.059, Seconds: 7.18\n",
            "Epoch   6/100 Batch 2160/14303 - Loss:  0.061, Seconds: 7.04\n",
            "Epoch   6/100 Batch 2190/14303 - Loss:  0.070, Seconds: 7.03\n",
            "Epoch   6/100 Batch 2220/14303 - Loss:  0.064, Seconds: 7.23\n",
            "Epoch   6/100 Batch 2250/14303 - Loss:  0.061, Seconds: 7.40\n",
            "Epoch   6/100 Batch 2280/14303 - Loss:  0.063, Seconds: 7.24\n",
            "Epoch   6/100 Batch 2310/14303 - Loss:  0.063, Seconds: 7.25\n",
            "Epoch   6/100 Batch 2340/14303 - Loss:  0.067, Seconds: 7.25\n",
            "Epoch   6/100 Batch 2370/14303 - Loss:  0.063, Seconds: 7.22\n",
            "Epoch   6/100 Batch 2400/14303 - Loss:  0.062, Seconds: 7.24\n",
            "Epoch   6/100 Batch 2430/14303 - Loss:  0.061, Seconds: 7.17\n",
            "Epoch   6/100 Batch 2460/14303 - Loss:  0.066, Seconds: 7.22\n",
            "Epoch   6/100 Batch 2490/14303 - Loss:  0.061, Seconds: 7.20\n",
            "Epoch   6/100 Batch 2520/14303 - Loss:  0.068, Seconds: 7.23\n",
            "Epoch   6/100 Batch 2550/14303 - Loss:  0.065, Seconds: 7.39\n",
            "Epoch   6/100 Batch 2580/14303 - Loss:  0.063, Seconds: 7.38\n",
            "Epoch   6/100 Batch 2610/14303 - Loss:  0.065, Seconds: 7.40\n",
            "Epoch   6/100 Batch 2640/14303 - Loss:  0.066, Seconds: 7.32\n",
            "Epoch   6/100 Batch 2670/14303 - Loss:  0.063, Seconds: 7.33\n",
            "Epoch   6/100 Batch 2700/14303 - Loss:  0.062, Seconds: 7.32\n",
            "Epoch   6/100 Batch 2730/14303 - Loss:  0.061, Seconds: 7.30\n",
            "Epoch   6/100 Batch 2760/14303 - Loss:  0.064, Seconds: 7.31\n",
            "Epoch   6/100 Batch 2790/14303 - Loss:  0.063, Seconds: 7.35\n",
            "Epoch   6/100 Batch 2820/14303 - Loss:  0.065, Seconds: 7.26\n",
            "Epoch   6/100 Batch 2850/14303 - Loss:  0.071, Seconds: 7.44\n",
            "Epoch   6/100 Batch 2880/14303 - Loss:  0.064, Seconds: 7.49\n",
            "Epoch   6/100 Batch 2910/14303 - Loss:  0.064, Seconds: 7.56\n",
            "Epoch   6/100 Batch 2940/14303 - Loss:  0.060, Seconds: 7.43\n",
            "Epoch   6/100 Batch 2970/14303 - Loss:  0.064, Seconds: 7.41\n",
            "Epoch   6/100 Batch 3000/14303 - Loss:  0.063, Seconds: 7.52\n",
            "Epoch   6/100 Batch 3030/14303 - Loss:  0.064, Seconds: 7.50\n",
            "Epoch   6/100 Batch 3060/14303 - Loss:  0.062, Seconds: 7.48\n",
            "Epoch   6/100 Batch 3090/14303 - Loss:  0.063, Seconds: 7.44\n",
            "Epoch   6/100 Batch 3120/14303 - Loss:  0.060, Seconds: 7.40\n",
            "Epoch   6/100 Batch 3150/14303 - Loss:  0.063, Seconds: 7.51\n",
            "Epoch   6/100 Batch 3180/14303 - Loss:  0.070, Seconds: 7.71\n",
            "Epoch   6/100 Batch 3210/14303 - Loss:  0.065, Seconds: 7.70\n",
            "Epoch   6/100 Batch 3240/14303 - Loss:  0.064, Seconds: 7.68\n",
            "Epoch   6/100 Batch 3270/14303 - Loss:  0.063, Seconds: 7.67\n",
            "Epoch   6/100 Batch 3300/14303 - Loss:  0.062, Seconds: 7.70\n",
            "Epoch   6/100 Batch 3330/14303 - Loss:  0.063, Seconds: 7.65\n",
            "Epoch   6/100 Batch 3360/14303 - Loss:  0.065, Seconds: 7.72\n",
            "Epoch   6/100 Batch 3390/14303 - Loss:  0.065, Seconds: 7.75\n",
            "Epoch   6/100 Batch 3420/14303 - Loss:  0.065, Seconds: 7.91\n",
            "Epoch   6/100 Batch 3450/14303 - Loss:  0.064, Seconds: 7.66\n",
            "Epoch   6/100 Batch 3480/14303 - Loss:  0.069, Seconds: 7.78\n",
            "Epoch   6/100 Batch 3510/14303 - Loss:  0.067, Seconds: 7.95\n",
            "Epoch   6/100 Batch 3540/14303 - Loss:  0.066, Seconds: 7.89\n",
            "Epoch   6/100 Batch 3570/14303 - Loss:  0.066, Seconds: 7.80\n",
            "Epoch   6/100 Batch 3600/14303 - Loss:  0.062, Seconds: 7.80\n",
            "Epoch   6/100 Batch 3630/14303 - Loss:  0.066, Seconds: 7.84\n",
            "Epoch   6/100 Batch 3660/14303 - Loss:  0.061, Seconds: 7.87\n",
            "Epoch   6/100 Batch 3690/14303 - Loss:  0.063, Seconds: 7.77\n",
            "Epoch   6/100 Batch 3720/14303 - Loss:  0.063, Seconds: 7.86\n",
            "Epoch   6/100 Batch 3750/14303 - Loss:  0.063, Seconds: 7.82\n",
            "Epoch   6/100 Batch 3780/14303 - Loss:  0.065, Seconds: 7.90\n",
            "Epoch   6/100 Batch 3810/14303 - Loss:  0.063, Seconds: 8.00\n",
            "Epoch   6/100 Batch 3840/14303 - Loss:  0.064, Seconds: 8.03\n",
            "Epoch   6/100 Batch 3870/14303 - Loss:  0.064, Seconds: 8.01\n",
            "Epoch   6/100 Batch 3900/14303 - Loss:  0.065, Seconds: 7.97\n",
            "Epoch   6/100 Batch 3930/14303 - Loss:  0.062, Seconds: 8.02\n",
            "Epoch   6/100 Batch 3960/14303 - Loss:  0.067, Seconds: 7.90\n",
            "Epoch   6/100 Batch 3990/14303 - Loss:  0.062, Seconds: 7.94\n",
            "Epoch   6/100 Batch 4020/14303 - Loss:  0.064, Seconds: 8.03\n",
            "Epoch   6/100 Batch 4050/14303 - Loss:  0.062, Seconds: 8.00\n",
            "Epoch   6/100 Batch 4080/14303 - Loss:  0.061, Seconds: 8.05\n",
            "Epoch   6/100 Batch 4110/14303 - Loss:  0.059, Seconds: 7.99\n",
            "Epoch   6/100 Batch 4140/14303 - Loss:  0.070, Seconds: 8.17\n",
            "Epoch   6/100 Batch 4170/14303 - Loss:  0.060, Seconds: 8.16\n",
            "Epoch   6/100 Batch 4200/14303 - Loss:  0.062, Seconds: 8.10\n",
            "Epoch   6/100 Batch 4230/14303 - Loss:  0.063, Seconds: 8.17\n",
            "Epoch   6/100 Batch 4260/14303 - Loss:  0.065, Seconds: 8.18\n",
            "Epoch   6/100 Batch 4290/14303 - Loss:  0.062, Seconds: 8.20\n",
            "Epoch   6/100 Batch 4320/14303 - Loss:  0.059, Seconds: 8.17\n",
            "Epoch   6/100 Batch 4350/14303 - Loss:  0.061, Seconds: 8.33\n",
            "Epoch   6/100 Batch 4380/14303 - Loss:  0.066, Seconds: 8.22\n",
            "Epoch   6/100 Batch 4410/14303 - Loss:  0.063, Seconds: 8.27\n",
            "Epoch   6/100 Batch 4440/14303 - Loss:  0.068, Seconds: 8.41\n",
            "Epoch   6/100 Batch 4470/14303 - Loss:  0.062, Seconds: 8.51\n",
            "Epoch   6/100 Batch 4500/14303 - Loss:  0.064, Seconds: 8.69\n",
            "Epoch   6/100 Batch 4530/14303 - Loss:  0.062, Seconds: 8.54\n",
            "Epoch   6/100 Batch 4560/14303 - Loss:  0.063, Seconds: 8.41\n",
            "Epoch   6/100 Batch 4590/14303 - Loss:  0.063, Seconds: 8.61\n",
            "Epoch   6/100 Batch 4620/14303 - Loss:  0.062, Seconds: 8.49\n",
            "Epoch   6/100 Batch 4650/14303 - Loss:  0.063, Seconds: 8.38\n",
            "Epoch   6/100 Batch 4680/14303 - Loss:  0.062, Seconds: 8.46\n",
            "Epoch   6/100 Batch 4710/14303 - Loss:  0.063, Seconds: 8.40\n",
            "Epoch   6/100 Batch 4740/14303 - Loss:  0.067, Seconds: 8.49\n",
            "Epoch   6/100 Batch 4770/14303 - Loss:  0.062, Seconds: 8.61\n",
            "Epoch   6/100 Batch 4800/14303 - Loss:  0.063, Seconds: 8.60\n",
            "Epoch   6/100 Batch 4830/14303 - Loss:  0.066, Seconds: 8.56\n",
            "Epoch   6/100 Batch 4860/14303 - Loss:  0.061, Seconds: 8.52\n",
            "Epoch   6/100 Batch 4890/14303 - Loss:  0.064, Seconds: 8.58\n",
            "Epoch   6/100 Batch 4920/14303 - Loss:  0.060, Seconds: 8.58\n",
            "Epoch   6/100 Batch 4950/14303 - Loss:  0.061, Seconds: 8.47\n",
            "Epoch   6/100 Batch 4980/14303 - Loss:  0.062, Seconds: 8.56\n",
            "Epoch   6/100 Batch 5010/14303 - Loss:  0.065, Seconds: 8.56\n",
            "Epoch   6/100 Batch 5040/14303 - Loss:  0.061, Seconds: 8.56\n",
            "Epoch   6/100 Batch 5070/14303 - Loss:  0.070, Seconds: 8.75\n",
            "Epoch   6/100 Batch 5100/14303 - Loss:  0.063, Seconds: 8.78\n",
            "Epoch   6/100 Batch 5130/14303 - Loss:  0.062, Seconds: 8.85\n",
            "Epoch   6/100 Batch 5160/14303 - Loss:  0.062, Seconds: 8.77\n",
            "Epoch   6/100 Batch 5190/14303 - Loss:  0.061, Seconds: 8.69\n",
            "Epoch   6/100 Batch 5220/14303 - Loss:  0.060, Seconds: 8.75\n",
            "Epoch   6/100 Batch 5250/14303 - Loss:  0.061, Seconds: 8.74\n",
            "Epoch   6/100 Batch 5280/14303 - Loss:  0.061, Seconds: 8.72\n",
            "Epoch   6/100 Batch 5310/14303 - Loss:  0.062, Seconds: 8.72\n",
            "Epoch   6/100 Batch 5340/14303 - Loss:  0.063, Seconds: 8.74\n",
            "Epoch   6/100 Batch 5370/14303 - Loss:  0.063, Seconds: 8.90\n",
            "Epoch   6/100 Batch 5400/14303 - Loss:  0.061, Seconds: 9.01\n",
            "Epoch   6/100 Batch 5430/14303 - Loss:  0.065, Seconds: 8.92\n",
            "Epoch   6/100 Batch 5460/14303 - Loss:  0.061, Seconds: 8.93\n",
            "Epoch   6/100 Batch 5490/14303 - Loss:  0.064, Seconds: 9.10\n",
            "Epoch   6/100 Batch 5520/14303 - Loss:  0.066, Seconds: 9.00\n",
            "Epoch   6/100 Batch 5550/14303 - Loss:  0.063, Seconds: 8.93\n",
            "Epoch   6/100 Batch 5580/14303 - Loss:  0.060, Seconds: 8.90\n",
            "Epoch   6/100 Batch 5610/14303 - Loss:  0.064, Seconds: 8.80\n",
            "Epoch   6/100 Batch 5640/14303 - Loss:  0.062, Seconds: 8.90\n",
            "Epoch   6/100 Batch 5670/14303 - Loss:  0.063, Seconds: 8.93\n",
            "Epoch   6/100 Batch 5700/14303 - Loss:  0.061, Seconds: 8.94\n",
            "Epoch   6/100 Batch 5730/14303 - Loss:  0.061, Seconds: 8.94\n",
            "Epoch   6/100 Batch 5760/14303 - Loss:  0.059, Seconds: 8.97\n",
            "Epoch   6/100 Batch 5790/14303 - Loss:  0.062, Seconds: 9.05\n",
            "Epoch   6/100 Batch 5820/14303 - Loss:  0.061, Seconds: 8.98\n",
            "Epoch   6/100 Batch 5850/14303 - Loss:  0.063, Seconds: 8.96\n",
            "Epoch   6/100 Batch 5880/14303 - Loss:  0.064, Seconds: 9.06\n",
            "Epoch   6/100 Batch 5910/14303 - Loss:  0.059, Seconds: 9.11\n",
            "Epoch   6/100 Batch 5940/14303 - Loss:  0.064, Seconds: 9.00\n",
            "Epoch   6/100 Batch 5970/14303 - Loss:  0.063, Seconds: 9.22\n",
            "Epoch   6/100 Batch 6000/14303 - Loss:  0.064, Seconds: 9.14\n",
            "Epoch   6/100 Batch 6030/14303 - Loss:  0.064, Seconds: 9.19\n",
            "Epoch   6/100 Batch 6060/14303 - Loss:  0.063, Seconds: 9.08\n",
            "Epoch   6/100 Batch 6090/14303 - Loss:  0.062, Seconds: 9.19\n",
            "Epoch   6/100 Batch 6120/14303 - Loss:  0.064, Seconds: 9.25\n",
            "Epoch   6/100 Batch 6150/14303 - Loss:  0.067, Seconds: 9.29\n",
            "Epoch   6/100 Batch 6180/14303 - Loss:  0.064, Seconds: 9.42\n",
            "Epoch   6/100 Batch 6210/14303 - Loss:  0.061, Seconds: 9.37\n",
            "Epoch   6/100 Batch 6240/14303 - Loss:  0.066, Seconds: 9.71\n",
            "Epoch   6/100 Batch 6270/14303 - Loss:  0.063, Seconds: 10.03\n",
            "Epoch   6/100 Batch 6300/14303 - Loss:  0.064, Seconds: 9.85\n",
            "Epoch   6/100 Batch 6330/14303 - Loss:  0.062, Seconds: 9.91\n",
            "Epoch   6/100 Batch 6360/14303 - Loss:  0.063, Seconds: 9.93\n",
            "Epoch   6/100 Batch 6390/14303 - Loss:  0.063, Seconds: 9.74\n",
            "Epoch   6/100 Batch 6420/14303 - Loss:  0.063, Seconds: 9.64\n",
            "Epoch   6/100 Batch 6450/14303 - Loss:  0.063, Seconds: 9.64\n",
            "Epoch   6/100 Batch 6480/14303 - Loss:  0.064, Seconds: 9.42\n",
            "Epoch   6/100 Batch 6510/14303 - Loss:  0.064, Seconds: 9.45\n",
            "Epoch   6/100 Batch 6540/14303 - Loss:  0.059, Seconds: 9.59\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}